{
  "models": [
    {
      "id": "phi-3.5-webgpu",
      "name": "Phi 3.5 WebGPU",
      "description": "Microsoft's compact yet powerful language model running in WebGPU",
      "longDescription": "A simple React + Vite application for running Phi-3.5-mini-instruct, a powerful small language model, locally in the browser using Transformers.js and WebGPU-acceleration. This model delivers impressive chat capabilities while maintaining efficiency for client-side deployment.",
      "category": "llm",
      "subcategory": "chat",
      "valueRanking": 9,
      "valueLabel": "Very High",
      "modelName": "onnx-community/Phi-3.5-mini-instruct-onnx-web",
      "modelType": "text-generation",
      "modelSize": "2.7B parameters", 
      "thumbnailUrl": "/images/models/phi-3.5.png",
      "screenshotUrls": [
        "/images/screenshots/phi-3.5-1.png",
        "/images/screenshots/phi-3.5-2.png"
      ],
      "demoUrl": "https://huggingface.co/spaces/webml-community/phi-3.5-webgpu",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/phi-3.5-webgpu",
      "tags": ["language-model", "chat", "webgpu", "microsoft"],
      "features": ["Conversational", "Instruction-tuned", "WebGPU-accelerated"],
      "generationConfig": {
        "max_new_tokens": 512,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "performance": {
        "loadTime": "~15 seconds",
        "memoryUsage": "~1.5GB",
        "browserSupport": ["Chrome", "Edge", "Safari TP"]
      },
      "relatedModels": ["llama-3.2-webgpu", "deepseek-r1-webgpu", "smollm-webgpu"]
    },
    {
      "id": "llama-3.2-webgpu",
      "name": "Llama 3.2 WebGPU",
      "description": "State-of-the-art language model running in browser with WebGPU acceleration",
      "longDescription": "Experience Meta's Llama 3.2, a powerful open-source language model running entirely in your browser using WebGPU acceleration. This demo showcases how sophisticated AI models can run client-side with excellent performance and no server requirements.",
      "category": "llm",
      "subcategory": "chat",
      "valueRanking": 9,
      "valueLabel": "Very High",
      "modelName": "onnx-community/Llama-3.2-1B-Instruct-q4f16",
      "modelType": "text-generation",
      "modelSize": "8B parameters",
      "thumbnailUrl": "/images/models/llama-3.2.png",
      "screenshotUrls": [
        "/images/screenshots/llama-3.2-1.png",
        "/images/screenshots/llama-3.2-2.png"
      ],
      "demoUrl": "https://huggingface.co/spaces/webml-community/llama-3.2-webgpu",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/llama-3.2-webgpu",
      "tags": ["language-model", "chat", "webgpu", "meta"],
      "features": ["Conversational", "Instruction-tuned", "WebGPU-accelerated"],
      "generationConfig": {
        "max_new_tokens": 512,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "performance": {
        "loadTime": "~20 seconds",
        "memoryUsage": "~2.0GB",
        "browserSupport": ["Chrome", "Edge", "Safari TP"]
      },
      "relatedModels": ["phi-3.5-webgpu", "llama-3.2-reasoning-webgpu", "smollm-webgpu"]
    },
    {
      "id": "realtime-whisper-webgpu",
      "name": "Realtime Whisper WebGPU",
      "description": "Real-time speech recognition with WebGPU acceleration",
      "longDescription": "This demo showcases OpenAI's Whisper model running in real-time directly in your browser using WebGPU acceleration. It can transcribe speech from your microphone or uploaded audio files with impressive accuracy and supports multiple languages.",
      "category": "speech",
      "subcategory": "recognition",
      "valueRanking": 9,
      "valueLabel": "Very High",
      "modelName": "openai/whisper-tiny.en-onnx",
      "modelType": "speech-recognition",
      "modelSize": "39M parameters",
      "thumbnailUrl": "/images/models/whisper-realtime.png",
      "screenshotUrls": [
        "/images/screenshots/whisper-realtime-1.png",
        "/images/screenshots/whisper-realtime-2.png"
      ],
      "demoUrl": "https://huggingface.co/spaces/webml-community/realtime-whisper-webgpu",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/realtime-whisper-webgpu",
      "tags": ["speech-recognition", "audio", "webgpu", "openai"],
      "features": ["Real-time", "Multi-language", "Microphone input", "WebGPU-accelerated"],
      "performance": {
        "loadTime": "~5 seconds",
        "memoryUsage": "~250MB",
        "browserSupport": ["Chrome", "Edge", "Safari TP"]
      },
      "relatedModels": ["whisper-word-timestamps", "speecht5-web", "text-to-speech-webgpu"]
    },
    {
      "id": "depth-anything",
      "name": "Depth Anything",
      "description": "Monocular depth estimation from images to create depth maps",
      "longDescription": "Depth Anything is a powerful computer vision model that estimates depth from a single image. This demo allows you to upload any image and get a detailed depth map showing the relative distances of objects in the scene, all running locally in your browser.",
      "category": "computer-vision",
      "subcategory": "depth-estimation",
      "valueRanking": 8,
      "valueLabel": "High",
      "modelName": "onnx-community/depth-anything-small-hf",
      "modelType": "depth-estimation",
      "modelSize": "~20M parameters",
      "thumbnailUrl": "/images/models/depth-anything.png",
      "screenshotUrls": [
        "/images/screenshots/depth-anything-1.png",
        "/images/screenshots/depth-anything-2.png"
      ],
      "demoUrl": "https://huggingface.co/spaces/webml-community/depth-anything",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/depth-anything",
      "tags": ["computer-vision", "depth-estimation", "image-processing"],
      "features": ["Image upload", "Depth visualization", "Color mapping"],
      "performance": {
        "loadTime": "~5 seconds",
        "memoryUsage": "~300MB",
        "browserSupport": ["Chrome", "Firefox", "Safari"]
      },
      "relatedModels": ["depth-estimation-video", "remove-background-webgpu", "video-background-removal"]
    },
    {
      "id": "adaptive-retrieval",
      "name": "Adaptive Retrieval",
      "description": "Matryoshka embedding system for adaptive document retrieval with adjustable dimensions",
      "longDescription": "This application demonstrates the power of Matryoshka embeddings (using Nomic Embed v1.5) for adaptive document retrieval. It allows you to adjust the embedding dimensions in real-time to see how the similarity between text passages changes, providing a unique view into how dimensionality affects semantic search.",
      "category": "text-embedding",
      "subcategory": "retrieval",
      "valueRanking": 8,
      "valueLabel": "High",
      "modelName": "nomic-ai/nomic-embed-text-v1.5",
      "modelType": "feature-extraction",
      "modelSize": "N/A",
      "thumbnailUrl": "/images/models/adaptive-retrieval.png",
      "screenshotUrls": [
        "/images/screenshots/adaptive-retrieval-1.png",
        "/images/screenshots/adaptive-retrieval-2.png"
      ],
      "demoUrl": "https://huggingface.co/spaces/webml-community/adaptive-retrieval",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/adaptive-retrieval",
      "tags": ["text-embedding", "semantic-search", "matryoshka", "dimensionality"],
      "features": ["Adjustable dimensions", "Real-time similarity scoring", "Text comparison"],
      "performance": {
        "loadTime": "~3 seconds",
        "memoryUsage": "~200MB",
        "browserSupport": ["All modern browsers"]
      },
      "relatedModels": ["pglite-semantic-search", "webgpu-nomic-embed", "cross-encoder"]
    },
    {
      "id": "cross-encoder",
      "name": "Cross-Encoder Reranking",
      "description": "Powerful reranking model for improving search relevance",
      "longDescription": "This demo showcases the mxbai-rerank-xsmall-v1 cross-encoder model for reranking search results. Cross-encoders provide much higher accuracy than bi-encoders for ranking by processing query and document pairs together.",
      "category": "text-embedding",
      "subcategory": "reranking",
      "valueRanking": 8,
      "valueLabel": "High",
      "modelName": "mixedbread-ai/mxbai-rerank-xsmall-v1",
      "modelType": "zero-shot-classification",
      "modelSize": "33M parameters",
      "thumbnailUrl": "/images/models/cross-encoder.png",
      "demoUrl": "https://huggingface.co/spaces/webml-community/cross-encoder",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/cross-encoder",
      "tags": ["reranking", "search", "crossencoder", "ranking"],
      "features": ["Document reranking", "Relevance scoring", "Text comparison"],
      "performance": {
        "loadTime": "~3 seconds",
        "memoryUsage": "~150MB",
        "browserSupport": ["All modern browsers"]
      },
      "relatedModels": ["adaptive-retrieval", "pglite-semantic-search", "zero-shot-classification"]
    },
    {
      "id": "zero-shot-classification",
      "name": "Zero-Shot Classification",
      "description": "Classify text into any set of categories without specific training",
      "longDescription": "This demo showcases zero-shot text classification, which allows you to classify text into arbitrary categories without having to fine-tune a model for each specific task. Just provide your categories and let the model determine the best match.",
      "category": "text-embedding",
      "subcategory": "classification",
      "valueRanking": 7,
      "valueLabel": "High",
      "modelName": "MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33",
      "modelType": "zero-shot-classification",
      "modelSize": "33M parameters",
      "thumbnailUrl": "/images/models/zero-shot-classification.png",
      "demoUrl": "https://huggingface.co/spaces/webml-community/zero-shot-classification",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/zero-shot-classification",
      "tags": ["classification", "nlp", "zero-shot"],
      "features": ["Custom categories", "Multi-label classification", "No fine-tuning needed"],
      "performance": {
        "loadTime": "~2 seconds",
        "memoryUsage": "~120MB",
        "browserSupport": ["All modern browsers"]
      },
      "relatedModels": ["cross-encoder", "adaptive-retrieval", "pglite-semantic-search"]
    },
    {
      "id": "speecht5-web",
      "name": "SpeechT5 TTS",
      "description": "Text-to-speech synthesis in your browser",
      "longDescription": "This demo showcases text-to-speech synthesis using the SpeechT5 model. It converts text to realistic speech directly in your browser without any server-side processing. Choose from multiple speakers to generate different voice styles.",
      "category": "speech",
      "subcategory": "synthesis",
      "valueRanking": 8,
      "valueLabel": "High",
      "modelName": "Xenova/speecht5_tts",
      "modelType": "text-to-speech",
      "modelSize": "120M parameters",
      "thumbnailUrl": "/images/models/speecht5-web.png",
      "demoUrl": "https://huggingface.co/spaces/webml-community/speecht5-web",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/speecht5-web",
      "tags": ["text-to-speech", "tts", "audio-generation", "speech-synthesis"],
      "features": ["Multiple speakers", "Customizable voice", "No server required"],
      "performance": {
        "loadTime": "~4 seconds",
        "memoryUsage": "~200MB",
        "browserSupport": ["Chrome", "Firefox", "Safari", "Edge"]
      },
      "relatedModels": ["text-to-speech-webgpu", "realtime-whisper-webgpu", "whisper-word-timestamps"]
    },
    {
      "id": "text-to-speech-webgpu",
      "name": "OuteTTS WebGPU",
      "description": "Multilingual text-to-speech synthesis with WebGPU acceleration",
      "longDescription": "This demo showcases OuteTTS, a multilingual text-to-speech model that runs entirely in your browser with WebGPU acceleration. It supports English, Chinese, Japanese, and Korean languages with natural-sounding speech generation.",
      "category": "speech",
      "subcategory": "synthesis",
      "valueRanking": 9,
      "valueLabel": "Very High",
      "modelName": "onnx-community/OuteTTS-0.2-500M",
      "modelType": "text-to-speech",
      "modelSize": "500M parameters",
      "thumbnailUrl": "/images/models/text-to-speech-webgpu.png",
      "demoUrl": "https://huggingface.co/spaces/webml-community/text-to-speech-webgpu",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/text-to-speech-webgpu",
      "tags": ["text-to-speech", "tts", "webgpu", "multilingual"],
      "features": ["Multiple languages", "WebGPU acceleration", "Voice customization"],
      "performance": {
        "loadTime": "~8 seconds",
        "memoryUsage": "~800MB",
        "browserSupport": ["Chrome", "Edge", "Safari TP"]
      },
      "relatedModels": ["speecht5-web", "realtime-whisper-webgpu", "whisper-word-timestamps"]
    },
    {
      "id": "florence2-webgpu",
      "name": "Florence 2 WebGPU",
      "description": "Versatile vision foundation model with multiple vision capabilities",
      "longDescription": "This demo showcases Florence-2, a vision foundation model that can perform various vision-language tasks like captioning, OCR, object detection, and more. It runs entirely in your browser with WebGPU acceleration.",
      "category": "multimodal",
      "subcategory": "vision-foundation",
      "valueRanking": 9,
      "valueLabel": "Very High",
      "modelName": "onnx-community/Florence-2-base-ft",
      "modelType": "image-classification",
      "modelSize": "230M parameters",
      "thumbnailUrl": "/images/models/florence2-webgpu.png",
      "demoUrl": "https://huggingface.co/spaces/webml-community/florence2-webgpu",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/florence2-webgpu",
      "tags": ["vision", "multimodal", "webgpu", "captioning", "ocr"],
      "features": ["Image captioning", "OCR", "Object detection", "WebGPU-accelerated"],
      "performance": {
        "loadTime": "~10 seconds",
        "memoryUsage": "~400MB",
        "browserSupport": ["Chrome", "Edge", "Safari TP"]
      },
      "relatedModels": ["depth-anything", "janus-webgpu", "smolvlm-webgpu"]
    },
    {
      "id": "janus-webgpu",
      "name": "Janus WebGPU",
      "description": "Text-to-image generation in your browser with WebGPU",
      "longDescription": "This demo showcases Janus, a multimodal model that can generate images from text prompts right in your browser using WebGPU acceleration. It's a compact but powerful text-to-image model that delivers impressive results without requiring a server.",
      "category": "multimodal",
      "subcategory": "text-to-image",
      "valueRanking": 9,
      "valueLabel": "Very High",
      "modelName": "onnx-community/Janus-1.3B-ONNX",
      "modelType": "text-generation",
      "modelSize": "1.3B parameters",
      "thumbnailUrl": "/images/models/janus-webgpu.png",
      "demoUrl": "https://huggingface.co/spaces/webml-community/janus-webgpu",
      "sourceUrl": "https://github.com/huggingface/transformers.js-examples/tree/main/janus-webgpu",
      "tags": ["text-to-image", "image-generation", "webgpu", "multimodal"],
      "features": ["Text-to-image", "WebGPU acceleration", "Chat interface"],
      "performance": {
        "loadTime": "~15 seconds",
        "memoryUsage": "~1.5GB",
        "browserSupport": ["Chrome", "Edge", "Safari TP"]
      },
      "relatedModels": ["janus-pro-webgpu", "florence2-webgpu", "smolvlm-webgpu"]
    }
  ],
  "categories": [
    {
      "id": "llm",
      "name": "Language Models",
      "description": "Advanced conversational AIs like Llama, Phi, and SmolLM",
      "color": "yellow",
      "icon": "chat"
    },
    {
      "id": "computer-vision",
      "name": "Computer Vision",
      "description": "Image analysis, depth estimation, and background removal",
      "color": "blue",
      "icon": "camera"
    },
    {
      "id": "multimodal",
      "name": "Multi-modal AI",
      "description": "Models that understand both text and images",
      "color": "purple",
      "icon": "image-text"
    },
    {
      "id": "speech",
      "name": "Speech",
      "description": "Recognition, synthesis and audio processing",
      "color": "green",
      "icon": "microphone"
    },
    {
      "id": "text-embedding",
      "name": "Embeddings",
      "description": "Text and semantic search using vector embeddings",
      "color": "red",
      "icon": "database"
    },
    {
      "id": "developer-tools",
      "name": "Developer Tools",
      "description": "Code generation and programming assistants",
      "color": "indigo",
      "icon": "code"
    },
    {
      "id": "audio",
      "name": "Audio Processing",
      "description": "Audio analysis and generation",
      "color": "pink",
      "icon": "music"
    },
    {
      "id": "education",
      "name": "Educational",
      "description": "Tools for learning about AI and models",
      "color": "orange",
      "icon": "book"
    },
    {
      "id": "search",
      "name": "Search & Retrieval",
      "description": "Information retrieval and search capabilities",
      "color": "teal",
      "icon": "search"
    }
  ]
}