----- .\adaptive-retrieval\src\App.jsx -----

import { useState, useRef, useEffect, useCallback } from "react";

const PLACEHOLDER_TEXTS = [
  "A panda is a large black-and-white bear native to China.",
  "The typical life span of a panda is 20 years in the wild.",
  "A panda's diet consists almost entirely of bamboo.",
  "Ailuropoda melanoleuca is a bear species endemic to China.",
  "I love pandas so much!",
  "Bamboo is a fast-growing, woody grass.",
  "My favorite movie is Kung Fu Panda.",
  "I love the color blue.",
  "Once upon a time, in a land far, far away...",
  "Hello world.",
  "This is an example sentence.",
].sort(() => Math.random() - 0.5);

function normalize(embedding) {
  const magnitude = Math.sqrt(
    embedding.reduce((sum, val) => sum + val * val, 0),
  );
  return embedding.map((val) => val / magnitude);
}

function dot(a, b) {
  return a.reduce((acc, val, i) => acc + val * b[i], 0);
}

function App() {
  const [status, setStatus] = useState("idle");

  const [source, setSource] = useState("What is a panda?");
  const [text, setText] = useState(PLACEHOLDER_TEXTS.join("\n"));

  const [dimensions, setDimensions] = useState(768);

  const [embeddings, setEmbeddings] = useState([]);
  const [results, setResults] = useState([]);

  // Create a reference to the worker object.
  const worker = useRef(null);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    worker.current ??= new Worker(new URL("./worker.js", import.meta.url), {
      type: "module",
    });

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      const status = e.data.status;
      if (status === "initiate") {
        setStatus("loading");
      } else if (status === "ready") {
        setStatus("ready");
      } else if (status === "complete") {
        const embeddings = e.data.embeddings;
        setDimensions(embeddings[0].length);
        setEmbeddings(embeddings);
        setStatus("idle");
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () =>
      worker.current.removeEventListener("message", onMessageReceived);
  }, []);

  const run = useCallback(() => {
    setStatus("processing");
    worker.current.postMessage({
      source,
      text,
    });
  }, [source, text]);

  useEffect(() => {
    if (embeddings.length === 0) return;
    const slicedEmbeddings = embeddings.map((x) =>
      normalize(x.slice(0, dimensions)),
    );

    const sourceEmbedding = slicedEmbeddings[0];
    const sentenceEmbeddings = slicedEmbeddings.slice(1);

    // Compute the cosine similarity between the source sentence and the other sentences.
    // NOTE: Since vectors are normalized, we use the dot product.
    const similarities = sentenceEmbeddings.map((embedding) =>
      dot(sourceEmbedding, embedding),
    );

    setResults(
      text
        .trim()
        .split("\n")
        .map((sentence, i) => ({
          sentence,
          similarity: similarities[i],
        }))
        .sort((a, b) => b.similarity - a.similarity),
    );
  }, [text, embeddings, dimensions]);

  const busy = status !== "idle";

  return (
    <div className="flex flex-col h-full p-4 h-screen w-screen bg-gray-50">
      <h1 className="text-2xl md:text-4xl font-bold text-center mb-1">
        Adaptive Retrieval w/ Matryoshka Embeddings
      </h1>
      <p className="text-lg md:text-xl font-medium text-center mb-2">
        Powered by{" "}
        <a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5">
          Nomic Embed v1.5
        </a>{" "}
        and{" "}
        <a href="http://huggingface.co/docs/transformers.js">
          ðŸ¤— Transformers.js
        </a>
      </p>
      <div className="flex-grow flex flex-wrap p-4">
        <div className="flex flex-col items-center gap-y-1 w-full md:w-1/2">
          <label className="text-lg font-medium">Query</label>
          <textarea
            placeholder="Enter source sentence."
            className="border w-full p-1 resize-none overflow-hidden h-10"
            value={source}
            onChange={(e) => {
              setSource(e.target.value);
              setResults([]);
              setEmbeddings([]);
            }}
          ></textarea>
          <label className="text-lg font-medium mt-1">Text</label>
          <textarea
            placeholder="Enter sentences to compare with the source sentence. One sentence per line."
            className="border w-full p-1 h-full resize-none"
            value={text}
            onChange={(e) => {
              setText(e.target.value);
              setResults([]);
              setEmbeddings([]);
            }}
          ></textarea>

          <button
            className="border py-1 px-2 bg-blue-400 rounded text-white text-lg font-medium disabled:opacity-50 disabled:cursor-not-allowed"
            disabled={busy}
            onClick={run}
          >
            {!busy
              ? embeddings.length === 0
                ? "Compute Embeddings"
                : "Recompute Embeddings"
              : status === "loading"
                ? "Model loading..."
                : "Processing"}
          </button>
        </div>
        <div className="flex flex-col items-center w-full md:w-1/2 gap-y-1">
          {embeddings.length > 0 && (
            <>
              <label className="text-lg font-medium">Dimensions</label>
              <input
                type="range"
                min="64"
                max="768"
                step="1"
                value={dimensions}
                onChange={(e) => {
                  setDimensions(e.target.value);
                }}
                className="w-[98%] h-[10px]"
              />
              <p className="font-bold text-sm">{dimensions}</p>
              <div className="w-full flex flex-col gap-y-1">
                <label className="text-lg font-medium text-center mt-1">
                  Results
                </label>
                <div className="flex flex-col gap-y-1">
                  {results.map((result, i) => (
                    <div
                      key={i}
                      className="flex gap-x-2 border mx-2 p-1 bg-white"
                    >
                      <span className="font-bold">
                        {result.similarity.toFixed(3)}
                      </span>
                      <span>{result.sentence}</span>
                    </div>
                  ))}
                </div>
              </div>
            </>
          )}
        </div>
      </div>
    </div>
  );
}

export default App;


----- .\adaptive-retrieval\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;


----- .\adaptive-retrieval\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\adaptive-retrieval\src\worker.js -----

import { pipeline } from "@huggingface/transformers";

class MyFeatureExtractionPipeline {
  static task = "feature-extraction";
  static model = "nomic-ai/nomic-embed-text-v1.5";
  static instance = null;

  static async getInstance(progress_callback = null) {
    this.instance ??= pipeline(this.task, this.model, {
      dtype: "q8",
      progress_callback,
    });

    return this.instance;
  }
}

// https://huggingface.co/nomic-ai/nomic-embed-text-v1.5#usage
const SEARCH_PREFIX = "search_query: ";
const DOCUMENT_PREFIX = "search_document: ";

// Listen for messages from the main thread
self.addEventListener("message", async (event) => {
  // Retrieve the pipeline. When called for the first time,
  // this will load the pipeline and save it for future use.
  const extractor = await MyFeatureExtractionPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  const { source, text } = event.data;

  const split = [
    SEARCH_PREFIX + source,
    ...text
      .trim()
      .split("\n")
      .map((x) => DOCUMENT_PREFIX + x),
  ];
  const embeddings = await extractor(split, {
    pooling: "mean",
    normalize: true,
  });

  // Send the output back to the main thread
  self.postMessage({ status: "complete", embeddings: embeddings.tolist() });
});


----- .\attention-visualization\src\App.jsx -----

import { useRef, useState, Suspense, useEffect, useMemo } from "react";
import { Canvas, useThree, useFrame, useLoader } from "@react-three/fiber";
import { Text } from "@react-three/drei";
import { Bloom, EffectComposer } from "@react-three/postprocessing";
import { a, useSpring } from "@react-spring/three";
import * as THREE from "three";

// Input image
const EXAMPLE_URL =
  "https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/tiger.jpg";
const IMAGE_HEIGHT = 4;
const MAX_IMAGE_WIDTH = 8;
const IMAGE_PADDING = 1.5;

// Attention heads
const ATTENTION_HEAD_HEIGHT = 2.4;
const FONT_SIZE = 0.2;
const X_SPACING = 0.4;
const Y_SPACING = ATTENTION_HEAD_HEIGHT - 0.25;
const Z_SPACING = 2;
const HOVER_PADDING = Y_SPACING + 2;
const LAYER_SPACING = 0.25;

// Scene
const START_PADDING = 0;
const END_PADDING = 1;
const TEXT_PADDING = 2;

// Camera
const ZOOM_DISTANCE = 3.5;
const CAMERA_ANGLE = (Math.PI * 5) / 12;
const CAMERA_DISTANCE = 16;
const DEFAULT_CAMERA_POSITION = [
  -CAMERA_DISTANCE * Math.cos(CAMERA_ANGLE),
  3.5,
  CAMERA_DISTANCE * Math.sin(CAMERA_ANGLE),
];
const TRANSLATE_ZONE_WIDTH = 0.5;
const TRANSLATE_SPEED = 12;

// Misc.
const GRID_SIZE = 400;
const TRANSITION_ALPHA = 0.05;

function AttentionHead({
  position,
  rotation,
  text,
  index,
  activeHead,
  setActiveHead,
  image,
}) {
  const groupRef = useRef();
  const texture = useMemo(() => new THREE.CanvasTexture(image), [image]);
  const width = useMemo(
    () => (ATTENTION_HEAD_HEIGHT * texture.image.width) / texture.image.height,
    [texture],
  );
  const [hovered, setHovered] = useState(false);
  const active = useMemo(
    () => hovered || activeHead === index,
    [hovered, activeHead, index],
  );
  const { positionY } = useSpring({
    positionY: active ? position[1] + Y_SPACING : position[1],
    config: { mass: 1, tension: 280, friction: 60 },
  });

  useEffect(() => {
    if (!groupRef.current) return;
    active
      ? groupRef.current.traverse((o) => o.isMesh && o.layers.enable(1))
      : groupRef.current.traverse((o) => o.isMesh && o.layers.disable(1));
  }, [active]);

  const visible = activeHead === null || activeHead === index;

  return (
    <a.group
      ref={groupRef}
      position-x={position[0]}
      position-y={positionY}
      position-z={position[2]}
      rotation={rotation}
      visible={visible}
      onPointerOver={(e) => {
        if (!visible) return;
        e.stopPropagation();
        setHovered(true);
      }}
      onPointerOut={(e) => {
        if (!visible) return;
        e.stopPropagation();
        setHovered(false);
      }}
      onClick={(e) => {
        if (activeHead !== index) {
          setActiveHead(null);
        }
        if (!visible) return;
        e.stopPropagation();
        setActiveHead(activeHead === index ? null : index);
      }}
    >
      <Text
        position={[
          -width / 2,
          ATTENTION_HEAD_HEIGHT / 2 + FONT_SIZE,
          2e-3, // Small epsilon to prevent z-fighting
        ]}
        fontSize={FONT_SIZE}
        color="#fff"
        anchorX="left"
        anchorY="top"
        fillOpacity={visible ? 1 : 0}
        raycast={() => null}
        lineHeight={0.5}
      >
        {text}
      </Text>

      <a.mesh position={[0, 0, 0]} raycast={() => null}>
        <planeGeometry args={[width, ATTENTION_HEAD_HEIGHT]} />
        <meshStandardMaterial map={texture} side={THREE.DoubleSide} />
      </a.mesh>

      <a.mesh
        position={[
          0,
          -(HOVER_PADDING - ATTENTION_HEAD_HEIGHT) / 2,
          1e-3, // Small epsilon to prevent z-fighting
        ]}
      >
        <planeGeometry
          args={[width, 2 * ATTENTION_HEAD_HEIGHT + HOVER_PADDING]}
        />
        <meshStandardMaterial
          color="white"
          transparent
          opacity={0}
          side={THREE.DoubleSide}
        />
      </a.mesh>
    </a.group>
  );
}

function AttentionHeads({ attentionData, activeHead, setActiveHead }) {
  return (
    <group position-y={1}>
      {attentionData.map((data, i) => (
        <AttentionHead
          key={i}
          index={i}
          activeHead={activeHead}
          setActiveHead={setActiveHead}
          position={data.position}
          rotation={[0, 0, 0]}
          text={data.label}
          image={data.image}
        />
      ))}
    </group>
  );
}

function SceneImage({ image, onImageChange }) {
  const texture = useLoader(THREE.TextureLoader, image);
  const [image_width, image_height] = useMemo(() => {
    const ar = texture.source.data.width / texture.source.data.height;
    let w = ar * IMAGE_HEIGHT;
    let h = IMAGE_HEIGHT;
    if (w > MAX_IMAGE_WIDTH) {
      w = MAX_IMAGE_WIDTH;
      h = w / ar;
    }
    return [w, h];
  }, [texture]);

  const handleClick = () => {
    const input = document.createElement("input");
    input.type = "file";
    input.accept = ".png,.jpg,.jpeg,.gif,.bmp,.webp";
    input.onchange = (e) => {
      if (e.target.files?.[0]) {
        onImageChange(URL.createObjectURL(e.target.files[0]));
      }
    };
    input.click();
  };

  return (
    <Suspense fallback={null}>
      <mesh
        position={[0 - image_width / 2 - IMAGE_PADDING, image_height / 2, 0]}
        onClick={handleClick}
      >
        <planeGeometry args={[image_width, image_height]} />
        <meshBasicMaterial map={texture} />
      </mesh>
    </Suspense>
  );
}

function CameraAnimator({
  start,
  end,
  attentionData,
  activeHead,
  mouseActive,
  mousePosition,
}) {
  const { camera } = useThree();
  const [sceneCenter, setSceneCenter] = useState([0, 0, 0]);
  const center =
    activeHead !== null
      ? attentionData[activeHead].position.slice()
      : [0, 0, 0];

  center[1] += Y_SPACING;
  let targetPosition;

  if (activeHead !== null) {
    center[1] += 1;
    targetPosition = center.slice();
    targetPosition[2] += ZOOM_DISTANCE;
  } else {
    targetPosition = DEFAULT_CAMERA_POSITION.slice();
    for (let i = 0; i < 3; ++i) {
      center[i] += sceneCenter[i];
      targetPosition[i] += sceneCenter[i];
    }
  }

  useEffect(() => {
    setSceneCenter([end + END_PADDING, 0, 0]);
  }, [end]);

  useFrame((state, delta) => {
    if (!mouseActive) return;
    const a = TRANSLATE_SPEED; // max speed
    const b = TRANSLATE_ZONE_WIDTH; // deadzone
    const c = 2; // acceleration
    const f = (x) => a * ((x ** 2 - b ** 2) / (1 - b ** 2)) ** c;
    if (Math.abs(mousePosition.x) >= b) {
      const value = f(mousePosition.x);
      setSceneCenter((prev) => {
        const newCenter = [...prev];
        newCenter[0] += value * delta * Math.sign(mousePosition.x); // Update x position
        newCenter[0] = Math.max(
          Math.min(newCenter[0], end + END_PADDING),
          start - START_PADDING,
        ); // Clamp x position
        return newCenter;
      });
    }
  });

  const spring = useSpring({
    pos: targetPosition,
    config: { mass: 1, tension: 500, friction: 20 },
  });
  const springLookAt = useSpring({
    lookAt: center,
    config: { mass: 1, tension: 500, friction: 20 },
  });
  const targetLookAt = useRef(new THREE.Vector3(0, 0, 0));

  useFrame(() => {
    camera.position.lerp(
      new THREE.Vector3(...spring.pos.get()),
      TRANSITION_ALPHA,
    );
    targetLookAt.current.lerp(
      new THREE.Vector3(...springLookAt.lookAt.get()),
      TRANSITION_ALPHA,
    );
    camera.lookAt(targetLookAt.current);
  });
  return null;
}

function AttentionVisualization({
  label,
  score,
  attentionData,
  image,
  onImageChange,
}) {
  const [dots, setDots] = useState(".");
  useEffect(() => {
    let idx = 1;
    const timer = setInterval(() => {
      idx = (idx % 3) + 1;
      setDots(".".repeat(idx));
    }, 500);
    return () => clearInterval(timer);
  }, []);

  const [start, end] = useMemo(
    () =>
      attentionData.length > 0
        ? attentionData.reduce(
            ([min, max], data) => [
              Math.min(min, data.position[0] - data.width / 2),
              Math.max(max, data.position[0] + data.width / 2),
            ],
            [Infinity, -Infinity],
          )
        : [0, 0],
    [attentionData],
  );

  const [activeHead, setActiveHead] = useState(null);
  const [mousePosition, setMousePosition] = useState({ x: 0, y: 0 });
  const [mouseActive, setMouseActive] = useState(true);

  useEffect(() => {
    const handleMouseMove = (event) => {
      const { clientX, clientY } = event;
      const width = window.innerWidth;
      const height = window.innerHeight;
      setMousePosition({
        x: (clientX / width - 0.5) * 2,
        y: (clientY / height - 0.5) * 2,
      });
    };
    const handleMouseLeave = () => setMouseActive(false);
    const handleMouseEnter = () => setMouseActive(true);

    // NOTE: Certain browsers, like Firefox, require us to attach the event listeners to `document.documentElement`
    document.documentElement.addEventListener("mouseleave", handleMouseLeave);
    document.documentElement.addEventListener("mouseenter", handleMouseEnter);
    document.documentElement.addEventListener("mousemove", handleMouseMove);

    return () => {
      document.documentElement.removeEventListener(
        "mouseleave",
        handleMouseLeave,
      );
      document.documentElement.removeEventListener(
        "mouseenter",
        handleMouseEnter,
      );
      document.documentElement.removeEventListener(
        "mousemove",
        handleMouseMove,
      );
    };
  }, []);

  return (
    <Canvas
      camera={{ fov: 45 }}
      gl={{ antialias: true, toneMapping: THREE.NoToneMapping }}
    >
      <CameraAnimator
        start={start}
        end={end}
        attentionData={attentionData}
        activeHead={activeHead}
        mouseActive={mouseActive}
        mousePosition={mousePosition}
      />
      <color attach="background" args={["#040b1b"]} />
      <gridHelper args={[GRID_SIZE, GRID_SIZE, "white", "gray"]} />
      {image && <SceneImage image={image} onImageChange={onImageChange} />}

      <Suspense fallback={null}>
        {!image && (
          <Text
            position={[-1.5, IMAGE_HEIGHT / 2, 0]}
            fontSize={1}
            color="#fff"
            anchorX="left"
            fillOpacity={1}
            raycast={() => null}
          >
            Loading{dots}
          </Text>
        )}
        <AttentionHeads
          attentionData={attentionData}
          activeHead={activeHead}
          setActiveHead={setActiveHead}
        />
      </Suspense>
      {label && (
        <Text
          position={[end + TEXT_PADDING, 1.25 * ATTENTION_HEAD_HEIGHT, 0]}
          fontSize={1}
          color="#fff"
          anchorX="left"
          fillOpacity={1}
          raycast={() => null}
        >
          {label}
        </Text>
      )}
      {score && (
        <Text
          position={[end + TEXT_PADDING, 0.75 * ATTENTION_HEAD_HEIGHT, 0]}
          fontSize={0.8}
          color="#fff"
          anchorX="left"
          fillOpacity={1}
          raycast={() => null}
        >
          {" ".repeat((label?.length || 0) * (2 / 3))}({score.toFixed(2)}%)
        </Text>
      )}
      <EffectComposer>
        <Bloom
          intensity={0.2}
          luminanceThreshold={0.1}
          luminanceSmoothing={0.8}
        />
      </EffectComposer>
      <ambientLight intensity={2} />
    </Canvas>
  );
}

export default function App() {
  const [result, setResult] = useState(null);
  const attentionData = useMemo(() => {
    if (!result) return [];
    return result.attentions.map(({ layer, head, num_heads, image }) => {
      const width = (ATTENTION_HEAD_HEIGHT * image.width) / image.height;
      const depthOffset = (num_heads - 1) * X_SPACING;
      const xOffset =
        width / 2 + depthOffset + layer * (width + depthOffset + LAYER_SPACING);
      const position = [
        xOffset - head * X_SPACING,
        0.5 * ATTENTION_HEAD_HEIGHT - 1,
        ((num_heads + 1) / 2 - head - 1) * Z_SPACING,
      ];
      const label = `Layer ${layer + 1}, Head ${head + 1}`;
      return { position, label, image, width };
    });
  }, [result]);

  const label = useMemo(() => result?.label, [result]);
  const score = useMemo(() => result?.score, [result]);

  const [state, setState] = useState(null);
  const [image, setImage] = useState(null);
  const worker = useRef(null);

  const handleImageChange = (image) => {
    setImage(image);
    worker.current.postMessage({ image });
  };

  useEffect(() => {
    // Initialize worker on mount
    worker.current ??= new Worker(new URL("./worker.js", import.meta.url), {
      type: "module",
    });

    // NOTE: Certain browsers handle error messages differently, so to ensure
    // compatibility, we need to handle errors in both `message` and `error` events.
    const onMessage = ({ data }) => {
      switch (data.type) {
        case "status":
        case "error":
          setState(data);
          break;
        case "output":
          setResult(data.result);
          break;
      }
    };
    const onError = (e) => setState({ type: "error", error: e.message });

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessage);
    worker.current.addEventListener("error", onError);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessage);
      worker.current.removeEventListener("error", onError);
    };
  }, []);

  useEffect(() => {
    if (
      state &&
      state.type === "status" &&
      state.status === "ready" &&
      image === null
    ) {
      // Run on first load
      handleImageChange(EXAMPLE_URL);
    }
  }, [state, image]);

  return (
    <div className="w-screen supports-[height:100cqh]:h-[100cqh] supports-[height:100svh]:h-[100svh] bg-black">
      {state?.type === "error" ? (
        <div className="absolute top-0 left-0 w-full h-full flex items-center justify-center z-1 text-red-600 text-3xl px-8 backdrop-blur-lg bg-black/75 text-center">
          {state.error}
        </div>
      ) : (
        <AttentionVisualization
          label={label}
          score={score}
          attentionData={attentionData}
          image={image}
          onImageChange={handleImageChange}
        />
      )}
      <div className="absolute bottom-4 right-4 z-1 text-white text-2xl">
        <a
          href="https://github.com/huggingface/transformers.js-examples/tree/main/attention-visualization"
          target="_blank"
          rel="noopener noreferrer"
          className="w-10 h-10 cursor-pointer bg-white rounded-full shadow-md flex items-center justify-center hover:bg-gray-100"
          title="View source code on GitHub"
        >
          <svg
            className="w-7 h-7 text-gray-800"
            aria-hidden="true"
            xmlns="http://www.w3.org/2000/svg"
            fill="currentColor"
            viewBox="0 0 24 24"
          >
            <path
              fillRule="evenodd"
              d="M12.006 2a9.847 9.847 0 0 0-6.484 2.44 10.32 10.32 0 0 0-3.393 6.17 10.48 10.48 0 0 0 1.317 6.955 10.045 10.045 0 0 0 5.4 4.418c.504.095.683-.223.683-.494 0-.245-.01-1.052-.014-1.908-2.78.62-3.366-1.21-3.366-1.21a2.711 2.711 0 0 0-1.11-1.5c-.907-.637.07-.621.07-.621.317.044.62.163.885.346.266.183.487.426.647.71.135.253.318.476.538.655a2.079 2.079 0 0 0 2.37.196c.045-.52.27-1.006.635-1.37-2.219-.259-4.554-1.138-4.554-5.07a4.022 4.022 0 0 1 1.031-2.75 3.77 3.77 0 0 1 .096-2.713s.839-.275 2.749 1.05a9.26 9.26 0 0 1 5.004 0c1.906-1.325 2.74-1.05 2.74-1.05.37.858.406 1.828.101 2.713a4.017 4.017 0 0 1 1.029 2.75c0 3.939-2.339 4.805-4.564 5.058a2.471 2.471 0 0 1 .679 1.897c0 1.372-.012 2.477-.012 2.814 0 .272.18.592.687.492a10.05 10.05 0 0 0 5.388-4.421 10.473 10.473 0 0 0 1.313-6.948 10.32 10.32 0 0 0-3.39-6.165A9.847 9.847 0 0 0 12.007 2Z"
              clipRule="evenodd"
            />
          </svg>
        </a>
      </div>
    </div>
  );
}


----- .\attention-visualization\src\index.css -----

@import "tailwindcss";


----- .\attention-visualization\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import "./index.css";
import App from "./App.jsx";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\attention-visualization\src\worker.js -----

import {
  AutoProcessor,
  AutoModelForImageClassification,
  interpolate_4d,
  RawImage,
  softmax,
} from "@huggingface/transformers";

export async function supportsWebGPU() {
  try {
    if (!navigator.gpu) return false;
    return !!(await navigator.gpu.requestAdapter());
  } catch (e) {
    return false;
  }
}

const webgpu = await supportsWebGPU();
// Load model and processor
const model_id = "onnx-community/dinov2-with-registers-small-with-attentions";
const model = await AutoModelForImageClassification.from_pretrained(model_id, {
  device: webgpu ? "webgpu" : "wasm",
  dtype: webgpu ? "q4" : "q8",
}).catch((error) => {
  console.error(error);
  self.postMessage({ type: "error", error: error.toString() });
  throw error;
});
const processor = await AutoProcessor.from_pretrained(model_id).catch(
  (error) => {
    console.error(error);
    self.postMessage({ type: "error", error: error.toString() });
    throw error;
  },
);

self.postMessage({ type: "status", status: "ready" });

const MAX_IMAGE_SIZE = 800;
self.onmessage = async (event) => {
  const { image } = event.data;
  self.postMessage({ type: "status", status: "read_image" });

  let raw_image = await RawImage.read(image);
  if (raw_image.width > MAX_IMAGE_SIZE || raw_image.height > MAX_IMAGE_SIZE) {
    const aspect_ratio = raw_image.width / raw_image.height;
    let new_width, new_height;
    if (raw_image.width > raw_image.height) {
      new_width = MAX_IMAGE_SIZE;
      new_height = Math.round(MAX_IMAGE_SIZE / aspect_ratio);
    } else {
      new_height = MAX_IMAGE_SIZE;
      new_width = Math.round(MAX_IMAGE_SIZE * aspect_ratio);
    }
    raw_image = await raw_image.resize(new_width, new_height);
  }

  // Pre-process image
  const inputs = await processor(raw_image);

  self.postMessage({ type: "status", status: "run_model" });

  // Perform inference
  const { logits, attentions } = await model(inputs);

  self.postMessage({ type: "status", status: "postprocess" });

  // Get the predicted class
  const scores = logits[0];
  const probabilities = softmax(scores.data);
  const cls = scores.argmax().item();

  const score = probabilities[cls] * 100;
  const label = model.config.id2label[cls];
  console.log(`Predicted class: ${label}`);

  // Set config values
  const patch_size = model.config.patch_size;
  const [width, height] = inputs.pixel_values.dims.slice(-2);
  const w_featmap = Math.floor(width / patch_size);
  const h_featmap = Math.floor(height / patch_size);
  const num_heads = model.config.num_attention_heads;
  const num_cls_tokens = 1;
  const num_register_tokens = model.config.num_register_tokens ?? 0;

  // Visualize attention maps
  const output = [];
  for (let i = 0; i < attentions.length; ++i) {
    const layer = attentions[i];

    const selected_attentions = layer
      .slice(0, null, 0, [num_cls_tokens + num_register_tokens, null])
      .view(num_heads, 1, w_featmap, h_featmap);

    const upscaled = await interpolate_4d(selected_attentions, {
      size: [width, height],
      mode: "nearest",
    });

    for (let j = 0; j < num_heads; ++j) {
      const head_attentions = upscaled[j];
      const minval = head_attentions.min().item();
      const maxval = head_attentions.max().item();
      const map = RawImage.fromTensor(
        head_attentions
          .sub_(minval)
          .div_(maxval - minval)
          .mul_(255)
          .to("uint8"),
      ).rgba();
      const image = await createImageBitmap(
        new ImageData(map.data, map.width, map.height),
        {
          imageOrientation: "flipY",
        },
      );
      output.push({
        layer: i,
        head: j,
        num_heads,
        image,
      });
    }
  }

  self.postMessage({
    type: "output",
    result: {
      attentions: output,
      label,
      score,
    },
  });
};


----- .\code-completion\src\App.jsx -----

import { useState, useRef, useEffect } from "react";
import Editor from "@monaco-editor/react";
import Progress from "./components/Progress";

const MODELS = [
  "onnx-community/Qwen2.5-Coder-0.5B-ONNX",
  "Xenova/tiny_starcoder_py",
  "Xenova/codegen-350M-mono",
  // 'Xenova/starcoderbase-1b',
];
function App() {
  // Editor setup
  const monaco = useRef(null);
  const [monacoReady, setMonacoReady] = useState(false);
  const [language, setLanguage] = useState("python"); // Only allow python for now

  // Model loading
  const [ready, setReady] = useState(null);
  const [progressItems, setProgressItems] = useState([]);

  // Inputs and outputs
  const [model, setModel] = useState(MODELS[0]);
  const [maxNewTokens, setMaxNewTokens] = useState(42);
  const [code, setCode] = useState(
    '\ndef fib(n):\n    """Calculates the nth Fibonacci number"""\n',
  );

  // Generation parameters
  const [temperature, setTemperature] = useState(0.5);
  const [topK, setTopK] = useState(5);
  const [doSample, setDoSample] = useState(false);

  // Create a reference to the worker object.
  const worker = useRef(null);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    if (!worker.current) {
      // Create the worker if it does not yet exist.
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "initiate":
          // Model file start load: add a new progress item to the list.
          setReady(false);
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setReady(true);
          break;

        case "update":
          // Generation update: update the output text.
          setCode((prev) => prev + e.data.output);
          break;
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () =>
      worker.current.removeEventListener("message", onMessageReceived);
  });

  useEffect(() => {
    const m = monaco.current;
    if (!m) return;

    let actionRegistration = m.addAction({
      id: "generate",
      label: "Generate",
      contextMenuGroupId: "0_custom",
      run: () => {
        const val = m.getValue();
        if (!val) return;

        worker.current.postMessage({
          model,
          text: val,
          max_new_tokens: maxNewTokens,
          temperature,
          top_k: topK,
          do_sample: doSample,
        });
      },
    });

    // Define a cleanup function for when the component is unmounted.
    return () => actionRegistration.dispose();
  }, [monacoReady, model, maxNewTokens, temperature, topK, doSample]);

  const showLoading = ready === false || progressItems.length > 0;

  return (
    <div className="flex h-screen w-screen">
      <div
        className={`gap-1 z-50 top-0 left-0 absolute w-full h-full transition-all px-8 flex flex-col justify-center text-center bg-black bg-opacity-50 backdrop-blur-sm ${
          showLoading
            ? "opacity-100 pointer-events-auto"
            : "opacity-0 pointer-events-none"
        }`}
      >
        {ready === false && (
          <label className="text-3xl p-3 text-white">Loading model...</label>
        )}
        {progressItems.map((data) => (
          <div key={data.file}>
            <Progress data={data} />
          </div>
        ))}
      </div>
      <div>
        <Editor
          width="calc(100vw - 360px)"
          language={language}
          value={code}
          onChange={(value) => setCode(value)}
          theme="vs-dark"
          onMount={(m) => {
            monaco.current = m;
            setMonacoReady(true);
          }}
          options={{
            fontSize: 24,
          }}
        />
      </div>
      <div className="flex-grow bg-gray-900 text-white p-4 flex flex-col overflow-y-auto">
        <h2 className="text-2xl font-semibold text-center mb-2">
          In-browser code completion
        </h2>
        <div className="text-center">
          Made with&nbsp;
          <a
            className="text-blue-400 underline"
            href="https://github.com/huggingface/transformers.js"
            target="_blank"
            rel="noreferrer"
          >
            ðŸ¤— Transformers.js
          </a>
        </div>

        <label className="mt-3">Instructions:</label>
        <ol className="list-decimal	list-outside pl-4">
          <li>
            Choose a model from the dropdown and modify the generation settings
          </li>
          <li>Write some code in the editor</li>
          <li>Right-click and select "Generate"</li>
        </ol>

        <label className="mt-3">Model:</label>
        <select
          value={model}
          onChange={(e) => setModel(e.target.value)}
          className="p-2.5 bg-gray-800 border border-gray-700 text-gray-100 rounded-lg"
        >
          {MODELS.map((value, i) => {
            return (
              <option key={i} value={value}>
                {value}
              </option>
            );
          })}
        </select>

        <div className="mt-3 flex justify-between">
          <label>Max new tokens:</label>
          <label>{maxNewTokens}</label>
        </div>
        <input
          type="range"
          min="1"
          max="512"
          value={maxNewTokens}
          onChange={(event) => {
            const newValue = parseInt(event.target.value);
            setMaxNewTokens(newValue);
          }}
          className="w-full"
        />

        <div className="mt-3 flex items-center">
          <input
            id="default-checkbox"
            type="checkbox"
            value={doSample}
            onInput={(event) => {
              setDoSample(event.target.checked);
            }}
            className="w-4 h-4 text-blue-600 rounded focus:ring focus:ring-blue-600 bg-gray-800 border-gray-700"
          />
          <label htmlFor="default-checkbox" className="ml-2 font-medium">
            Sample
          </label>
        </div>

        <div
          className={`mt-3 flex justify-between ${
            doSample ? "opacity-100" : "opacity-50"
          }`}
        >
          <label>Top K:</label>
          <label>{topK}</label>
        </div>
        <input
          type="range"
          min="1"
          max="50"
          value={topK}
          onChange={(e) => setTopK(parseInt(e.target.value))}
          disabled={!doSample}
          className="w-full"
        />

        <div
          className={`mt-3 flex justify-between ${
            doSample ? "opacity-100" : "opacity-50"
          }`}
        >
          <label>Temperature:</label>
          <label>{temperature}</label>
        </div>
        <input
          type="range"
          min="0"
          step="0.05"
          max="1"
          value={temperature}
          onChange={(event) => {
            const newValue = Number(event.target.value);
            setTemperature(newValue);
          }}
          disabled={!doSample}
          className="w-full"
        />
        <div className="flex gap-2 justify-center mt-auto">
          <svg
            className="w-6 h-6 text-white"
            aria-hidden="true"
            xmlns="http://www.w3.org/2000/svg"
            fill="currentColor"
            viewBox="0 0 20 20"
          >
            <path
              fillRule="evenodd"
              d="M10 .333A9.911 9.911 0 0 0 6.866 19.65c.5.092.678-.215.678-.477 0-.237-.01-1.017-.014-1.845-2.757.6-3.338-1.169-3.338-1.169a2.627 2.627 0 0 0-1.1-1.451c-.9-.615.07-.6.07-.6a2.084 2.084 0 0 1 1.518 1.021 2.11 2.11 0 0 0 2.884.823c.044-.503.268-.973.63-1.325-2.2-.25-4.516-1.1-4.516-4.9A3.832 3.832 0 0 1 4.7 7.068a3.56 3.56 0 0 1 .095-2.623s.832-.266 2.726 1.016a9.409 9.409 0 0 1 4.962 0c1.89-1.282 2.717-1.016 2.717-1.016.366.83.402 1.768.1 2.623a3.827 3.827 0 0 1 1.02 2.659c0 3.807-2.319 4.644-4.525 4.889a2.366 2.366 0 0 1 .673 1.834c0 1.326-.012 2.394-.012 2.72 0 .263.18.572.681.475A9.911 9.911 0 0 0 10 .333Z"
              clipRule="evenodd"
            />
          </svg>

          <a
            className="text-white font-normal underline underline-offset-1"
            href="https://github.com/huggingface/transformers.js-examples/tree/main/code-completion"
            target="_blank"
            rel="noreferrer"
          >
            Source code
          </a>
        </div>
      </div>
    </div>
  );
}

export default App;


----- .\code-completion\src\index.css -----

@import "tailwindcss";


----- .\code-completion\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import "./index.css";
import App from "./App.jsx";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\code-completion\src\worker.js -----

import { pipeline, TextStreamer } from "@huggingface/transformers";

/**
 * This class uses the Singleton pattern to ensure that only one instance of the pipeline is loaded.
 */
class CodeCompletionPipeline {
  static task = "text-generation";
  static model = null;
  static instance = null;

  static async getInstance(progress_callback = null) {
    this.instance ??= pipeline(this.task, this.model, { progress_callback });

    return this.instance;
  }
}

// Listen for messages from the main thread
self.addEventListener("message", async (event) => {
  const {
    model,
    text,
    max_new_tokens,

    // Generation parameters
    temperature,
    top_k,
    do_sample,
  } = event.data;

  if (CodeCompletionPipeline.model !== model) {
    // Invalidate model if different
    CodeCompletionPipeline.model = model;

    if (CodeCompletionPipeline.instance !== null) {
      (await CodeCompletionPipeline.getInstance()).dispose();
      CodeCompletionPipeline.instance = null;
    }
  }

  // Retrieve the code-completion pipeline. When called for the first time,
  // this will load the pipeline and save it for future use.
  const generator = await CodeCompletionPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  const streamer = new TextStreamer(generator.tokenizer, {
    skip_prompt: true,
    callback_function: (x) => {
      console.log(x);
      self.postMessage({
        status: "update",
        output: x,
      });
    },
  });
  // self.postMessage({
  //     status: 'update',
  //     output: generator.tokenizer.decode(x[0].output_token_ids, { skip_special_tokens: true })
  // });

  // Actually perform the code-completion
  console.log(text);
  const output = await generator(text, {
    max_new_tokens,
    temperature,
    top_k,
    do_sample,
    streamer,
  });

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: output,
  });
  console.log(output);
});


----- .\code-completion\src\components\Progress.jsx -----

function formatBytes(bytes, decimals = 0) {
  const sizes = ["Bytes", "KB", "MB", "GB", "TB"];
  if (bytes === 0) return "0 Bytes";
  const i = parseInt(Math.floor(Math.log(bytes) / Math.log(1000)), 10);
  const rounded = (bytes / Math.pow(1000, i)).toFixed(decimals);
  return rounded + " " + sizes[i];
}

export default function Progress({ data }) {
  const progress = data.progress ?? 0;
  const text = data.file;

  const a = formatBytes(data.loaded ?? 0);
  const b = formatBytes(data.total ?? 0);
  return (
    <div className="relative text-white text-lg rounded-lg overflow-hidden">
      <div
        className="absolute inset-0 z-0 bg-blue-500"
        style={{ width: `${progress}%` }}
      ></div>
      <div className="relative z-10 p-1">
        {text} ({`${a} / ${b}`})
      </div>
    </div>
  );
}


----- .\cross-encoder\src\App.jsx -----

import { useState, useRef, useEffect, useCallback } from "react";

const PLACEHOLDER_TEXTS = [
  "'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.",
  "The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.",
  "Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.",
  "Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.",
  "The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.",
  "'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.",
].sort(() => Math.random() - 0.5);

function App() {
  const [status, setStatus] = useState("idle");

  const [query, setQuery] = useState(`Who wrote 'To Kill a Mockingbird'?`);
  const [documents, setDocuments] = useState(PLACEHOLDER_TEXTS.join("\n"));

  const [results, setResults] = useState([]);

  // Create a reference to the worker object.
  const worker = useRef(null);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    worker.current ??= new Worker(new URL("./worker.js", import.meta.url), {
      type: "module",
    });

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      const status = e.data.status;
      if (e.data.file?.endsWith(".onnx")) {
        if (status === "initiate") {
          setStatus("loading");
        } else if (status === "done") {
          setStatus("ready");
        }
      } else if (status === "complete") {
        setResults(e.data.output);
        setStatus("idle");
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () =>
      worker.current.removeEventListener("message", onMessageReceived);
  }, []);

  const run = useCallback(() => {
    setStatus("processing");
    worker.current.postMessage({
      query,
      documents,
    });
  }, [query, documents]);

  const busy = status !== "idle";

  return (
    <div className="flex flex-col h-screen w-screen p-8">
      <h1 className="text-2xl md:text-4xl font-bold text-center mb-1">
        Reranking w/ The Crispy mixedbread Rerank Models
      </h1>
      <p className="text-lg md:text-xl font-medium text-center mb-2">
        Powered by{" "}
        <a
          href="https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1"
          target="_blank"
          rel="noreferrer"
        >
          mxbai-rerank-xsmall-v1
        </a>{" "}
        and{" "}
        <a
          href="http://huggingface.co/docs/transformers.js"
          target="_blank"
          rel="noreferrer"
        >
          ðŸ¤— Transformers.js
        </a>
      </p>
      <div className="flex-grow flex flex-wrap p-4">
        <div className="flex flex-col items-center gap-y-1 w-full md:w-1/2">
          <label className="text-lg font-medium">Query</label>
          <textarea
            placeholder="Enter query."
            className="border w-full p-1 resize-none overflow-hidden h-10"
            value={query}
            onChange={(e) => {
              setQuery(e.target.value);
              setResults([]);
            }}
          ></textarea>
          <label className="text-lg font-medium mt-1">Documents</label>
          <textarea
            placeholder="Enter documents to compare with the query. One sentence per line."
            className="border w-full p-1 h-full resize-none"
            value={documents}
            onChange={(e) => {
              setDocuments(e.target.value);
              setResults([]);
            }}
          ></textarea>

          <button
            className="border py-1 px-2 bg-green-400 rounded text-white text-lg font-medium disabled:opacity-50 disabled:cursor-not-allowed cursor-pointer"
            disabled={busy}
            onClick={run}
          >
            {!busy
              ? "Rerank"
              : status === "loading"
                ? "Model loading..."
                : "Processing"}
          </button>
        </div>
        <div className="flex flex-col items-center w-full md:w-1/2 gap-y-1">
          {results.length > 0 && (
            <>
              <div className="w-full flex flex-col gap-y-1">
                <label className="text-lg font-medium text-center">
                  Results
                </label>
                <div className="flex flex-col gap-y-1">
                  {results.map((result, i) => (
                    <div key={i} className="flex gap-x-2 border mx-2 p-1">
                      <span className="font-bold">
                        {result.score.toFixed(3)}
                      </span>
                      <span>{result.text}</span>
                    </div>
                  ))}
                </div>
              </div>
            </>
          )}
        </div>
      </div>
    </div>
  );
}

export default App;


----- .\cross-encoder\src\index.css -----

@import "tailwindcss";


----- .\cross-encoder\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\cross-encoder\src\worker.js -----

import {
  AutoTokenizer,
  AutoModelForSequenceClassification,
} from "@huggingface/transformers";

class CrossEncoderSingleton {
  static model_id = "mixedbread-ai/mxbai-rerank-xsmall-v1";
  static model = null;
  static tokenizer = null;

  static async getInstance(progress_callback) {
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id);

    this.model ??= AutoModelForSequenceClassification.from_pretrained(
      this.model_id,
      {
        progress_callback,
      },
    );

    return Promise.all([this.tokenizer, this.model]);
  }
}

// Listen for messages from the main thread
self.addEventListener("message", async (event) => {
  // Retrieve the pipeline. When called for the first time,
  // this will load the pipeline and save it for future use.
  const [tokenizer, model] = await CrossEncoderSingleton.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  const { query, documents } = event.data;

  const docs = documents.trim().split("\n");

  const inputs = tokenizer(new Array(docs.length).fill(query), {
    text_pair: docs,
    padding: true,
    truncation: true,
  });
  const { logits } = await model(inputs);
  const output = logits
    .sigmoid()
    .tolist()
    .map(([score], i) => ({
      corpus_id: i,
      score,
      text: docs[i],
    }))
    .sort((a, b) => b.score - a.score);

  // Send the output back to the main thread
  self.postMessage({ status: "complete", output });
});


----- .\deepseek-r1-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import Chat from "./components/Chat";
import ArrowRightIcon from "./components/icons/ArrowRightIcon";
import StopIcon from "./components/icons/StopIcon";
import Progress from "./components/Progress";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;
const STICKY_SCROLL_THRESHOLD = 120;
const EXAMPLES = [
  "Solve the equation x^2 - 3x + 2 = 0",
  "Lily is three times older than her son. In 15 years, she will be twice as old as him. How old is she now?",
  "Write python code to compute the nth fibonacci number.",
];

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const textareaRef = useRef(null);
  const chatContainerRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);
  const [isRunning, setIsRunning] = useState(false);

  // Inputs and outputs
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState([]);
  const [tps, setTps] = useState(null);
  const [numTokens, setNumTokens] = useState(null);

  function onEnter(message) {
    setMessages((prev) => [...prev, { role: "user", content: message }]);
    setTps(null);
    setIsRunning(true);
    setInput("");
  }

  function onInterrupt() {
    // NOTE: We do not set isRunning to false here because the worker
    // will send a 'complete' message when it is done.
    worker.current.postMessage({ type: "interrupt" });
  }

  useEffect(() => {
    resizeInput();
  }, [input]);

  function resizeInput() {
    if (!textareaRef.current) return;

    const target = textareaRef.current;
    target.style.height = "auto";
    const newHeight = Math.min(Math.max(target.scrollHeight, 24), 200);
    target.style.height = `${newHeight}px`;
  }

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    if (!worker.current) {
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
      worker.current.postMessage({ type: "check" }); // Do a feature check
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "start":
          {
            // Start generation
            setMessages((prev) => [
              ...prev,
              { role: "assistant", content: "" },
            ]);
          }
          break;

        case "update":
          {
            // Generation update: update the output text.
            // Parse messages
            const { output, tps, numTokens, state } = e.data;
            setTps(tps);
            setNumTokens(numTokens);
            setMessages((prev) => {
              const cloned = [...prev];
              const last = cloned.at(-1);
              const data = {
                ...last,
                content: last.content + output,
              };
              if (data.answerIndex === undefined && state === "answering") {
                // When state changes to answering, we set the answerIndex
                data.answerIndex = last.content.length;
              }
              cloned[cloned.length - 1] = data;
              return cloned;
            });
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsRunning(false);
          break;

        case "error":
          setError(e.data.data);
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
    };
  }, []);

  // Send the messages to the worker thread whenever the `messages` state changes.
  useEffect(() => {
    if (messages.filter((x) => x.role === "user").length === 0) {
      // No user messages yet: do nothing.
      return;
    }
    if (messages.at(-1).role === "assistant") {
      // Do not update if the last message is from the assistant
      return;
    }
    setTps(null);
    worker.current.postMessage({ type: "generate", data: messages });
  }, [messages, isRunning]);

  useEffect(() => {
    if (!chatContainerRef.current || !isRunning) return;
    const element = chatContainerRef.current;
    if (
      element.scrollHeight - element.scrollTop - element.clientHeight <
      STICKY_SCROLL_THRESHOLD
    ) {
      element.scrollTop = element.scrollHeight;
    }
  }, [messages, isRunning]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto items justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {status === null && messages.length === 0 && (
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[400px] text-center">
            <img
              src="logo.png"
              width="80%"
              height="auto"
              className="block drop-shadow-lg bg-transparent"
            ></img>
            <h1 className="text-4xl font-bold mb-1">DeepSeek-R1 WebGPU</h1>
            <h2 className="font-semibold">
              A next-generation reasoning model that runs locally in your
              browser with WebGPU acceleration.
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            <p className="max-w-[510px] mb-4">
              <br />
              You are about to load{" "}
              <a
                href="https://huggingface.co/onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                DeepSeek-R1-Distill-Qwen-1.5B
              </a>
              , a 1.5B parameter reasoning LLM optimized for in-browser
              inference. Everything runs entirely in your browser with{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning no data is sent to a server. Once
              loaded, it can even be used offline. The source code for the demo
              is available on{" "}
              <a
                href="https://github.com/huggingface/transformers.js-examples/tree/main/deepseek-r1-webgpu"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                GitHub
              </a>
              .
            </p>

            {error && (
              <div className="text-red-500 text-center mb-2">
                <p className="mb-1">
                  Unable to load model due to the following error:
                </p>
                <p className="text-sm">{error}</p>
              </div>
            )}

            <button
              className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 cursor-pointer disabled:cursor-not-allowed select-none"
              onClick={() => {
                worker.current.postMessage({ type: "load" });
                setStatus("loading");
              }}
              disabled={status !== null || error !== null}
            >
              Load model
            </button>
          </div>
        </div>
      )}
      {status === "loading" && (
        <>
          <div className="w-full max-w-[500px] text-left mx-auto p-4 bottom-0 mt-auto">
            <p className="text-center mb-1">{loadingMessage}</p>
            {progressItems.map(({ file, progress, total }, i) => (
              <Progress
                key={i}
                text={file}
                percentage={progress}
                total={total}
              />
            ))}
          </div>
        </>
      )}

      {status === "ready" && (
        <div
          ref={chatContainerRef}
          className="overflow-y-auto scrollbar-thin w-full flex flex-col items-center h-full"
        >
          <Chat messages={messages} />
          {messages.length === 0 && (
            <div>
              {EXAMPLES.map((msg, i) => (
                <div
                  key={i}
                  className="m-1 border border-gray-300 dark:border-gray-600 rounded-md p-2 bg-gray-100 dark:bg-gray-700 cursor-pointer"
                  onClick={() => onEnter(msg)}
                >
                  {msg}
                </div>
              ))}
            </div>
          )}
          <p className="text-center text-sm min-h-6 text-gray-500 dark:text-gray-300">
            {tps && messages.length > 0 && (
              <>
                {!isRunning && (
                  <span>
                    Generated {numTokens} tokens in{" "}
                    {(numTokens / tps).toFixed(2)} seconds&nbsp;&#40;
                  </span>
                )}
                {
                  <>
                    <span className="font-medium text-center mr-1 text-black dark:text-white">
                      {tps.toFixed(2)}
                    </span>
                    <span className="text-gray-500 dark:text-gray-300">
                      tokens/second
                    </span>
                  </>
                }
                {!isRunning && (
                  <>
                    <span className="mr-1">&#41;.</span>
                    <span
                      className="underline cursor-pointer"
                      onClick={() => {
                        worker.current.postMessage({ type: "reset" });
                        setMessages([]);
                      }}
                    >
                      Reset
                    </span>
                  </>
                )}
              </>
            )}
          </p>
        </div>
      )}

      <div className="mt-2 border border-gray-300 dark:bg-gray-700 rounded-lg w-[600px] max-w-[80%] max-h-[200px] mx-auto relative mb-3 flex">
        <textarea
          ref={textareaRef}
          className="scrollbar-thin w-[550px] dark:bg-gray-700 px-3 py-4 rounded-lg bg-transparent border-none outline-hidden text-gray-800 disabled:text-gray-400 dark:text-gray-200 placeholder-gray-500 dark:placeholder-gray-400 disabled:placeholder-gray-200 resize-none disabled:cursor-not-allowed"
          placeholder="Type your message..."
          type="text"
          rows={1}
          value={input}
          disabled={status !== "ready"}
          title={status === "ready" ? "Model is ready" : "Model not loaded yet"}
          onKeyDown={(e) => {
            if (
              input.length > 0 &&
              !isRunning &&
              e.key === "Enter" &&
              !e.shiftKey
            ) {
              e.preventDefault(); // Prevent default behavior of Enter key
              onEnter(input);
            }
          }}
          onInput={(e) => setInput(e.target.value)}
        />
        {isRunning ? (
          <div className="cursor-pointer" onClick={onInterrupt}>
            <StopIcon className="h-8 w-8 p-1 rounded-md text-gray-800 dark:text-gray-100 absolute right-3 bottom-3" />
          </div>
        ) : input.length > 0 ? (
          <div className="cursor-pointer" onClick={() => onEnter(input)}>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-800 dark:bg-gray-100 text-white dark:text-black rounded-md absolute right-3 bottom-3`}
            />
          </div>
        ) : (
          <div>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-200 dark:bg-gray-600 text-gray-50 dark:text-gray-800 rounded-md absolute right-3 bottom-3`}
            />
          </div>
        )}
      </div>

      <p className="text-xs text-gray-400 text-center mb-3">
        Disclaimer: Generated content may be inaccurate or false.
      </p>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\deepseek-r1-webgpu\src\index.css -----

@import "tailwindcss";

/* Custom scrollbar styles */
.scrollbar-thin::-webkit-scrollbar {
  width: 0.5rem; /* Equivalent to w-2 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-track {
  border-radius: 9999px; /* Equivalent to rounded-full in Tailwind */
  background-color: #f3f4f6; /* Equivalent to bg-gray-100 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-track.dark {
  background-color: #374151; /* Equivalent to dark:bg-gray-700 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-thumb {
  border-radius: 9999px; /* Equivalent to rounded-full in Tailwind */
  background-color: #d1d5db; /* Equivalent to bg-gray-300 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-thumb:hover {
  background-color: #6b7280; /* Equivalent to bg-gray-500 in Tailwind */
}

/* Animation delay classes */
.animation-delay-200 {
  animation-delay: 200ms;
}

.animation-delay-400 {
  animation-delay: 400ms;
}

/* Overflow wrap class */
.overflow-wrap-anywhere {
  overflow-wrap: anywhere;
}


----- .\deepseek-r1-webgpu\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import "./index.css";
import App from "./App.jsx";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\deepseek-r1-webgpu\src\worker.js -----

import {
  AutoTokenizer,
  AutoModelForCausalLM,
  TextStreamer,
  InterruptableStoppingCriteria,
} from "@huggingface/transformers";

/**
 * Helper function to perform feature detection for WebGPU
 */
// let fp16_supported = false;
async function check() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) {
      throw new Error("WebGPU is not supported (no adapter found)");
    }
    // fp16_supported = adapter.features.has("shader-f16")
  } catch (e) {
    self.postMessage({
      status: "error",
      data: e.toString(),
    });
  }
}

/**
 * This class uses the Singleton pattern to enable lazy-loading of the pipeline
 */
class TextGenerationPipeline {
  static model_id = "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX";

  static async getInstance(progress_callback = null) {
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= AutoModelForCausalLM.from_pretrained(this.model_id, {
      dtype: "q4f16",
      device: "webgpu",
      progress_callback,
    });

    return Promise.all([this.tokenizer, this.model]);
  }
}

const stopping_criteria = new InterruptableStoppingCriteria();

let past_key_values_cache = null;
async function generate(messages) {
  // Retrieve the text-generation pipeline.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance();

  const inputs = tokenizer.apply_chat_template(messages, {
    add_generation_prompt: true,
    return_dict: true,
  });

  // 151648: <think>
  // 151649: </think>
  const [START_THINKING_TOKEN_ID, END_THINKING_TOKEN_ID] = tokenizer.encode(
    "<think></think>",
    { add_special_tokens: false },
  );

  let state = "thinking"; // 'thinking' or 'answering'
  let startTime;
  let numTokens = 0;
  let tps;
  const token_callback_function = (tokens) => {
    startTime ??= performance.now();

    if (numTokens++ > 0) {
      tps = (numTokens / (performance.now() - startTime)) * 1000;
    }
    if (tokens[0] == END_THINKING_TOKEN_ID) {
      state = "answering";
    }
  };
  const callback_function = (output) => {
    self.postMessage({
      status: "update",
      output,
      tps,
      numTokens,
      state,
    });
  };

  const streamer = new TextStreamer(tokenizer, {
    skip_prompt: true,
    skip_special_tokens: true,
    callback_function,
    token_callback_function,
  });

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  const { past_key_values, sequences } = await model.generate({
    ...inputs,
    // TODO: Add back when fixed
    // past_key_values: past_key_values_cache,

    // Sampling
    do_sample: false,
    // repetition_penalty: 1.1,
    // top_k: 3,
    // temperature: 0.2,

    max_new_tokens: 2048,
    streamer,
    stopping_criteria,
    return_dict_in_generate: true,
  });
  past_key_values_cache = past_key_values;

  const decoded = tokenizer.batch_decode(sequences, {
    skip_special_tokens: true,
  });

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: decoded,
  });
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  self.postMessage({
    status: "loading",
    data: "Compiling shaders and warming up model...",
  });

  // Run model with dummy input to compile shaders
  const inputs = tokenizer("a");
  await model.generate({ ...inputs, max_new_tokens: 1 });
  self.postMessage({ status: "ready" });
}
// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "check":
      check();
      break;

    case "load":
      load();
      break;

    case "generate":
      stopping_criteria.reset();
      generate(data);
      break;

    case "interrupt":
      stopping_criteria.interrupt();
      break;

    case "reset":
      past_key_values_cache = null;
      stopping_criteria.reset();
      break;
  }
});


----- .\deepseek-r1-webgpu\src\components\Chat.css -----

@scope (.markdown) {
  /* Code blocks */
  pre {
    margin: 0.5rem 0;
    white-space: break-spaces;
  }

  code {
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
    font-size: 0.9em;
  }

  pre,
  code {
    background-color: #f2f2f2;
  }

  @media (prefers-color-scheme: dark) {
    pre,
    code {
      background-color: #333;
    }
  }

  pre:has(code) {
    padding: 1rem 0.5rem;
  }

  pre > code {
    padding: 0;
  }

  /* Headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-weight: 600;
    line-height: 1.2;
  }

  h1 {
    font-size: 2em;
    margin: 1rem 0;
  }

  h2 {
    font-size: 1.5em;
    margin: 0.83rem 0;
  }

  h3 {
    font-size: 1.25em;
    margin: 0.67rem 0;
  }

  h4 {
    font-size: 1em;
    margin: 0.5rem 0;
  }

  h5 {
    font-size: 0.875em;
    margin: 0.33rem 0;
  }

  h6 {
    font-size: 0.75em;
    margin: 0.25rem 0;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6:first-child {
    margin-top: 0;
  }

  /* Unordered List */
  ul {
    list-style-type: disc;
    margin-left: 1.5rem;
  }

  /* Ordered List */
  ol {
    list-style-type: decimal;
    margin-left: 1.5rem;
  }

  /* List Items */
  li {
    margin: 0.25rem 0;
  }

  p:not(:first-child) {
    margin-top: 0.75rem;
  }

  p:not(:last-child) {
    margin-bottom: 0.75rem;
  }

  ul > li {
    margin-left: 1rem;
  }

  /* Table */
  table,
  th,
  td {
    border: 1px solid lightgray;
    padding: 0.25rem;
  }

  @media (prefers-color-scheme: dark) {
    table,
    th,
    td {
      border: 1px solid #f2f2f2;
    }
  }
}


----- .\deepseek-r1-webgpu\src\components\Chat.jsx -----

import { useState } from "react";
import { marked } from "marked";
import DOMPurify from "dompurify";

import BotIcon from "./icons/BotIcon";
import BrainIcon from "./icons/BrainIcon";
import UserIcon from "./icons/UserIcon";

import { MathJaxContext, MathJax } from "better-react-mathjax";
import "./Chat.css";

function render(text) {
  // Replace all instances of single backslashes before brackets with double backslashes
  // See https://github.com/markedjs/marked/issues/546 for more information.
  text = text.replace(/\\([\[\]\(\)])/g, "\\\\$1");

  const result = DOMPurify.sanitize(
    marked.parse(text, {
      async: false,
      breaks: true,
    }),
  );
  return result;
}
function Message({ role, content, answerIndex }) {
  const thinking = answerIndex ? content.slice(0, answerIndex) : content;
  const answer = answerIndex ? content.slice(answerIndex) : "";

  const [showThinking, setShowThinking] = useState(false);

  const doneThinking = answer.length > 0;

  return (
    <div className="flex items-start space-x-4">
      {role === "assistant" ? (
        <>
          <BotIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
          <div className="bg-gray-200 dark:bg-gray-700 rounded-lg p-4">
            <div className="min-h-6 text-gray-800 dark:text-gray-200 overflow-wrap-anywhere">
              {thinking.length > 0 ? (
                <>
                  <div className="bg-white dark:bg-gray-800 rounded-lg flex flex-col">
                    <button
                      className="flex items-center gap-2 cursor-pointer p-4 hover:bg-gray-50 dark:hover:bg-gray-900 rounded-lg "
                      onClick={() => setShowThinking((prev) => !prev)}
                      style={{ width: showThinking ? "100%" : "auto" }}
                    >
                      <BrainIcon
                        className={doneThinking ? "" : "animate-pulse"}
                      />
                      <span>
                        {doneThinking ? "View reasoning." : "Thinking..."}
                      </span>
                      <span className="ml-auto text-gray-700">
                        {showThinking ? "â–²" : "â–¼"}
                      </span>
                    </button>
                    {showThinking && (
                      <MathJax
                        className="border-t border-gray-200 dark:border-gray-700 px-4 py-2"
                        dynamic
                      >
                        <span
                          className="markdown"
                          dangerouslySetInnerHTML={{
                            __html: render(thinking),
                          }}
                        />
                      </MathJax>
                    )}
                  </div>
                  {doneThinking && (
                    <MathJax className="mt-2" dynamic>
                      <span
                        className="markdown"
                        dangerouslySetInnerHTML={{
                          __html: render(answer),
                        }}
                      />
                    </MathJax>
                  )}
                </>
              ) : (
                <span className="h-6 flex items-center gap-1">
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse"></span>
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-200"></span>
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-400"></span>
                </span>
              )}
            </div>
          </div>
        </>
      ) : (
        <>
          <UserIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
          <div className="bg-blue-500 text-white rounded-lg p-4">
            <p className="min-h-6 overflow-wrap-anywhere">{content}</p>
          </div>
        </>
      )}
    </div>
  );
}

export default function Chat({ messages }) {
  const empty = messages.length === 0;

  return (
    <div
      className={`flex-1 p-6 max-w-[960px] w-full ${empty ? "flex flex-col items-center justify-end" : "space-y-4"}`}
    >
      <MathJaxContext>
        {empty ? (
          <div className="text-xl">Ready!</div>
        ) : (
          messages.map((msg, i) => <Message key={`message-${i}`} {...msg} />)
        )}
      </MathJaxContext>
    </div>
  );
}


----- .\deepseek-r1-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\deepseek-r1-webgpu\src\components\icons\ArrowRightIcon.jsx -----

export default function ArrowRightIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M5 12h14" />
      <path d="m12 5 7 7-7 7" />
    </svg>
  );
}


----- .\deepseek-r1-webgpu\src\components\icons\BotIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M12 8V4H8" />
      <rect width="16" height="12" x="4" y="8" rx="2" />
      <path d="M2 14h2" />
      <path d="M20 14h2" />
      <path d="M15 13v2" />
      <path d="M9 13v2" />
    </svg>
  );
}


----- .\deepseek-r1-webgpu\src\components\icons\BrainIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 32 32"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path
        className="stroke-gray-600 dark:stroke-gray-400"
        d="M16 6v3.33M16 6c0-2.65 3.25-4.3 5.4-2.62 1.2.95 1.6 2.65.95 4.04a3.63 3.63 0 0 1 4.61.16 3.45 3.45 0 0 1 .46 4.37 5.32 5.32 0 0 1 1.87 4.75c-.22 1.66-1.39 3.6-3.07 4.14M16 6c0-2.65-3.25-4.3-5.4-2.62a3.37 3.37 0 0 0-.95 4.04 3.65 3.65 0 0 0-4.6.16 3.37 3.37 0 0 0-.49 4.27 5.57 5.57 0 0 0-1.85 4.85 5.3 5.3 0 0 0 3.07 4.15M16 9.33v17.34m0-17.34c0 2.18 1.82 4 4 4m6.22 7.5c.67 1.3.56 2.91-.27 4.11a4.05 4.05 0 0 1-4.62 1.5c0 1.53-1.05 2.9-2.66 2.9A2.7 2.7 0 0 1 16 26.66m10.22-5.83a4.05 4.05 0 0 0-3.55-2.17m-16.9 2.18a4.05 4.05 0 0 0 .28 4.1c1 1.44 2.92 2.09 4.59 1.5 0 1.52 1.12 2.88 2.7 2.88A2.7 2.7 0 0 0 16 26.67M5.78 20.85a4.04 4.04 0 0 1 3.55-2.18"
      ></path>
    </svg>
  );
}


----- .\deepseek-r1-webgpu\src\components\icons\StopIcon.jsx -----

export default function StopIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
      <path
        fill="currentColor"
        d="M9 9.563C9 9.252 9.252 9 9.563 9h4.874c.311 0 .563.252.563.563v4.874c0 .311-.252.563-.563.563H9.564A.562.562 0 0 1 9 14.437V9.564Z"
      />
    </svg>
  );
}


----- .\deepseek-r1-webgpu\src\components\icons\UserIcon.jsx -----

export default function UserIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2" />
      <circle cx="12" cy="7" r="4" />
    </svg>
  );
}


----- .\florence2-webgpu\src\App.jsx -----

import { useEffect, useState, useRef, useCallback } from "react";

import Progress from "./components/Progress";
import ImageInput from "./components/ImageInput";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);

  const [task, setTask] = useState("<CAPTION>");
  const [text, setText] = useState("");
  const [image, setImage] = useState(null);
  const [result, setResult] = useState(null);
  const [time, setTime] = useState(null);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    worker.current ??= new Worker(new URL("./worker.js", import.meta.url), {
      type: "module",
    });

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "complete":
          setResult(e.data.result);
          setTime(e.data.time);
          setStatus("ready");
          break;
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
    };
  }, []);

  const handleClick = useCallback(() => {
    if (status === null) {
      setStatus("loading");
      worker.current.postMessage({ type: "load" });
    } else {
      setStatus("running");
      worker.current.postMessage({
        type: "run",
        data: { text, url: image, task },
      });
    }
  }, [status, task, image, text]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="h-screen w-screen text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      <div className="flex flex-col mx-auto items justify-end max-w-[630px] h-full">
        {status === "loading" && (
          <div className="flex justify-center items-center fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] top-0 left-0">
            <div className="w-[500px]">
              <p className="text-center mb-1 text-white text-md">
                {loadingMessage}
              </p>
              {progressItems.map(({ file, progress, total }, i) => (
                <Progress
                  key={i}
                  text={file}
                  percentage={progress}
                  total={total}
                />
              ))}
            </div>
          </div>
        )}
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 text-center">
            <h1 className="text-6xl font-bold mb-2">Florence-2 WebGPU</h1>
            <h2 className="text-xl font-semibold">
              Powerful vision foundation model running locally in your browser.
            </h2>
          </div>

          <div className="w-full min-h-[220px] flex flex-col justify-center items-center p-2">
            <p className="mb-2">
              You are about to download{" "}
              <a
                href="https://huggingface.co/onnx-community/Florence-2-base-ft"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                Florence-2-base-ft
              </a>
              , a 230 million parameter vision foundation model that uses a
              prompt-based approach to handle a wide range of vision and
              vision-language tasks like captioning, object detection, and
              segmentation. Once loaded, the model (340&nbsp;MB) will be cached
              and reused when you revisit the page.
              <br />
              <br />
              Everything runs locally in your browser using{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning no API calls are made to a server
              for inference. You can even disconnect from the internet after the
              model has loaded!
            </p>

            <div className="flex w-full justify-around m-4">
              <div className="flex flex-col gap-2 w-full max-w-[48%]">
                <div className="flex flex-col">
                  <span className="text-sm mb-0.5">Task</span>
                  <select
                    className="border rounded-md p-1 dark:bg-gray-800"
                    value={task}
                    onChange={(e) => setTask(e.target.value)}
                  >
                    <option value="<CAPTION>">Caption</option>
                    <option value="<DETAILED_CAPTION>">Detailed Caption</option>
                    <option value="<MORE_DETAILED_CAPTION>">
                      More Detailed Caption
                    </option>
                    <option value="<OCR>">OCR</option>
                    <option value="<OCR_WITH_REGION>">OCR with Region</option>
                    <option value="<OD>">Object Detection</option>
                    <option value="<DENSE_REGION_CAPTION>">
                      Dense Region Caption
                    </option>
                    <option value="<CAPTION_TO_PHRASE_GROUNDING>">
                      Caption to Phrase Grounding
                    </option>
                    {/* <option value="<REFERRING_EXPRESSION_SEGMENTATION>">Referring Expression Segmentation</option> */}
                    {/* <option value="<REGION_TO_SEGMENTATION>">Region to Segmentation</option> */}
                    {/* <option value="<OPEN_VOCABULARY_DETECTION>">Open Vocabulary Detection</option> */}
                    {/* <option value="<REGION_TO_CATEGORY>">Region to Category</option> */}
                    {/* <option value="<REGION_TO_DESCRIPTION>">Region to Description</option> */}
                    {/* <option value="<REGION_TO_OCR>">Region to OCR</option> */}
                    {/* <option value="<REGION_PROPOSAL>">Region Proposal</option> */}
                  </select>
                </div>
                <div className="flex flex-col">
                  <span className="text-sm mb-0.5">Input Image</span>
                  <ImageInput
                    className="flex flex-col items-center border border-gray-300 rounded-md cursor-pointer h-[250px]"
                    onImageChange={(file, result) => {
                      worker.current.postMessage({ type: "reset" }); // Reset image cache
                      setResult(null);
                      setImage(result);
                    }}
                  />
                </div>
              </div>
              <div className="flex flex-col gap-2 w-full max-w-[48%] justify-end">
                {task === "<CAPTION_TO_PHRASE_GROUNDING>" && (
                  <div className="flex flex-col">
                    <span className="text-sm mb-0.5">Text input</span>
                    <input
                      className="border rounded-md px-2 py-[3.5px]"
                      value={text}
                      onChange={(e) => setText(e.target.value)}
                    />
                  </div>
                )}

                <div className="flex flex-col relative">
                  <span className="text-sm mb-0.5">Output</span>
                  <div className="flex justify-center border border-gray-300 rounded-md h-[250px]">
                    {result?.[task] && (
                      <>
                        {typeof result[task] === "string" ? (
                          <p className="pt-4 px-4 text-center max-h-[205px] overflow-y-auto">
                            {result[task]}
                          </p>
                        ) : (
                          <pre className="w-full h-full p-2 overflow-y-auto">
                            {JSON.stringify(result[task], null, 2)}
                          </pre>
                        )}
                        {time && (
                          <p className="text-sm text-gray-500 absolute bottom-2 bg-white p-1 rounded border">
                            Execution time: {time.toFixed(2)} ms
                          </p>
                        )}
                      </>
                    )}
                  </div>
                </div>
              </div>
            </div>

            <button
              className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 disabled:cursor-not-allowed select-none cursor-pointer"
              onClick={handleClick}
              disabled={
                status === "running" || (status !== null && image === null)
              }
            >
              {status === null
                ? "Load model"
                : status === "running"
                  ? "Running..."
                  : "Run model"}
            </button>
          </div>
        </div>
      </div>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\florence2-webgpu\src\index.css -----

@import "tailwindcss";

*::-webkit-scrollbar {
  width: 0.5rem;
}

*::-webkit-scrollbar-track {
  border-radius: 9999px;
  background-color: #f3f4f6;
  /* bg-gray-100 */
}

*::-webkit-scrollbar-thumb {
  border-radius: 9999px;
  background-color: #d1d5db;
  /* bg-gray-300 */
}

*::-webkit-scrollbar-thumb:hover {
  background-color: #6b7280;
  /* bg-gray-500 */
}

@media (prefers-color-scheme: dark) {
  *::-webkit-scrollbar-track {
    background-color: #374151;
    /* dark:bg-gray-700 */
  }

  *::-webkit-scrollbar-thumb {
    background-color: #4b5563;
    /* dark:bg-gray-600 */
  }
}


----- .\florence2-webgpu\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\florence2-webgpu\src\worker.js -----

import {
  Florence2ForConditionalGeneration,
  AutoProcessor,
  AutoTokenizer,
  RawImage,
  full,
} from "@huggingface/transformers";

async function hasFp16() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    return adapter.features.has("shader-f16");
  } catch (e) {
    return false;
  }
}

/**
 * This class uses the Singleton pattern to ensure that only one instance of the model is loaded.
 */
class Florence2Singleton {
  static model_id = "onnx-community/Florence-2-base-ft";

  static async getInstance(progress_callback = null) {
    this.processor ??= AutoProcessor.from_pretrained(this.model_id);
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id);

    this.supports_fp16 ??= await hasFp16();
    this.model ??= Florence2ForConditionalGeneration.from_pretrained(
      this.model_id,
      {
        dtype: {
          embed_tokens: this.supports_fp16 ? "fp16" : "fp32",
          vision_encoder: this.supports_fp16 ? "fp16" : "fp32",
          encoder_model: "q4", // or 'fp16' or 'fp32'
          decoder_model_merged: "q4", // or 'fp16' or 'fp32'
        },
        device: "webgpu",
        progress_callback,
      },
    );

    return Promise.all([this.model, this.tokenizer, this.processor]);
  }
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [model, tokenizer, processor] = await Florence2Singleton.getInstance(
    (x) => {
      // We also add a progress callback to the pipeline so that we can
      // track model loading.
      self.postMessage(x);
    },
  );

  self.postMessage({
    status: "loading",
    data: "Compiling shaders and warming up model...",
  });

  // Dummy text and vision inputs
  const text_inputs = tokenizer("a");
  const pixel_values = full([1, 3, 768, 768], 0.0);

  // Run model with dummy input to compile shaders
  await model.generate({
    ...text_inputs,
    pixel_values,
    max_new_tokens: 1,
  });

  self.postMessage({ status: "ready" });
}

const TASKS_WITH_INPUTS = ["<CAPTION_TO_PHRASE_GROUNDING>"];

let vision_inputs;
let image_size;
async function run({ text, url, task }) {
  const [model, tokenizer, processor] = await Florence2Singleton.getInstance();

  // Read and preprocess image
  const start = performance.now();
  if (!vision_inputs) {
    // Cache vision inputs when possible
    const image = await RawImage.fromURL(url);
    image_size = image.size;
    vision_inputs = await processor(image);
  }

  let user_input = task;
  if (TASKS_WITH_INPUTS.includes(task) && text) {
    user_input += text;
  }
  const prompts = processor.construct_prompts(user_input);
  const text_inputs = tokenizer(prompts);

  // Generate text
  const generated_ids = await model.generate({
    ...text_inputs,
    ...vision_inputs,
    max_new_tokens: 128,
    num_beams: 1,
    do_sample: false,
  });

  // Decode generated text
  const generated_text = tokenizer.batch_decode(generated_ids, {
    skip_special_tokens: false,
  })[0];

  // Post-process the generated text
  const result = processor.post_process_generation(
    generated_text,
    task,
    image_size,
  );

  const end = performance.now();

  self.postMessage({ status: "complete", result, time: end - start });
}

// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "load":
      load();
      break;

    case "run":
      run(data);
      break;

    case "reset":
      vision_inputs = image_size = null;
      break;
  }
});


----- .\florence2-webgpu\src\components\ImageInput.jsx -----

import { useState, useRef } from "react";

const EXAMPLE_URL =
  "https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/beetle.png";

const ImageInput = ({ onImageChange, ...props }) => {
  const [imagePreview, setImagePreview] = useState(null);
  const fileInputRef = useRef(null);

  const readFile = (file) => {
    if (!file) return;
    const reader = new FileReader();
    reader.onloadend = () => {
      setImagePreview(reader.result);
      if (onImageChange) {
        onImageChange(file, reader.result);
      }
    };
    reader.readAsDataURL(file);
  };

  const handleImageChange = (event) => {
    readFile(event.target.files[0]);
  };

  const handleDragOver = (event) => {
    event.preventDefault();
  };

  const handleDrop = (event) => {
    event.preventDefault();
    readFile(event.dataTransfer.files[0]);
  };

  const handleClick = () => {
    fileInputRef.current.click();
  };

  return (
    <div
      {...props}
      onClick={handleClick}
      onDragOver={handleDragOver}
      onDrop={handleDrop}
    >
      <input
        type="file"
        accept="image/*"
        onChange={handleImageChange}
        ref={fileInputRef}
        className="hidden"
      />
      {imagePreview ? (
        <img
          src={imagePreview}
          alt="Selected"
          className="w-full max-h-[250px] h-full object-contain rounded-md"
        />
      ) : (
        <div className="w-full h-full flex flex-col items-center justify-center border-2 border-dashed border-gray-300 rounded-md">
          <span className="text-gray-600 text-center m-3">
            <u>Drag & drop</u> or <u>click</u>
            <br />
            to select an image
          </span>
          <span
            className="text-gray-500 text-sm hover:text-gray-800 dark:hover:text-gray-300"
            onClick={(e) => {
              e.stopPropagation();
              setImagePreview(EXAMPLE_URL);
              onImageChange(null, EXAMPLE_URL);
            }}
          >
            (or <u>try an example</u>)
          </span>
        </div>
      )}
    </div>
  );
};

export default ImageInput;


----- .\florence2-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\janus-pro-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import Chat from "./components/Chat";
import ArrowRightIcon from "./components/icons/ArrowRightIcon";
import StopIcon from "./components/icons/StopIcon";
import Progress from "./components/Progress";
import ImageIcon from "./components/icons/ImageIcon";
import ImagePreview from "./components/ImagePreview";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;
const STICKY_SCROLL_THRESHOLD = 120;
const EXAMPLES = [
  {
    display: "Generate an image of a cute baby fox.",
    prompt:
      "/imagine A cute and adorable baby fox with big brown eyes, autumn leaves in the background enchanting, immortal, fluffy, shiny mane, Petals, fairyism, unreal engine 5 and Octane Render, highly detailed, photorealistic, cinematic, natural colors.",
  },
  {
    prompt: "Convert the formula into latex code.",
    image:
      "https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/quadratic_formula.png",
  },
  {
    prompt: "What is the difference between AI and ML?",
  },
  {
    prompt: "Write python code to compute the nth fibonacci number.",
  },
];

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const textareaRef = useRef(null);
  const chatContainerRef = useRef(null);
  const imageUploadRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);
  const [isRunning, setIsRunning] = useState(false);

  // Inputs and outputs
  const [input, setInput] = useState("");
  const [image, setImage] = useState(null);
  const [messages, setMessages] = useState([]);
  const [tps, setTps] = useState(null);
  const [numTokens, setNumTokens] = useState(null);
  const [imageProgress, setImageProgress] = useState(null);
  const [imageGenerationTime, setImageGenerationTime] = useState(null);

  function onEnter(message, img) {
    setMessages((prev) => [
      ...prev,
      { role: "user", content: message, image: img ?? image },
    ]);
    setTps(null);
    setIsRunning(true);
    setInput("");
    setImage(null);
    setNumTokens(null);
    setImageProgress(null);
    setImageGenerationTime(null);
  }

  function onInterrupt() {
    // NOTE: We do not set isRunning to false here because the worker
    // will send a 'complete' message when it is done.
    worker.current.postMessage({ type: "interrupt" });
  }

  function resizeInput() {
    if (!textareaRef.current) return;

    const target = textareaRef.current;
    target.style.height = "auto";
    const newHeight = Math.min(Math.max(target.scrollHeight, 24), 200);
    target.style.height = `${newHeight}px`;
  }

  useEffect(() => {
    resizeInput();
  }, [input]);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    if (!worker.current) {
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
      worker.current.postMessage({ type: "check" }); // Do a feature check
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        // WebGPU feature checking
        case "success":
          setStatus("idle");
          break;
        case "error":
          setError(e.data.data);
          break;

        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "start":
          {
            // Start generation
            setMessages((prev) => [
              ...prev,
              { role: "assistant", content: "" },
            ]);
          }
          break;

        case "text-update":
          // Generation update: update the output text.
          // Parse messages
          const { output, tps, numTokens } = e.data;
          setTps(tps);
          setNumTokens(numTokens);
          setMessages((prev) => {
            const cloned = [...prev];
            const last = cloned.at(-1);
            cloned[cloned.length - 1] = {
              ...last,
              content: last.content + output,
            };
            return cloned;
          });
          break;

        case "image-update":
          const { blob, progress, time } = e.data;

          if (blob) {
            // Add image to the last message
            const url = URL.createObjectURL(blob);
            setMessages((prev) => {
              const cloned = [...prev];
              const last = cloned.at(-1);
              cloned[cloned.length - 1] = {
                ...last,
                image: url,
              };
              return cloned;
            });
          } else {
            setImageProgress(progress);
            setImageGenerationTime(time);
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsRunning(false);
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
    };
  }, []);

  // Send the messages to the worker thread whenever the `messages` state changes.
  useEffect(() => {
    if (messages.filter((x) => x.role === "user").length === 0) {
      // No user messages yet: do nothing.
      return;
    }
    if (messages.at(-1).role === "assistant") {
      // Do not update if the last message is from the assistant
      return;
    }
    setTps(null);
    worker.current.postMessage({ type: "generate", data: messages });
  }, [messages, isRunning]);

  useEffect(() => {
    if (!chatContainerRef.current || !isRunning) return;
    const element = chatContainerRef.current;
    if (
      element.scrollHeight - element.scrollTop - element.clientHeight <
      STICKY_SCROLL_THRESHOLD
    ) {
      element.scrollTop = element.scrollHeight;
    }
  }, [messages, isRunning]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto items justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {(status === null || status === "idle") && messages.length === 0 && (
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[436px] text-center">
            <img
              src="logo.png"
              width="70%"
              height="auto"
              className="block"
            ></img>
            <h1 className="text-5xl font-bold mb-1">Janus Pro WebGPU</h1>
            <h2 className="font-semibold">
              A novel autoregressive framework for unified
              <br />
              multimodal understanding and generation.
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            <p className="max-w-[470px] mb-4">
              <br />
              You are about to load{" "}
              <a
                href="https://huggingface.co/onnx-community/Janus-Pro-1B-ONNX"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                Janus-Pro-1B
              </a>
              , a multimodal vision-language model that is optimized for
              inference on the web. Everything runs 100% locally in your browser
              with{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning no data is sent to a server. Once
              the model has loaded, it can even be used offline. The source code
              for the demo can be found on{" "}
              <a
                href="https://github.com/huggingface/transformers.js-examples/tree/main/janus-pro-webgpu"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                GitHub
              </a>
              .
            </p>

            {error && (
              <div className="text-red-500 text-center mb-2">
                <p className="mb-1">
                  Unable to load model due to the following error:
                </p>
                <p className="text-sm">{error}</p>
              </div>
            )}

            {!error && (
              <button
                className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 disabled:cursor-not-allowed select-none"
                onClick={() => {
                  worker.current.postMessage({ type: "load" });
                  setStatus("loading");
                }}
                disabled={status === null || status === "loading"}
              >
                {status === null ? "Running feature checks..." : "Load model"}
              </button>
            )}
          </div>
        </div>
      )}
      {status === "loading" && (
        <>
          <div className="w-full max-w-[500px] text-left mx-auto p-4 bottom-0 mt-auto">
            <p className="text-center mb-1">{loadingMessage}</p>
            {progressItems.map(({ file, progress, total }, i) => (
              <Progress
                key={i}
                text={file}
                percentage={progress}
                total={total}
              />
            ))}
          </div>
        </>
      )}

      {status === "ready" && (
        <div
          ref={chatContainerRef}
          className="overflow-y-auto scrollbar-thin w-full flex flex-col items-center h-full"
        >
          <Chat messages={messages} />
          {messages.length === 0 && !image && (
            <div className="flex flex-col center">
              {EXAMPLES.map(({ display, prompt, image }, i) => (
                <div
                  key={i}
                  className="max-w-[600px] m-1 border dark:border-gray-600 rounded-md p-2 bg-gray-100 dark:bg-gray-700 cursor-pointer"
                  onClick={() => onEnter(prompt, image)}
                >
                  {display ?? prompt}
                </div>
              ))}
            </div>
          )}

          <p className="text-center text-sm min-h-6 text-gray-500 dark:text-gray-300">
            {messages.length > 0 && (
              <>
                {tps ? (
                  <>
                    {!isRunning && (
                      <span>
                        Generated {numTokens} tokens in{" "}
                        {(numTokens / tps).toFixed(2)} seconds&nbsp;&#40;
                      </span>
                    )}
                    <span className="font-medium font-mono text-center mr-1 text-black dark:text-white">
                      {tps.toFixed(2)}
                    </span>
                    <span className="text-gray-500 dark:text-gray-300">
                      tokens/second
                    </span>
                    {!isRunning && <span className="mr-1">&#41;.</span>}
                  </>
                ) : (
                  imageProgress && (
                    <>
                      {isRunning ? (
                        <>
                          <span>Generating image...</span>&nbsp;&#40;
                          <span className="font-medium font-mono text-center text-black dark:text-white">
                            {(imageProgress * 100).toFixed(2)}%
                          </span>
                          <span className="mr-1">&#41;</span>
                        </>
                      ) : (
                        <span>
                          Generated image in{" "}
                          {(imageGenerationTime / 1000).toFixed(2)}{" "}
                          seconds.&nbsp;
                        </span>
                      )}
                    </>
                  )
                )}

                {!isRunning && (
                  <span
                    className="underline cursor-pointer"
                    onClick={() => setMessages([])}
                  >
                    Reset
                  </span>
                )}
              </>
            )}
          </p>
        </div>
      )}

      <div className="mt-2 border dark:bg-gray-700 rounded-lg w-[600px] max-w-[80%] max-h-[200px] mx-auto relative mb-3 flex">
        <label
          htmlFor="file-upload"
          className={
            status === "ready"
              ? "cursor-pointer"
              : "cursor-not-allowed pointer-events-none"
          }
        >
          <ImageIcon
            className={`h-8 w-8 p-1 rounded-md ${status === "ready" ? "text-gray-800 dark:text-gray-100" : "text-gray-400 dark:text-gray-500"} absolute bottom-3 left-1.5`}
          ></ImageIcon>
          <input
            ref={imageUploadRef}
            id="file-upload"
            type="file"
            accept="image/*"
            className="hidden"
            onInput={(e) => {
              const file = e.target.files[0];
              if (!file) {
                return;
              }

              const reader = new FileReader();

              // Set up a callback when the file is loaded
              reader.onload = (e2) => {
                setImage(e2.target.result);
                e.target.value = "";
              };

              reader.readAsDataURL(file);
            }}
          ></input>
        </label>
        <div className="w-full flex flex-col">
          {image && (
            <ImagePreview
              onRemove={() => {
                setImage(null);
              }}
              src={image}
              className="w-20 h-20 min-w-20 min-h-20 relative p-2"
            />
          )}

          <textarea
            ref={textareaRef}
            className="scrollbar-thin w-full pl-11 pr-12 dark:bg-gray-700 py-4 rounded-lg bg-transparent border-none outline-none text-gray-800 disabled:text-gray-400 dark:text-gray-100 placeholder-gray-500 disabled:placeholder-gray-200 dark:placeholder-gray-300 dark:disabled:placeholder-gray-500 resize-none disabled:cursor-not-allowed"
            placeholder="Type message or use '/imagine <prompt>' to generate an image."
            type="text"
            rows={1}
            value={input}
            disabled={status !== "ready"}
            title={
              status === "ready" ? "Model is ready" : "Model not loaded yet"
            }
            onKeyDown={(e) => {
              if (
                input.length > 0 &&
                !isRunning &&
                e.key === "Enter" &&
                !e.shiftKey
              ) {
                e.preventDefault(); // Prevent default behavior of Enter key
                onEnter(input, image);
              }
            }}
            onInput={(e) => setInput(e.target.value)}
          />
        </div>
        {isRunning ? (
          <div className="cursor-pointer" onClick={onInterrupt}>
            <StopIcon className="h-8 w-8 p-1 rounded-md text-gray-800 dark:text-gray-100 absolute right-3 bottom-3" />
          </div>
        ) : input.length > 0 ? (
          <div className="cursor-pointer" onClick={() => onEnter(input)}>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-800 dark:bg-gray-100 text-white dark:text-black rounded-md absolute right-3 bottom-3`}
            />
          </div>
        ) : (
          <div>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-200 dark:bg-gray-600 text-gray-50 dark:text-gray-800 rounded-md absolute right-3 bottom-3`}
            />
          </div>
        )}
      </div>

      <p className="text-xs text-gray-400 text-center mb-3">
        Disclaimer: Generated content may be inaccurate or false.
      </p>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\janus-pro-webgpu\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;

@layer utilities {
  .scrollbar-thin::-webkit-scrollbar {
    @apply w-2;
  }

  .scrollbar-thin::-webkit-scrollbar-track {
    @apply rounded-full bg-gray-100 dark:bg-gray-700;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb {
    @apply rounded-full bg-gray-300 dark:bg-gray-600;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb:hover {
    @apply bg-gray-500;
  }

  .animation-delay-200 {
    animation-delay: 200ms;
  }
  .animation-delay-400 {
    animation-delay: 400ms;
  }

  .overflow-wrap-anywhere {
    overflow-wrap: anywhere;
  }
}


----- .\janus-pro-webgpu\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\janus-pro-webgpu\src\worker.js -----

import {
  AutoProcessor,
  MultiModalityCausalLM,
  BaseStreamer,
  TextStreamer,
  InterruptableStoppingCriteria,
} from "@huggingface/transformers";

// Define constants
const IMAGE_GENERATION_COMMAND_PREFIX = "/imagine ";
const MAX_NEW_TEXT_TOKENS = 1024;

/**
 * Helper function to perform WebGPU feature detection
 */
let fp16_supported = false;
async function check() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) {
      throw new Error("WebGPU is not supported (no adapter found)");
    }
    fp16_supported = adapter.features.has("shader-f16");
    self.postMessage({
      status: "success",
      data: fp16_supported,
    });
  } catch (e) {
    self.postMessage({
      status: "error",
      data: e.toString(),
    });
  }
}

/**
 * This class uses the Singleton pattern to enable lazy-loading of the pipeline
 */
class ImageGenerationPipeline {
  static model_id = "onnx-community/Janus-Pro-1B-ONNX";

  static async getInstance(progress_callback = null) {
    this.processor ??= AutoProcessor.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= MultiModalityCausalLM.from_pretrained(this.model_id, {
      dtype: fp16_supported
        ? {
            prepare_inputs_embeds: "q4",
            language_model: "q4f16",
            lm_head: "fp16",
            gen_head: "fp16",
            gen_img_embeds: "fp16",
            image_decode: "fp32",
          }
        : {
            prepare_inputs_embeds: "fp32",
            language_model: "q4",
            lm_head: "fp32",
            gen_head: "fp32",
            gen_img_embeds: "fp32",
            image_decode: "fp32",
          },
      device: {
        prepare_inputs_embeds: "wasm", // TODO use "webgpu" when bug is fixed
        language_model: "webgpu",
        lm_head: "webgpu",
        gen_head: "webgpu",
        gen_img_embeds: "webgpu",
        image_decode: "webgpu",
      },
      progress_callback,
    });

    return Promise.all([this.processor, this.model]);
  }
}

class ProgressStreamer extends BaseStreamer {
  constructor(total, on_progress) {
    super();
    this.total = total;
    this.on_progress = on_progress;

    this.count = null;
    this.start_time = null;
  }

  put(value) {
    if (this.count === null) {
      // Ignore the first batch of tokens (prompt)
      this.count = 0;
      this.start_time = performance.now();
      return;
    }

    const progress = ++this.count / this.total;

    this.on_progress({
      count: this.count,
      total: this.total,
      progress,
      time: performance.now() - this.start_time,
    });
  }

  end() {
    /* no nothing */
  }
}

const stopping_criteria = new InterruptableStoppingCriteria();

async function generate(messages) {
  // For this demo, we only respond to the last message
  const message = messages.at(-1);

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  // Load the pipeline
  const [processor, model] = await ImageGenerationPipeline.getInstance();

  // Determine if the user wants to generate an image or text
  if (message.content.startsWith(IMAGE_GENERATION_COMMAND_PREFIX)) {
    const text = message.content.replace(IMAGE_GENERATION_COMMAND_PREFIX, "");

    const conversation = [
      {
        role: "<|User|>", // uses title case
        content: text,
      },
    ];
    const inputs = await processor(conversation, {
      chat_template: "text_to_image",
    });

    const callback_function = (output) => {
      self.postMessage({
        status: "image-update",
        ...output,
      });
    };

    const num_image_tokens = processor.num_image_tokens;
    const streamer = new ProgressStreamer(num_image_tokens, callback_function);

    const outputs = await model.generate_images({
      ...inputs,
      min_new_tokens: num_image_tokens,
      max_new_tokens: num_image_tokens,
      do_sample: true,
      streamer,
    });

    const blob = await outputs[0].toBlob();

    // Send the output back to the main thread
    self.postMessage({
      status: "image-update",
      blob,
    });
  } else {
    const inputs = await processor(
      message.image
        ? [
            {
              role: "<|User|>",
              content: "<image_placeholder>\n" + message.content,
              images: [message.image],
            },
          ]
        : [
            {
              role: "<|System|>",
              content:
                "You are a helpful assistant. Answer the user's questions in a concise manner.",
            },
            {
              role: "<|User|>",
              content: message.content,
            },
          ],
    );

    let startTime;
    let numTokens = 0;
    let tps;
    const token_callback_function = () => {
      startTime ??= performance.now();

      if (numTokens++ > 0) {
        tps = (numTokens / (performance.now() - startTime)) * 1000;
      }
    };
    const callback_function = (output) => {
      self.postMessage({
        status: "text-update",
        output,
        tps,
        numTokens,
      });
    };

    const streamer = new TextStreamer(processor.tokenizer, {
      skip_prompt: true,
      skip_special_tokens: true,
      callback_function,
      token_callback_function,
    });

    // Generate response
    const outputs = await model.generate({
      ...inputs,
      max_new_tokens: MAX_NEW_TEXT_TOKENS,
      do_sample: false,
      streamer,
      stopping_criteria,
    });
  }

  // Tell the main thread we are done
  self.postMessage({
    status: "complete",
  });
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [processor, model] = await ImageGenerationPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  self.postMessage({ status: "ready" });
}

// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "check":
      check();
      break;

    case "load":
      load();
      break;

    case "generate":
      stopping_criteria.reset();
      generate(data);
      break;

    case "interrupt":
      stopping_criteria.interrupt();
      break;

    case "reset":
      stopping_criteria.reset();
      break;
  }
});


----- .\janus-pro-webgpu\src\components\Chat.css -----

@scope (.markdown) {
  /* Code blocks */
  pre {
    margin: 0.5rem 0;
    white-space: break-spaces;
  }

  code {
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
    font-size: 0.9em;
  }

  pre,
  code {
    background-color: #f2f2f2;
  }

  @media (prefers-color-scheme: dark) {
    pre,
    code {
      background-color: #333;
    }
  }

  pre:has(code) {
    padding: 1rem 0.5rem;
  }

  pre > code {
    padding: 0;
  }

  /* Headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-weight: 600;
    line-height: 1.2;
  }

  h1 {
    font-size: 2em;
    margin: 1rem 0;
  }

  h2 {
    font-size: 1.5em;
    margin: 0.83rem 0;
  }

  h3 {
    font-size: 1.25em;
    margin: 0.67rem 0;
  }

  h4 {
    font-size: 1em;
    margin: 0.5rem 0;
  }

  h5 {
    font-size: 0.875em;
    margin: 0.33rem 0;
  }

  h6 {
    font-size: 0.75em;
    margin: 0.25rem 0;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6:first-child {
    margin-top: 0;
  }

  /* Unordered List */
  ul {
    list-style-type: disc;
    margin-left: 1.5rem;
  }

  /* Ordered List */
  ol {
    list-style-type: decimal;
    margin-left: 1.5rem;
  }

  /* List Items */
  li {
    margin: 0.25rem 0;
  }

  p:not(:first-child) {
    margin-top: 0.75rem;
  }

  p:not(:last-child) {
    margin-bottom: 0.75rem;
  }
}


----- .\janus-pro-webgpu\src\components\Chat.jsx -----

import { marked } from "marked";
import DOMPurify from "dompurify";

import BotIcon from "./icons/BotIcon";
import UserIcon from "./icons/UserIcon";

import "./Chat.css";
import { useEffect } from "react";

function render(text) {
  return DOMPurify.sanitize(marked.parse(text));
}

export default function Chat({ messages }) {
  const empty = messages.length === 0;

  useEffect(() => {
    window.MathJax.typeset();
  }, [messages]);

  return (
    <div
      className={`flex-1 p-6 max-w-[960px] w-full ${empty ? "flex flex-col items-center justify-end" : "space-y-4"}`}
    >
      {empty ? (
        <div className="text-xl">Ready!</div>
      ) : (
        messages.map((msg, i) => (
          <div key={`message-${i}`} className="flex items-start space-x-4">
            {msg.role === "assistant" ? (
              <>
                <BotIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-gray-200 dark:bg-gray-700 rounded-lg p-4">
                  <p className="min-h-6 text-gray-800 dark:text-gray-200 overflow-wrap-anywhere">
                    {msg.image ? (
                      <img
                        src={msg.image}
                        className="max-w-full w-[384px] rounded-md"
                      />
                    ) : msg.content.length > 0 ? (
                      <span
                        className="markdown"
                        dangerouslySetInnerHTML={{
                          __html: render(msg.content),
                        }}
                      />
                    ) : (
                      <span className="h-6 flex items-center gap-1">
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-200"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-400"></span>
                      </span>
                    )}
                  </p>
                </div>
              </>
            ) : (
              <>
                <UserIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-blue-500 text-white rounded-lg p-4">
                  {msg.image && (
                    <img
                      src={msg.image}
                      className="max-w-full max-h-64 rounded-md mb-3"
                    />
                  )}
                  <p className="min-h-6 overflow-wrap-anywhere">
                    {msg.content}
                  </p>
                </div>
              </>
            )}
          </div>
        ))
      )}
    </div>
  );
}


----- .\janus-pro-webgpu\src\components\ImagePreview.jsx -----

import { useState } from "react";
import CrossIcon from "./icons/CrossIcon";

export default function ImagePreview({ src, onRemove, ...props }) {
  const [hover, setHover] = useState(false);

  return (
    <div
      {...props}
      onMouseEnter={() => setHover(true)}
      onMouseLeave={() => setHover(false)}
    >
      <CrossIcon
        onClick={onRemove}
        className={`absolute top-0 right-0 cursor-pointer dark:fill-gray-400 dark:text-gray-100 fill-gray-200 text-gray-800 ${hover ? "" : "hidden"}`}
      />
      <img
        src={src}
        alt="Upload preview"
        className="w-full h-full object-cover rounded-md"
      />
    </div>
  );
}


----- .\janus-pro-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\janus-pro-webgpu\src\components\icons\ArrowRightIcon.jsx -----

export default function ArrowRightIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M5 12h14" />
      <path d="m12 5 7 7-7 7" />
    </svg>
  );
}


----- .\janus-pro-webgpu\src\components\icons\BotIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M12 8V4H8" />
      <rect width="16" height="12" x="4" y="8" rx="2" />
      <path d="M2 14h2" />
      <path d="M20 14h2" />
      <path d="M15 13v2" />
      <path d="M9 13v2" />
    </svg>
  );
}


----- .\janus-pro-webgpu\src\components\icons\CrossIcon.jsx -----

export default function CrossIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="m9.75 9.75 4.5 4.5m0-4.5-4.5 4.5M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
    </svg>
  );
}


----- .\janus-pro-webgpu\src\components\icons\ImageIcon.jsx -----

export default function ImageIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="m2.25 15.75 5.159-5.159a2.25 2.25 0 0 1 3.182 0l5.159 5.159m-1.5-1.5 1.409-1.409a2.25 2.25 0 0 1 3.182 0l2.909 2.909m-18 3.75h16.5a1.5 1.5 0 0 0 1.5-1.5V6a1.5 1.5 0 0 0-1.5-1.5H3.75A1.5 1.5 0 0 0 2.25 6v12a1.5 1.5 0 0 0 1.5 1.5Zm10.5-11.25h.008v.008h-.008V8.25Zm.375 0a.375.375 0 1 1-.75 0 .375.375 0 0 1 .75 0Z" />
    </svg>
  );
}


----- .\janus-pro-webgpu\src\components\icons\StopIcon.jsx -----

export default function StopIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
      <path
        fill="currentColor"
        d="M9 9.563C9 9.252 9.252 9 9.563 9h4.874c.311 0 .563.252.563.563v4.874c0 .311-.252.563-.563.563H9.564A.562.562 0 0 1 9 14.437V9.564Z"
      />
    </svg>
  );
}


----- .\janus-pro-webgpu\src\components\icons\UserIcon.jsx -----

export default function UserIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2" />
      <circle cx="12" cy="7" r="4" />
    </svg>
  );
}


----- .\janus-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import Chat from "./components/Chat";
import ArrowRightIcon from "./components/icons/ArrowRightIcon";
import StopIcon from "./components/icons/StopIcon";
import Progress from "./components/Progress";
import ImageIcon from "./components/icons/ImageIcon";
import ImagePreview from "./components/ImagePreview";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;
const STICKY_SCROLL_THRESHOLD = 120;
const EXAMPLES = [
  {
    display: "Generate an image of a cute baby fox.",
    prompt:
      "/imagine A cute and adorable baby fox with big brown eyes, autumn leaves in the background enchanting, immortal, fluffy, shiny mane, Petals, fairyism, unreal engine 5 and Octane Render, highly detailed, photorealistic, cinematic, natural colors.",
  },
  {
    prompt: "Convert the formula into latex code.",
    image:
      "https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/quadratic_formula.png",
  },
  {
    prompt: "What is the difference between AI and ML?",
  },
  {
    prompt: "Write python code to compute the nth fibonacci number.",
  },
];

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const textareaRef = useRef(null);
  const chatContainerRef = useRef(null);
  const imageUploadRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);
  const [isRunning, setIsRunning] = useState(false);

  // Inputs and outputs
  const [input, setInput] = useState("");
  const [image, setImage] = useState(null);
  const [messages, setMessages] = useState([]);
  const [tps, setTps] = useState(null);
  const [numTokens, setNumTokens] = useState(null);
  const [imageProgress, setImageProgress] = useState(null);
  const [imageGenerationTime, setImageGenerationTime] = useState(null);

  function onEnter(message, img) {
    setMessages((prev) => [
      ...prev,
      { role: "user", content: message, image: img ?? image },
    ]);
    setTps(null);
    setIsRunning(true);
    setInput("");
    setImage(null);
    setNumTokens(null);
    setImageProgress(null);
    setImageGenerationTime(null);
  }

  function onInterrupt() {
    // NOTE: We do not set isRunning to false here because the worker
    // will send a 'complete' message when it is done.
    worker.current.postMessage({ type: "interrupt" });
  }

  function resizeInput() {
    if (!textareaRef.current) return;

    const target = textareaRef.current;
    target.style.height = "auto";
    const newHeight = Math.min(Math.max(target.scrollHeight, 24), 200);
    target.style.height = `${newHeight}px`;
  }

  useEffect(() => {
    resizeInput();
  }, [input]);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    if (!worker.current) {
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
      worker.current.postMessage({ type: "check" }); // Do a feature check
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        // WebGPU feature checking
        case "success":
          setStatus("idle");
          break;
        case "error":
          setError(e.data.data);
          break;

        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "start":
          {
            // Start generation
            setMessages((prev) => [
              ...prev,
              { role: "assistant", content: "" },
            ]);
          }
          break;

        case "text-update":
          // Generation update: update the output text.
          // Parse messages
          const { output, tps, numTokens } = e.data;
          setTps(tps);
          setNumTokens(numTokens);
          setMessages((prev) => {
            const cloned = [...prev];
            const last = cloned.at(-1);
            cloned[cloned.length - 1] = {
              ...last,
              content: last.content + output,
            };
            return cloned;
          });
          break;

        case "image-update":
          const { blob, progress, time } = e.data;

          if (blob) {
            // Add image to the last message
            const url = URL.createObjectURL(blob);
            setMessages((prev) => {
              const cloned = [...prev];
              const last = cloned.at(-1);
              cloned[cloned.length - 1] = {
                ...last,
                image: url,
              };
              return cloned;
            });
          } else {
            setImageProgress(progress);
            setImageGenerationTime(time);
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsRunning(false);
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
    };
  }, []);

  // Send the messages to the worker thread whenever the `messages` state changes.
  useEffect(() => {
    if (messages.filter((x) => x.role === "user").length === 0) {
      // No user messages yet: do nothing.
      return;
    }
    if (messages.at(-1).role === "assistant") {
      // Do not update if the last message is from the assistant
      return;
    }
    setTps(null);
    worker.current.postMessage({ type: "generate", data: messages });
  }, [messages, isRunning]);

  useEffect(() => {
    if (!chatContainerRef.current || !isRunning) return;
    const element = chatContainerRef.current;
    if (
      element.scrollHeight - element.scrollTop - element.clientHeight <
      STICKY_SCROLL_THRESHOLD
    ) {
      element.scrollTop = element.scrollHeight;
    }
  }, [messages, isRunning]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto items justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {(status === null || status === "idle") && messages.length === 0 && (
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[350px] text-center">
            <img
              src="logo.png"
              width="80%"
              height="auto"
              className="block"
            ></img>
            <h1 className="text-5xl font-bold mb-1">Janus WebGPU</h1>
            <h2 className="font-semibold">
              A novel autoregressive framework for unified multimodal
              understanding and generation.
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            <p className="max-w-[452px] mb-4">
              <br />
              You are about to load{" "}
              <a
                href="https://huggingface.co/onnx-community/Janus-1.3B-ONNX"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                Janus-1.3B
              </a>
              , a multimodal vision-language model that is optimized for
              inference on the web. Everything runs 100% locally in your browser
              with{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning no data is sent to a server. Once
              the model has loaded, it can even be used offline. The source code
              for the demo can be found on{" "}
              <a
                href="https://github.com/huggingface/transformers.js-examples/tree/main/janus-webgpu"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                GitHub
              </a>
              .
            </p>

            {error && (
              <div className="text-red-500 text-center mb-2">
                <p className="mb-1">
                  Unable to load model due to the following error:
                </p>
                <p className="text-sm">{error}</p>
              </div>
            )}

            {!error && (
              <button
                className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 disabled:cursor-not-allowed select-none"
                onClick={() => {
                  worker.current.postMessage({ type: "load" });
                  setStatus("loading");
                }}
                disabled={status === null || status === "loading"}
              >
                {status === null ? "Running feature checks..." : "Load model"}
              </button>
            )}
          </div>
        </div>
      )}
      {status === "loading" && (
        <>
          <div className="w-full max-w-[500px] text-left mx-auto p-4 bottom-0 mt-auto">
            <p className="text-center mb-1">{loadingMessage}</p>
            {progressItems.map(({ file, progress, total }, i) => (
              <Progress
                key={i}
                text={file}
                percentage={progress}
                total={total}
              />
            ))}
          </div>
        </>
      )}

      {status === "ready" && (
        <div
          ref={chatContainerRef}
          className="overflow-y-auto scrollbar-thin w-full flex flex-col items-center h-full"
        >
          <Chat messages={messages} />
          {messages.length === 0 && !image && (
            <div className="flex flex-col center">
              {EXAMPLES.map(({ display, prompt, image }, i) => (
                <div
                  key={i}
                  className="max-w-[600px] m-1 border dark:border-gray-600 rounded-md p-2 bg-gray-100 dark:bg-gray-700 cursor-pointer"
                  onClick={() => onEnter(prompt, image)}
                >
                  {display ?? prompt}
                </div>
              ))}
            </div>
          )}

          <p className="text-center text-sm min-h-6 text-gray-500 dark:text-gray-300">
            {messages.length > 0 && (
              <>
                {tps ? (
                  <>
                    {!isRunning && (
                      <span>
                        Generated {numTokens} tokens in{" "}
                        {(numTokens / tps).toFixed(2)} seconds&nbsp;&#40;
                      </span>
                    )}
                    <span className="font-medium font-mono text-center mr-1 text-black dark:text-white">
                      {tps.toFixed(2)}
                    </span>
                    <span className="text-gray-500 dark:text-gray-300">
                      tokens/second
                    </span>
                    {!isRunning && <span className="mr-1">&#41;.</span>}
                  </>
                ) : (
                  imageProgress && (
                    <>
                      {isRunning ? (
                        <>
                          <span>Generating image...</span>&nbsp;&#40;
                          <span className="font-medium font-mono text-center text-black dark:text-white">
                            {(imageProgress * 100).toFixed(2)}%
                          </span>
                          <span className="mr-1">&#41;</span>
                        </>
                      ) : (
                        <span>
                          Generated image in{" "}
                          {(imageGenerationTime / 1000).toFixed(2)}{" "}
                          seconds.&nbsp;
                        </span>
                      )}
                    </>
                  )
                )}

                {!isRunning && (
                  <span
                    className="underline cursor-pointer"
                    onClick={() => setMessages([])}
                  >
                    Reset
                  </span>
                )}
              </>
            )}
          </p>
        </div>
      )}

      <div className="mt-2 border dark:bg-gray-700 rounded-lg w-[600px] max-w-[80%] max-h-[200px] mx-auto relative mb-3 flex">
        <label
          htmlFor="file-upload"
          className={
            status === "ready"
              ? "cursor-pointer"
              : "cursor-not-allowed pointer-events-none"
          }
        >
          <ImageIcon
            className={`h-8 w-8 p-1 rounded-md ${status === "ready" ? "text-gray-800 dark:text-gray-100" : "text-gray-400 dark:text-gray-500"} absolute bottom-3 left-1.5`}
          ></ImageIcon>
          <input
            ref={imageUploadRef}
            id="file-upload"
            type="file"
            accept="image/*"
            className="hidden"
            onInput={(e) => {
              const file = e.target.files[0];
              if (!file) {
                return;
              }

              const reader = new FileReader();

              // Set up a callback when the file is loaded
              reader.onload = (e2) => {
                setImage(e2.target.result);
                e.target.value = "";
              };

              reader.readAsDataURL(file);
            }}
          ></input>
        </label>
        <div className="w-full flex flex-col">
          {image && (
            <ImagePreview
              onRemove={() => {
                setImage(null);
              }}
              src={image}
              className="w-20 h-20 min-w-20 min-h-20 relative p-2"
            />
          )}

          <textarea
            ref={textareaRef}
            className="scrollbar-thin w-full pl-11 pr-12 dark:bg-gray-700 py-4 rounded-lg bg-transparent border-none outline-none text-gray-800 disabled:text-gray-400 dark:text-gray-100 placeholder-gray-500 disabled:placeholder-gray-200 dark:placeholder-gray-300 dark:disabled:placeholder-gray-500 resize-none disabled:cursor-not-allowed"
            placeholder="Type message or use '/imagine <prompt>' to generate an image."
            type="text"
            rows={1}
            value={input}
            disabled={status !== "ready"}
            title={
              status === "ready" ? "Model is ready" : "Model not loaded yet"
            }
            onKeyDown={(e) => {
              if (
                input.length > 0 &&
                !isRunning &&
                e.key === "Enter" &&
                !e.shiftKey
              ) {
                e.preventDefault(); // Prevent default behavior of Enter key
                onEnter(input, image);
              }
            }}
            onInput={(e) => setInput(e.target.value)}
          />
        </div>
        {isRunning ? (
          <div className="cursor-pointer" onClick={onInterrupt}>
            <StopIcon className="h-8 w-8 p-1 rounded-md text-gray-800 dark:text-gray-100 absolute right-3 bottom-3" />
          </div>
        ) : input.length > 0 ? (
          <div className="cursor-pointer" onClick={() => onEnter(input)}>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-800 dark:bg-gray-100 text-white dark:text-black rounded-md absolute right-3 bottom-3`}
            />
          </div>
        ) : (
          <div>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-200 dark:bg-gray-600 text-gray-50 dark:text-gray-800 rounded-md absolute right-3 bottom-3`}
            />
          </div>
        )}
      </div>

      <p className="text-xs text-gray-400 text-center mb-3">
        Disclaimer: Generated content may be inaccurate or false.
      </p>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\janus-webgpu\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;

@layer utilities {
  .scrollbar-thin::-webkit-scrollbar {
    @apply w-2;
  }

  .scrollbar-thin::-webkit-scrollbar-track {
    @apply rounded-full bg-gray-100 dark:bg-gray-700;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb {
    @apply rounded-full bg-gray-300 dark:bg-gray-600;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb:hover {
    @apply bg-gray-500;
  }

  .animation-delay-200 {
    animation-delay: 200ms;
  }
  .animation-delay-400 {
    animation-delay: 400ms;
  }

  .overflow-wrap-anywhere {
    overflow-wrap: anywhere;
  }
}


----- .\janus-webgpu\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\janus-webgpu\src\worker.js -----

import {
  AutoProcessor,
  MultiModalityCausalLM,
  BaseStreamer,
  TextStreamer,
  InterruptableStoppingCriteria,
} from "@huggingface/transformers";

// Define constants
const IMAGE_GENERATION_COMMAND_PREFIX = "/imagine ";
const MAX_NEW_TEXT_TOKENS = 1024;

/**
 * Helper function to perform WebGPU feature detection
 */
let fp16_supported = false;
async function check() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) {
      throw new Error("WebGPU is not supported (no adapter found)");
    }
    fp16_supported = adapter.features.has("shader-f16");
    self.postMessage({
      status: "success",
      data: fp16_supported,
    });
  } catch (e) {
    self.postMessage({
      status: "error",
      data: e.toString(),
    });
  }
}

/**
 * This class uses the Singleton pattern to enable lazy-loading of the pipeline
 */
class ImageGenerationPipeline {
  static model_id = "onnx-community/Janus-1.3B-ONNX";

  static async getInstance(progress_callback = null) {
    this.processor ??= AutoProcessor.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= MultiModalityCausalLM.from_pretrained(this.model_id, {
      dtype: fp16_supported
        ? {
            prepare_inputs_embeds: "q4",
            language_model: "q4f16",
            lm_head: "fp16",
            gen_head: "fp16",
            gen_img_embeds: "fp16",
            image_decode: "fp32",
          }
        : {
            prepare_inputs_embeds: "fp32",
            language_model: "q4",
            lm_head: "fp32",
            gen_head: "fp32",
            gen_img_embeds: "fp32",
            image_decode: "fp32",
          },
      device: {
        prepare_inputs_embeds: "wasm", // TODO use "webgpu" when bug is fixed
        language_model: "webgpu",
        lm_head: "webgpu",
        gen_head: "webgpu",
        gen_img_embeds: "webgpu",
        image_decode: "webgpu",
      },
      progress_callback,
    });

    return Promise.all([this.processor, this.model]);
  }
}

class ProgressStreamer extends BaseStreamer {
  constructor(total, on_progress) {
    super();
    this.total = total;
    this.on_progress = on_progress;

    this.count = null;
    this.start_time = null;
  }

  put(value) {
    if (this.count === null) {
      // Ignore the first batch of tokens (prompt)
      this.count = 0;
      this.start_time = performance.now();
      return;
    }

    const progress = ++this.count / this.total;

    this.on_progress({
      count: this.count,
      total: this.total,
      progress,
      time: performance.now() - this.start_time,
    });
  }

  end() {
    /* no nothing */
  }
}

const stopping_criteria = new InterruptableStoppingCriteria();

async function generate(messages) {
  // For this demo, we only respond to the last message
  const message = messages.at(-1);

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  // Load the pipeline
  const [processor, model] = await ImageGenerationPipeline.getInstance();

  // Determine if the user wants to generate an image or text
  if (message.content.startsWith(IMAGE_GENERATION_COMMAND_PREFIX)) {
    const text = message.content.replace(IMAGE_GENERATION_COMMAND_PREFIX, "");

    const conversation = [
      {
        role: "User", // uses title case
        content: text,
      },
    ];
    const inputs = await processor(conversation, {
      chat_template: "text_to_image",
    });

    const callback_function = (output) => {
      self.postMessage({
        status: "image-update",
        ...output,
      });
    };

    const num_image_tokens = processor.num_image_tokens;
    const streamer = new ProgressStreamer(num_image_tokens, callback_function);

    const outputs = await model.generate_images({
      ...inputs,
      min_new_tokens: num_image_tokens,
      max_new_tokens: num_image_tokens,
      do_sample: true,
      streamer,
    });

    const blob = await outputs[0].toBlob();

    // Send the output back to the main thread
    self.postMessage({
      status: "image-update",
      blob,
    });
  } else {
    const inputs = await processor(
      message.image
        ? [
            {
              role: "User",
              content: "<image_placeholder>\n" + message.content,
              images: [message.image],
            },
          ]
        : [
            {
              role: "System",
              content:
                "You are a helpful assistant. Answer the user's questions in a concise manner.",
            },
            {
              role: "User",
              content: message.content,
            },
          ],
    );

    let startTime;
    let numTokens = 0;
    let tps;
    const token_callback_function = () => {
      startTime ??= performance.now();

      if (numTokens++ > 0) {
        tps = (numTokens / (performance.now() - startTime)) * 1000;
      }
    };
    const callback_function = (output) => {
      self.postMessage({
        status: "text-update",
        output,
        tps,
        numTokens,
      });
    };

    const streamer = new TextStreamer(processor.tokenizer, {
      skip_prompt: true,
      skip_special_tokens: true,
      callback_function,
      token_callback_function,
    });

    // Generate response
    const outputs = await model.generate({
      ...inputs,
      max_new_tokens: MAX_NEW_TEXT_TOKENS,
      do_sample: false,
      streamer,
      stopping_criteria,
    });
  }

  // Tell the main thread we are done
  self.postMessage({
    status: "complete",
  });
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [processor, model] = await ImageGenerationPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  self.postMessage({ status: "ready" });
}

// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "check":
      check();
      break;

    case "load":
      load();
      break;

    case "generate":
      stopping_criteria.reset();
      generate(data);
      break;

    case "interrupt":
      stopping_criteria.interrupt();
      break;

    case "reset":
      stopping_criteria.reset();
      break;
  }
});


----- .\janus-webgpu\src\components\Chat.css -----

@scope (.markdown) {
  /* Code blocks */
  pre {
    margin: 0.5rem 0;
    white-space: break-spaces;
  }

  code {
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
    font-size: 0.9em;
  }

  pre,
  code {
    background-color: #f2f2f2;
  }

  @media (prefers-color-scheme: dark) {
    pre,
    code {
      background-color: #333;
    }
  }

  pre:has(code) {
    padding: 1rem 0.5rem;
  }

  pre > code {
    padding: 0;
  }

  /* Headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-weight: 600;
    line-height: 1.2;
  }

  h1 {
    font-size: 2em;
    margin: 1rem 0;
  }

  h2 {
    font-size: 1.5em;
    margin: 0.83rem 0;
  }

  h3 {
    font-size: 1.25em;
    margin: 0.67rem 0;
  }

  h4 {
    font-size: 1em;
    margin: 0.5rem 0;
  }

  h5 {
    font-size: 0.875em;
    margin: 0.33rem 0;
  }

  h6 {
    font-size: 0.75em;
    margin: 0.25rem 0;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6:first-child {
    margin-top: 0;
  }

  /* Unordered List */
  ul {
    list-style-type: disc;
    margin-left: 1.5rem;
  }

  /* Ordered List */
  ol {
    list-style-type: decimal;
    margin-left: 1.5rem;
  }

  /* List Items */
  li {
    margin: 0.25rem 0;
  }

  p:not(:first-child) {
    margin-top: 0.75rem;
  }

  p:not(:last-child) {
    margin-bottom: 0.75rem;
  }
}


----- .\janus-webgpu\src\components\Chat.jsx -----

import { marked } from "marked";
import DOMPurify from "dompurify";

import BotIcon from "./icons/BotIcon";
import UserIcon from "./icons/UserIcon";

import "./Chat.css";
import { useEffect } from "react";

function render(text) {
  return DOMPurify.sanitize(marked.parse(text));
}

export default function Chat({ messages }) {
  const empty = messages.length === 0;

  useEffect(() => {
    window.MathJax.typeset();
  }, [messages]);

  return (
    <div
      className={`flex-1 p-6 max-w-[960px] w-full ${empty ? "flex flex-col items-center justify-end" : "space-y-4"}`}
    >
      {empty ? (
        <div className="text-xl">Ready!</div>
      ) : (
        messages.map((msg, i) => (
          <div key={`message-${i}`} className="flex items-start space-x-4">
            {msg.role === "assistant" ? (
              <>
                <BotIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-gray-200 dark:bg-gray-700 rounded-lg p-4">
                  <p className="min-h-6 text-gray-800 dark:text-gray-200 overflow-wrap-anywhere">
                    {msg.image ? (
                      <img
                        src={msg.image}
                        className="max-w-full w-[384px] rounded-md"
                      />
                    ) : msg.content.length > 0 ? (
                      <span
                        className="markdown"
                        dangerouslySetInnerHTML={{
                          __html: render(msg.content),
                        }}
                      />
                    ) : (
                      <span className="h-6 flex items-center gap-1">
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-200"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-400"></span>
                      </span>
                    )}
                  </p>
                </div>
              </>
            ) : (
              <>
                <UserIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-blue-500 text-white rounded-lg p-4">
                  {msg.image && (
                    <img
                      src={msg.image}
                      className="max-w-full max-h-64 rounded-md mb-3"
                    />
                  )}
                  <p className="min-h-6 overflow-wrap-anywhere">
                    {msg.content}
                  </p>
                </div>
              </>
            )}
          </div>
        ))
      )}
    </div>
  );
}


----- .\janus-webgpu\src\components\ImagePreview.jsx -----

import { useState } from "react";
import CrossIcon from "./icons/CrossIcon";

export default function ImagePreview({ src, onRemove, ...props }) {
  const [hover, setHover] = useState(false);

  return (
    <div
      {...props}
      onMouseEnter={() => setHover(true)}
      onMouseLeave={() => setHover(false)}
    >
      <CrossIcon
        onClick={onRemove}
        className={`absolute top-0 right-0 cursor-pointer dark:fill-gray-400 dark:text-gray-100 fill-gray-200 text-gray-800 ${hover ? "" : "hidden"}`}
      />
      <img
        src={src}
        alt="Upload preview"
        className="w-full h-full object-cover rounded-md"
      />
    </div>
  );
}


----- .\janus-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\janus-webgpu\src\components\icons\ArrowRightIcon.jsx -----

export default function ArrowRightIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M5 12h14" />
      <path d="m12 5 7 7-7 7" />
    </svg>
  );
}


----- .\janus-webgpu\src\components\icons\BotIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M12 8V4H8" />
      <rect width="16" height="12" x="4" y="8" rx="2" />
      <path d="M2 14h2" />
      <path d="M20 14h2" />
      <path d="M15 13v2" />
      <path d="M9 13v2" />
    </svg>
  );
}


----- .\janus-webgpu\src\components\icons\CrossIcon.jsx -----

export default function CrossIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="m9.75 9.75 4.5 4.5m0-4.5-4.5 4.5M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
    </svg>
  );
}


----- .\janus-webgpu\src\components\icons\ImageIcon.jsx -----

export default function ImageIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="m2.25 15.75 5.159-5.159a2.25 2.25 0 0 1 3.182 0l5.159 5.159m-1.5-1.5 1.409-1.409a2.25 2.25 0 0 1 3.182 0l2.909 2.909m-18 3.75h16.5a1.5 1.5 0 0 0 1.5-1.5V6a1.5 1.5 0 0 0-1.5-1.5H3.75A1.5 1.5 0 0 0 2.25 6v12a1.5 1.5 0 0 0 1.5 1.5Zm10.5-11.25h.008v.008h-.008V8.25Zm.375 0a.375.375 0 1 1-.75 0 .375.375 0 0 1 .75 0Z" />
    </svg>
  );
}


----- .\janus-webgpu\src\components\icons\StopIcon.jsx -----

export default function StopIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
      <path
        fill="currentColor"
        d="M9 9.563C9 9.252 9.252 9 9.563 9h4.874c.311 0 .563.252.563.563v4.874c0 .311-.252.563-.563.563H9.564A.562.562 0 0 1 9 14.437V9.564Z"
      />
    </svg>
  );
}


----- .\janus-webgpu\src\components\icons\UserIcon.jsx -----

export default function UserIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2" />
      <circle cx="12" cy="7" r="4" />
    </svg>
  );
}


----- .\llama-3.2-reasoning-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import Chat from "./components/Chat";
import ArrowRightIcon from "./components/icons/ArrowRightIcon";
import StopIcon from "./components/icons/StopIcon";
import Progress from "./components/Progress";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;
const STICKY_SCROLL_THRESHOLD = 120;
const EXAMPLES = [
  "Solve the equation x^2 - 3x + 2 = 0",
  "Lily is three times older than her son. In 15 years, she will be twice as old as him. How old is she now?",
  "Write python code to compute the nth fibonacci number.",
];

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const textareaRef = useRef(null);
  const chatContainerRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);
  const [isRunning, setIsRunning] = useState(false);

  // Inputs and outputs
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState([]);
  const [tps, setTps] = useState(null);
  const [numTokens, setNumTokens] = useState(null);

  function onEnter(message) {
    setMessages((prev) => [...prev, { role: "user", content: message }]);
    setTps(null);
    setIsRunning(true);
    setInput("");
  }

  function onInterrupt() {
    // NOTE: We do not set isRunning to false here because the worker
    // will send a 'complete' message when it is done.
    worker.current.postMessage({ type: "interrupt" });
  }

  useEffect(() => {
    resizeInput();
  }, [input]);

  function resizeInput() {
    if (!textareaRef.current) return;

    const target = textareaRef.current;
    target.style.height = "auto";
    const newHeight = Math.min(Math.max(target.scrollHeight, 24), 200);
    target.style.height = `${newHeight}px`;
  }

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    if (!worker.current) {
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
      worker.current.postMessage({ type: "check" }); // Do a feature check
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "start":
          {
            // Start generation
            setMessages((prev) => [
              ...prev,
              { role: "assistant", content: "" },
            ]);
          }
          break;

        case "update":
          {
            // Generation update: update the output text.
            // Parse messages
            const { output, tps, numTokens, state } = e.data;
            setTps(tps);
            setNumTokens(numTokens);
            setMessages((prev) => {
              const cloned = [...prev];
              const last = cloned.at(-1);
              const data = {
                ...last,
                content: last.content + output,
              };
              if (data.answerIndex === undefined && state === "answering") {
                // When state changes to answering, we set the answerIndex
                data.answerIndex = last.content.length;
              }
              cloned[cloned.length - 1] = data;
              return cloned;
            });
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsRunning(false);
          break;

        case "error":
          setError(e.data.data);
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
    };
  }, []);

  // Send the messages to the worker thread whenever the `messages` state changes.
  useEffect(() => {
    if (messages.filter((x) => x.role === "user").length === 0) {
      // No user messages yet: do nothing.
      return;
    }
    if (messages.at(-1).role === "assistant") {
      // Do not update if the last message is from the assistant
      return;
    }
    setTps(null);
    worker.current.postMessage({ type: "generate", data: messages });
  }, [messages, isRunning]);

  useEffect(() => {
    if (!chatContainerRef.current || !isRunning) return;
    const element = chatContainerRef.current;
    if (
      element.scrollHeight - element.scrollTop - element.clientHeight <
      STICKY_SCROLL_THRESHOLD
    ) {
      element.scrollTop = element.scrollHeight;
    }
  }, [messages, isRunning]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto items justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {status === null && messages.length === 0 && (
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[360px] text-center">
            <img
              src="logo.png"
              width="100%"
              height="auto"
              className="block drop-shadow-lg bg-transparent"
            ></img>
            <h1 className="text-4xl font-bold mb-1">Llama-3.2 Reasoning</h1>
            <h2 className="font-semibold">
              A blazingly fast and powerful reasoning AI chatbot that runs
              locally in your browser.
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            <p className="max-w-[480px] mb-4">
              <br />
              You are about to load{" "}
              <a
                href="https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                MiniThinky-v2
              </a>
              , a 1.2B parameter reasoning LLM optimized for in-browser
              inference. Everything runs entirely in your browser with{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning no data is sent to a server. Once
              loaded, it can even be used offline. The source code for the demo
              is available on{" "}
              <a
                href="https://github.com/huggingface/transformers.js-examples/tree/main/llama-3.2-reasoning-webgpu"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                GitHub
              </a>
              .
            </p>

            {error && (
              <div className="text-red-500 text-center mb-2">
                <p className="mb-1">
                  Unable to load model due to the following error:
                </p>
                <p className="text-sm">{error}</p>
              </div>
            )}

            <button
              className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 cursor-pointer disabled:cursor-not-allowed select-none"
              onClick={() => {
                worker.current.postMessage({ type: "load" });
                setStatus("loading");
              }}
              disabled={status !== null || error !== null}
            >
              Load model
            </button>
          </div>
        </div>
      )}
      {status === "loading" && (
        <>
          <div className="w-full max-w-[500px] text-left mx-auto p-4 bottom-0 mt-auto">
            <p className="text-center mb-1">{loadingMessage}</p>
            {progressItems.map(({ file, progress, total }, i) => (
              <Progress
                key={i}
                text={file}
                percentage={progress}
                total={total}
              />
            ))}
          </div>
        </>
      )}

      {status === "ready" && (
        <div
          ref={chatContainerRef}
          className="overflow-y-auto scrollbar-thin w-full flex flex-col items-center h-full"
        >
          <Chat messages={messages} />
          {messages.length === 0 && (
            <div>
              {EXAMPLES.map((msg, i) => (
                <div
                  key={i}
                  className="m-1 border border-gray-300 dark:border-gray-600 rounded-md p-2 bg-gray-100 dark:bg-gray-700 cursor-pointer"
                  onClick={() => onEnter(msg)}
                >
                  {msg}
                </div>
              ))}
            </div>
          )}
          <p className="text-center text-sm min-h-6 text-gray-500 dark:text-gray-300">
            {tps && messages.length > 0 && (
              <>
                {!isRunning && (
                  <span>
                    Generated {numTokens} tokens in{" "}
                    {(numTokens / tps).toFixed(2)} seconds&nbsp;&#40;
                  </span>
                )}
                {
                  <>
                    <span className="font-medium text-center mr-1 text-black dark:text-white">
                      {tps.toFixed(2)}
                    </span>
                    <span className="text-gray-500 dark:text-gray-300">
                      tokens/second
                    </span>
                  </>
                }
                {!isRunning && (
                  <>
                    <span className="mr-1">&#41;.</span>
                    <span
                      className="underline cursor-pointer"
                      onClick={() => {
                        worker.current.postMessage({ type: "reset" });
                        setMessages([]);
                      }}
                    >
                      Reset
                    </span>
                  </>
                )}
              </>
            )}
          </p>
        </div>
      )}

      <div className="mt-2 border border-gray-300 dark:bg-gray-700 rounded-lg w-[600px] max-w-[80%] max-h-[200px] mx-auto relative mb-3 flex">
        <textarea
          ref={textareaRef}
          className="scrollbar-thin w-[550px] dark:bg-gray-700 px-3 py-4 rounded-lg bg-transparent border-none outline-hidden text-gray-800 disabled:text-gray-400 dark:text-gray-200 placeholder-gray-500 dark:placeholder-gray-400 disabled:placeholder-gray-200 resize-none disabled:cursor-not-allowed"
          placeholder="Type your message..."
          type="text"
          rows={1}
          value={input}
          disabled={status !== "ready"}
          title={status === "ready" ? "Model is ready" : "Model not loaded yet"}
          onKeyDown={(e) => {
            if (
              input.length > 0 &&
              !isRunning &&
              e.key === "Enter" &&
              !e.shiftKey
            ) {
              e.preventDefault(); // Prevent default behavior of Enter key
              onEnter(input);
            }
          }}
          onInput={(e) => setInput(e.target.value)}
        />
        {isRunning ? (
          <div className="cursor-pointer" onClick={onInterrupt}>
            <StopIcon className="h-8 w-8 p-1 rounded-md text-gray-800 dark:text-gray-100 absolute right-3 bottom-3" />
          </div>
        ) : input.length > 0 ? (
          <div className="cursor-pointer" onClick={() => onEnter(input)}>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-800 dark:bg-gray-100 text-white dark:text-black rounded-md absolute right-3 bottom-3`}
            />
          </div>
        ) : (
          <div>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-200 dark:bg-gray-600 text-gray-50 dark:text-gray-800 rounded-md absolute right-3 bottom-3`}
            />
          </div>
        )}
      </div>

      <p className="text-xs text-gray-400 text-center mb-3">
        Disclaimer: Generated content may be inaccurate or false.
      </p>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\llama-3.2-reasoning-webgpu\src\index.css -----

@import "tailwindcss";

/* Custom scrollbar styles */
.scrollbar-thin::-webkit-scrollbar {
  width: 0.5rem; /* Equivalent to w-2 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-track {
  border-radius: 9999px; /* Equivalent to rounded-full in Tailwind */
  background-color: #f3f4f6; /* Equivalent to bg-gray-100 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-track.dark {
  background-color: #374151; /* Equivalent to dark:bg-gray-700 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-thumb {
  border-radius: 9999px; /* Equivalent to rounded-full in Tailwind */
  background-color: #d1d5db; /* Equivalent to bg-gray-300 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-thumb:hover {
  background-color: #6b7280; /* Equivalent to bg-gray-500 in Tailwind */
}

/* Animation delay classes */
.animation-delay-200 {
  animation-delay: 200ms;
}

.animation-delay-400 {
  animation-delay: 400ms;
}

/* Overflow wrap class */
.overflow-wrap-anywhere {
  overflow-wrap: anywhere;
}


----- .\llama-3.2-reasoning-webgpu\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import "./index.css";
import App from "./App.jsx";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\llama-3.2-reasoning-webgpu\src\worker.js -----

import {
  AutoTokenizer,
  AutoModelForCausalLM,
  TextStreamer,
  InterruptableStoppingCriteria,
} from "@huggingface/transformers";

/**
 * Helper function to perform feature detection for WebGPU
 */
// let fp16_supported = false;
async function check() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) {
      throw new Error("WebGPU is not supported (no adapter found)");
    }
    // fp16_supported = adapter.features.has("shader-f16")
  } catch (e) {
    self.postMessage({
      status: "error",
      data: e.toString(),
    });
  }
}

/**
 * This class uses the Singleton pattern to enable lazy-loading of the pipeline
 */
class TextGenerationPipeline {
  static model_id = "ngxson/MiniThinky-v2-1B-Llama-3.2";

  static async getInstance(progress_callback = null) {
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= AutoModelForCausalLM.from_pretrained(this.model_id, {
      dtype: "q4f16", // TODO: use "q4" as fallback when fixed
      device: "webgpu",
      progress_callback,
    });

    return Promise.all([this.tokenizer, this.model]);
  }
}

const stopping_criteria = new InterruptableStoppingCriteria();

let past_key_values_cache = null;
async function generate(messages) {
  // Retrieve the text-generation pipeline.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance();

  messages = [
    {
      role: "system",
      content:
        "You are MiniThinky, a helpful AI assistant. You always think before giving the answer. Use <|thinking|> before thinking and <|answer|> before giving the answer.",
    },
    ...messages,
  ];
  const inputs = tokenizer.apply_chat_template(messages, {
    add_generation_prompt: true,
    return_dict: true,
  });

  // 128011: <|thinking|>
  // 128012: <|answer|>
  const [THINKING_TOKEN_ID, ANSWER_TOKEN_ID] = tokenizer.encode(
    "<|thinking|><|answer|>",
    { add_special_tokens: false },
  );

  let state = "thinking"; // 'thinking' or 'answering'
  let startTime;
  let numTokens = 0;
  let tps;
  const token_callback_function = (tokens) => {
    startTime ??= performance.now();

    if (numTokens++ > 0) {
      tps = (numTokens / (performance.now() - startTime)) * 1000;
    }
    if (tokens[0] == ANSWER_TOKEN_ID) {
      state = "answering";
    }
  };
  const callback_function = (output) => {
    self.postMessage({
      status: "update",
      output,
      tps,
      numTokens,
      state,
    });
  };

  const streamer = new TextStreamer(tokenizer, {
    skip_prompt: true,
    skip_special_tokens: true,
    callback_function,
    token_callback_function,
  });

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  const { past_key_values, sequences } = await model.generate({
    ...inputs,
    // TODO: Add back when fixed
    // past_key_values: past_key_values_cache,

    // Sampling
    do_sample: false,
    repetition_penalty: 1.1,
    // top_k: 3,
    // temperature: 0.2,

    max_new_tokens: 2048,
    streamer,
    stopping_criteria,
    return_dict_in_generate: true,
  });
  past_key_values_cache = past_key_values;

  const decoded = tokenizer.batch_decode(sequences, {
    skip_special_tokens: true,
  });

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: decoded,
  });
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  self.postMessage({
    status: "loading",
    data: "Compiling shaders and warming up model...",
  });

  // Run model with dummy input to compile shaders
  const inputs = tokenizer("a");
  await model.generate({ ...inputs, max_new_tokens: 1 });
  self.postMessage({ status: "ready" });
}
// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "check":
      check();
      break;

    case "load":
      load();
      break;

    case "generate":
      stopping_criteria.reset();
      generate(data);
      break;

    case "interrupt":
      stopping_criteria.interrupt();
      break;

    case "reset":
      past_key_values_cache = null;
      stopping_criteria.reset();
      break;
  }
});


----- .\llama-3.2-reasoning-webgpu\src\components\Chat.css -----

@scope (.markdown) {
  /* Code blocks */
  pre {
    margin: 0.5rem 0;
    white-space: break-spaces;
  }

  code {
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
    font-size: 0.9em;
  }

  pre,
  code {
    background-color: #f2f2f2;
  }

  @media (prefers-color-scheme: dark) {
    pre,
    code {
      background-color: #333;
    }
  }

  pre:has(code) {
    padding: 1rem 0.5rem;
  }

  pre > code {
    padding: 0;
  }

  /* Headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-weight: 600;
    line-height: 1.2;
  }

  h1 {
    font-size: 2em;
    margin: 1rem 0;
  }

  h2 {
    font-size: 1.5em;
    margin: 0.83rem 0;
  }

  h3 {
    font-size: 1.25em;
    margin: 0.67rem 0;
  }

  h4 {
    font-size: 1em;
    margin: 0.5rem 0;
  }

  h5 {
    font-size: 0.875em;
    margin: 0.33rem 0;
  }

  h6 {
    font-size: 0.75em;
    margin: 0.25rem 0;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6:first-child {
    margin-top: 0;
  }

  /* Unordered List */
  ul {
    list-style-type: disc;
    margin-left: 1.5rem;
  }

  /* Ordered List */
  ol {
    list-style-type: decimal;
    margin-left: 1.5rem;
  }

  /* List Items */
  li {
    margin: 0.25rem 0;
  }

  p:not(:first-child) {
    margin-top: 0.75rem;
  }

  p:not(:last-child) {
    margin-bottom: 0.75rem;
  }

  ul > li {
    margin-left: 1rem;
  }

  /* Table */
  table,
  th,
  td {
    border: 1px solid lightgray;
    padding: 0.25rem;
  }

  @media (prefers-color-scheme: dark) {
    table,
    th,
    td {
      border: 1px solid #f2f2f2;
    }
  }
}


----- .\llama-3.2-reasoning-webgpu\src\components\Chat.jsx -----

import { useState } from "react";
import { marked } from "marked";
import DOMPurify from "dompurify";

import BotIcon from "./icons/BotIcon";
import BrainIcon from "./icons/BrainIcon";
import UserIcon from "./icons/UserIcon";

import { MathJaxContext, MathJax } from "better-react-mathjax";
import "./Chat.css";

function render(text) {
  // Replace all instances of single backslashes before brackets with double backslashes
  // See https://github.com/markedjs/marked/issues/546 for more information.
  text = text.replace(/\\([\[\]\(\)])/g, "\\\\$1");

  const result = DOMPurify.sanitize(
    marked.parse(text, {
      async: false,
      breaks: true,
    }),
  );
  return result;
}
function Message({ role, content, answerIndex }) {
  const thinking = answerIndex ? content.slice(0, answerIndex) : content;
  const answer = answerIndex ? content.slice(answerIndex) : "";

  const [showThinking, setShowThinking] = useState(false);

  const doneThinking = answer.length > 0;

  return (
    <div className="flex items-start space-x-4">
      {role === "assistant" ? (
        <>
          <BotIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
          <div className="bg-gray-200 dark:bg-gray-700 rounded-lg p-4">
            <div className="min-h-6 text-gray-800 dark:text-gray-200 overflow-wrap-anywhere">
              {thinking.length > 0 ? (
                <>
                  <div className="bg-white dark:bg-gray-800 rounded-lg flex flex-col">
                    <button
                      className="flex items-center gap-2 cursor-pointer p-4 hover:bg-gray-50 dark:hover:bg-gray-900 rounded-lg "
                      onClick={() => setShowThinking((prev) => !prev)}
                      style={{ width: showThinking ? "100%" : "auto" }}
                    >
                      <BrainIcon
                        className={doneThinking ? "" : "animate-pulse"}
                      />
                      <span>
                        {doneThinking ? "View reasoning." : "Thinking..."}
                      </span>
                      <span className="ml-auto text-gray-700">
                        {showThinking ? "â–²" : "â–¼"}
                      </span>
                    </button>
                    {showThinking && (
                      <MathJax
                        className="border-t border-gray-200 dark:border-gray-700 px-4 py-2"
                        dynamic
                      >
                        <span
                          className="markdown"
                          dangerouslySetInnerHTML={{
                            __html: render(thinking),
                          }}
                        />
                      </MathJax>
                    )}
                  </div>
                  {doneThinking && (
                    <MathJax className="mt-2" dynamic>
                      <span
                        className="markdown"
                        dangerouslySetInnerHTML={{
                          __html: render(answer),
                        }}
                      />
                    </MathJax>
                  )}
                </>
              ) : (
                <span className="h-6 flex items-center gap-1">
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse"></span>
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-200"></span>
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-400"></span>
                </span>
              )}
            </div>
          </div>
        </>
      ) : (
        <>
          <UserIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
          <div className="bg-blue-500 text-white rounded-lg p-4">
            <p className="min-h-6 overflow-wrap-anywhere">{content}</p>
          </div>
        </>
      )}
    </div>
  );
}

export default function Chat({ messages }) {
  const empty = messages.length === 0;

  return (
    <div
      className={`flex-1 p-6 max-w-[960px] w-full ${empty ? "flex flex-col items-center justify-end" : "space-y-4"}`}
    >
      <MathJaxContext>
        {empty ? (
          <div className="text-xl">Ready!</div>
        ) : (
          messages.map((msg, i) => <Message key={`message-${i}`} {...msg} />)
        )}
      </MathJaxContext>
    </div>
  );
}


----- .\llama-3.2-reasoning-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\llama-3.2-reasoning-webgpu\src\components\icons\ArrowRightIcon.jsx -----

export default function ArrowRightIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M5 12h14" />
      <path d="m12 5 7 7-7 7" />
    </svg>
  );
}


----- .\llama-3.2-reasoning-webgpu\src\components\icons\BotIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M12 8V4H8" />
      <rect width="16" height="12" x="4" y="8" rx="2" />
      <path d="M2 14h2" />
      <path d="M20 14h2" />
      <path d="M15 13v2" />
      <path d="M9 13v2" />
    </svg>
  );
}


----- .\llama-3.2-reasoning-webgpu\src\components\icons\BrainIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 32 32"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path
        className="stroke-gray-600 dark:stroke-gray-400"
        d="M16 6v3.33M16 6c0-2.65 3.25-4.3 5.4-2.62 1.2.95 1.6 2.65.95 4.04a3.63 3.63 0 0 1 4.61.16 3.45 3.45 0 0 1 .46 4.37 5.32 5.32 0 0 1 1.87 4.75c-.22 1.66-1.39 3.6-3.07 4.14M16 6c0-2.65-3.25-4.3-5.4-2.62a3.37 3.37 0 0 0-.95 4.04 3.65 3.65 0 0 0-4.6.16 3.37 3.37 0 0 0-.49 4.27 5.57 5.57 0 0 0-1.85 4.85 5.3 5.3 0 0 0 3.07 4.15M16 9.33v17.34m0-17.34c0 2.18 1.82 4 4 4m6.22 7.5c.67 1.3.56 2.91-.27 4.11a4.05 4.05 0 0 1-4.62 1.5c0 1.53-1.05 2.9-2.66 2.9A2.7 2.7 0 0 1 16 26.66m10.22-5.83a4.05 4.05 0 0 0-3.55-2.17m-16.9 2.18a4.05 4.05 0 0 0 .28 4.1c1 1.44 2.92 2.09 4.59 1.5 0 1.52 1.12 2.88 2.7 2.88A2.7 2.7 0 0 0 16 26.67M5.78 20.85a4.04 4.04 0 0 1 3.55-2.18"
      ></path>
    </svg>
  );
}


----- .\llama-3.2-reasoning-webgpu\src\components\icons\StopIcon.jsx -----

export default function StopIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
      <path
        fill="currentColor"
        d="M9 9.563C9 9.252 9.252 9 9.563 9h4.874c.311 0 .563.252.563.563v4.874c0 .311-.252.563-.563.563H9.564A.562.562 0 0 1 9 14.437V9.564Z"
      />
    </svg>
  );
}


----- .\llama-3.2-reasoning-webgpu\src\components\icons\UserIcon.jsx -----

export default function UserIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2" />
      <circle cx="12" cy="7" r="4" />
    </svg>
  );
}


----- .\llama-3.2-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import Chat from "./components/Chat";
import ArrowRightIcon from "./components/icons/ArrowRightIcon";
import StopIcon from "./components/icons/StopIcon";
import Progress from "./components/Progress";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;
const STICKY_SCROLL_THRESHOLD = 120;
const EXAMPLES = [
  "Give me some tips to improve my time management skills.",
  "What is the difference between AI and ML?",
  "Write python code to compute the nth fibonacci number.",
];

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const textareaRef = useRef(null);
  const chatContainerRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);
  const [isRunning, setIsRunning] = useState(false);

  // Inputs and outputs
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState([]);
  const [tps, setTps] = useState(null);
  const [numTokens, setNumTokens] = useState(null);

  function onEnter(message) {
    setMessages((prev) => [...prev, { role: "user", content: message }]);
    setTps(null);
    setIsRunning(true);
    setInput("");
  }

  function onInterrupt() {
    // NOTE: We do not set isRunning to false here because the worker
    // will send a 'complete' message when it is done.
    worker.current.postMessage({ type: "interrupt" });
  }

  useEffect(() => {
    resizeInput();
  }, [input]);

  function resizeInput() {
    if (!textareaRef.current) return;

    const target = textareaRef.current;
    target.style.height = "auto";
    const newHeight = Math.min(Math.max(target.scrollHeight, 24), 200);
    target.style.height = `${newHeight}px`;
  }

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    if (!worker.current) {
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
      worker.current.postMessage({ type: "check" }); // Do a feature check
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "start":
          {
            // Start generation
            setMessages((prev) => [
              ...prev,
              { role: "assistant", content: "" },
            ]);
          }
          break;

        case "update":
          {
            // Generation update: update the output text.
            // Parse messages
            const { output, tps, numTokens } = e.data;
            setTps(tps);
            setNumTokens(numTokens);
            setMessages((prev) => {
              const cloned = [...prev];
              const last = cloned.at(-1);
              cloned[cloned.length - 1] = {
                ...last,
                content: last.content + output,
              };
              return cloned;
            });
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsRunning(false);
          break;

        case "error":
          setError(e.data.data);
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
    };
  }, []);

  // Send the messages to the worker thread whenever the `messages` state changes.
  useEffect(() => {
    if (messages.filter((x) => x.role === "user").length === 0) {
      // No user messages yet: do nothing.
      return;
    }
    if (messages.at(-1).role === "assistant") {
      // Do not update if the last message is from the assistant
      return;
    }
    setTps(null);
    worker.current.postMessage({ type: "generate", data: messages });
  }, [messages, isRunning]);

  useEffect(() => {
    if (!chatContainerRef.current || !isRunning) return;
    const element = chatContainerRef.current;
    if (
      element.scrollHeight - element.scrollTop - element.clientHeight <
      STICKY_SCROLL_THRESHOLD
    ) {
      element.scrollTop = element.scrollHeight;
    }
  }, [messages, isRunning]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto items justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {status === null && messages.length === 0 && (
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[340px] text-center">
            <img
              src="logo.png"
              width="75%"
              height="auto"
              className="block"
            ></img>
            <h1 className="text-4xl font-bold mb-1">Llama-3.2 WebGPU</h1>
            <h2 className="font-semibold">
              A private and powerful AI chatbot <br />
              that runs locally in your browser.
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            <p className="max-w-[514px] mb-4">
              <br />
              You are about to load{" "}
              <a
                href="https://huggingface.co/onnx-community/Llama-3.2-1B-Instruct-q4f16"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                Llama-3.2-1B-Instruct
              </a>
              , a 1.24 billion parameter LLM that is optimized for inference on
              the web. Once downloaded, the model (1.15&nbsp;GB) will be cached
              and reused when you revisit the page.
              <br />
              <br />
              Everything runs directly in your browser using{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning your conversations aren&#39;t sent
              to a server. You can even disconnect from the internet after the
              model has loaded!
              <br />
              Want to learn more? Check out the demo's source code on{" "}
              <a
                href="https://github.com/huggingface/transformers.js-examples/tree/main/llama-3.2-webgpu"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                GitHub
              </a>
              !
            </p>

            {error && (
              <div className="text-red-500 text-center mb-2">
                <p className="mb-1">
                  Unable to load model due to the following error:
                </p>
                <p className="text-sm">{error}</p>
              </div>
            )}

            <button
              className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 disabled:cursor-not-allowed select-none"
              onClick={() => {
                worker.current.postMessage({ type: "load" });
                setStatus("loading");
              }}
              disabled={status !== null || error !== null}
            >
              Load model
            </button>
          </div>
        </div>
      )}
      {status === "loading" && (
        <>
          <div className="w-full max-w-[500px] text-left mx-auto p-4 bottom-0 mt-auto">
            <p className="text-center mb-1">{loadingMessage}</p>
            {progressItems.map(({ file, progress, total }, i) => (
              <Progress
                key={i}
                text={file}
                percentage={progress}
                total={total}
              />
            ))}
          </div>
        </>
      )}

      {status === "ready" && (
        <div
          ref={chatContainerRef}
          className="overflow-y-auto scrollbar-thin w-full flex flex-col items-center h-full"
        >
          <Chat messages={messages} />
          {messages.length === 0 && (
            <div>
              {EXAMPLES.map((msg, i) => (
                <div
                  key={i}
                  className="m-1 border dark:border-gray-600 rounded-md p-2 bg-gray-100 dark:bg-gray-700 cursor-pointer"
                  onClick={() => onEnter(msg)}
                >
                  {msg}
                </div>
              ))}
            </div>
          )}
          <p className="text-center text-sm min-h-6 text-gray-500 dark:text-gray-300">
            {tps && messages.length > 0 && (
              <>
                {!isRunning && (
                  <span>
                    Generated {numTokens} tokens in{" "}
                    {(numTokens / tps).toFixed(2)} seconds&nbsp;&#40;
                  </span>
                )}
                {
                  <>
                    <span className="font-medium text-center mr-1 text-black dark:text-white">
                      {tps.toFixed(2)}
                    </span>
                    <span className="text-gray-500 dark:text-gray-300">
                      tokens/second
                    </span>
                  </>
                }
                {!isRunning && (
                  <>
                    <span className="mr-1">&#41;.</span>
                    <span
                      className="underline cursor-pointer"
                      onClick={() => {
                        worker.current.postMessage({ type: "reset" });
                        setMessages([]);
                      }}
                    >
                      Reset
                    </span>
                  </>
                )}
              </>
            )}
          </p>
        </div>
      )}

      <div className="mt-2 border dark:bg-gray-700 rounded-lg w-[600px] max-w-[80%] max-h-[200px] mx-auto relative mb-3 flex">
        <textarea
          ref={textareaRef}
          className="scrollbar-thin w-[550px] dark:bg-gray-700 px-3 py-4 rounded-lg bg-transparent border-none outline-none text-gray-800 disabled:text-gray-400 dark:text-gray-200 placeholder-gray-500 dark:placeholder-gray-400 disabled:placeholder-gray-200 resize-none disabled:cursor-not-allowed"
          placeholder="Type your message..."
          type="text"
          rows={1}
          value={input}
          disabled={status !== "ready"}
          title={status === "ready" ? "Model is ready" : "Model not loaded yet"}
          onKeyDown={(e) => {
            if (
              input.length > 0 &&
              !isRunning &&
              e.key === "Enter" &&
              !e.shiftKey
            ) {
              e.preventDefault(); // Prevent default behavior of Enter key
              onEnter(input);
            }
          }}
          onInput={(e) => setInput(e.target.value)}
        />
        {isRunning ? (
          <div className="cursor-pointer" onClick={onInterrupt}>
            <StopIcon className="h-8 w-8 p-1 rounded-md text-gray-800 dark:text-gray-100 absolute right-3 bottom-3" />
          </div>
        ) : input.length > 0 ? (
          <div className="cursor-pointer" onClick={() => onEnter(input)}>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-800 dark:bg-gray-100 text-white dark:text-black rounded-md absolute right-3 bottom-3`}
            />
          </div>
        ) : (
          <div>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-200 dark:bg-gray-600 text-gray-50 dark:text-gray-800 rounded-md absolute right-3 bottom-3`}
            />
          </div>
        )}
      </div>

      <p className="text-xs text-gray-400 text-center mb-3">
        Disclaimer: Generated content may be inaccurate or false.
      </p>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\llama-3.2-webgpu\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;

@layer utilities {
  .scrollbar-thin::-webkit-scrollbar {
    @apply w-2;
  }

  .scrollbar-thin::-webkit-scrollbar-track {
    @apply rounded-full bg-gray-100 dark:bg-gray-700;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb {
    @apply rounded-full bg-gray-300 dark:bg-gray-600;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb:hover {
    @apply bg-gray-500;
  }

  .animation-delay-200 {
    animation-delay: 200ms;
  }
  .animation-delay-400 {
    animation-delay: 400ms;
  }

  .overflow-wrap-anywhere {
    overflow-wrap: anywhere;
  }
}


----- .\llama-3.2-webgpu\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\llama-3.2-webgpu\src\worker.js -----

import {
  AutoTokenizer,
  AutoModelForCausalLM,
  TextStreamer,
  InterruptableStoppingCriteria,
} from "@huggingface/transformers";

/**
 * This class uses the Singleton pattern to enable lazy-loading of the pipeline
 */
class TextGenerationPipeline {
  static model_id = "onnx-community/Llama-3.2-1B-Instruct-q4f16";

  static async getInstance(progress_callback = null) {
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= AutoModelForCausalLM.from_pretrained(this.model_id, {
      dtype: "q4f16",
      device: "webgpu",
      progress_callback,
    });

    return Promise.all([this.tokenizer, this.model]);
  }
}

const stopping_criteria = new InterruptableStoppingCriteria();

let past_key_values_cache = null;
async function generate(messages) {
  // Retrieve the text-generation pipeline.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance();

  const inputs = tokenizer.apply_chat_template(messages, {
    add_generation_prompt: true,
    return_dict: true,
  });

  let startTime;
  let numTokens = 0;
  let tps;
  const token_callback_function = () => {
    startTime ??= performance.now();

    if (numTokens++ > 0) {
      tps = (numTokens / (performance.now() - startTime)) * 1000;
    }
  };
  const callback_function = (output) => {
    self.postMessage({
      status: "update",
      output,
      tps,
      numTokens,
    });
  };

  const streamer = new TextStreamer(tokenizer, {
    skip_prompt: true,
    skip_special_tokens: true,
    callback_function,
    token_callback_function,
  });

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  const { past_key_values, sequences } = await model.generate({
    ...inputs,
    // TODO: Add when model is fixed
    // past_key_values: past_key_values_cache,

    // Sampling
    do_sample: false,

    max_new_tokens: 1024,
    streamer,
    stopping_criteria,
    return_dict_in_generate: true,
  });
  // past_key_values_cache = past_key_values;

  const decoded = tokenizer.batch_decode(sequences, {
    skip_special_tokens: true,
  });

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: decoded,
  });
}

async function check() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) {
      throw new Error("WebGPU is not supported (no adapter found)");
    }
  } catch (e) {
    self.postMessage({
      status: "error",
      data: e.toString(),
    });
  }
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  self.postMessage({
    status: "loading",
    data: "Compiling shaders and warming up model...",
  });

  // Run model with dummy input to compile shaders
  const inputs = tokenizer("a");
  await model.generate({ ...inputs, max_new_tokens: 1 });
  self.postMessage({ status: "ready" });
}
// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "check":
      check();
      break;

    case "load":
      load();
      break;

    case "generate":
      stopping_criteria.reset();
      generate(data);
      break;

    case "interrupt":
      stopping_criteria.interrupt();
      break;

    case "reset":
      // past_key_values_cache = null;
      stopping_criteria.reset();
      break;
  }
});


----- .\llama-3.2-webgpu\src\components\Chat.css -----

@scope (.markdown) {
  /* Code blocks */
  pre {
    margin: 0.5rem 0;
    white-space: break-spaces;
  }

  code {
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
    font-size: 0.9em;
  }

  pre,
  code {
    background-color: #f2f2f2;
  }

  @media (prefers-color-scheme: dark) {
    pre,
    code {
      background-color: #333;
    }
  }

  pre:has(code) {
    padding: 1rem 0.5rem;
  }

  pre > code {
    padding: 0;
  }

  /* Headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-weight: 600;
    line-height: 1.2;
  }

  h1 {
    font-size: 2em;
    margin: 1rem 0;
  }

  h2 {
    font-size: 1.5em;
    margin: 0.83rem 0;
  }

  h3 {
    font-size: 1.25em;
    margin: 0.67rem 0;
  }

  h4 {
    font-size: 1em;
    margin: 0.5rem 0;
  }

  h5 {
    font-size: 0.875em;
    margin: 0.33rem 0;
  }

  h6 {
    font-size: 0.75em;
    margin: 0.25rem 0;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6:first-child {
    margin-top: 0;
  }

  /* Unordered List */
  ul {
    list-style-type: disc;
    margin-left: 1.5rem;
  }

  /* Ordered List */
  ol {
    list-style-type: decimal;
    margin-left: 1.5rem;
  }

  /* List Items */
  li {
    margin: 0.25rem 0;
  }

  p:not(:first-child) {
    margin-top: 0.75rem;
  }

  p:not(:last-child) {
    margin-bottom: 0.75rem;
  }

  ul > li {
    margin-left: 1rem;
  }

  /* Table */
  table,
  th,
  td {
    border: 1px solid lightgray;
    padding: 0.25rem;
  }

  @media (prefers-color-scheme: dark) {
    table,
    th,
    td {
      border: 1px solid #f2f2f2;
    }
  }
}


----- .\llama-3.2-webgpu\src\components\Chat.jsx -----

import { marked } from "marked";
import DOMPurify from "dompurify";

import BotIcon from "./icons/BotIcon";
import UserIcon from "./icons/UserIcon";

import "./Chat.css";
import { useEffect } from "react";

function render(text) {
  return DOMPurify.sanitize(marked.parse(text));
}

export default function Chat({ messages }) {
  const empty = messages.length === 0;

  useEffect(() => {
    window.MathJax.typeset();
  }, [messages]);

  return (
    <div
      className={`flex-1 p-6 max-w-[960px] w-full ${empty ? "flex flex-col items-center justify-end" : "space-y-4"}`}
    >
      {empty ? (
        <div className="text-xl">Ready!</div>
      ) : (
        messages.map((msg, i) => (
          <div key={`message-${i}`} className="flex items-start space-x-4">
            {msg.role === "assistant" ? (
              <>
                <BotIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-gray-200 dark:bg-gray-700 rounded-lg p-4">
                  <p className="min-h-6 text-gray-800 dark:text-gray-200 overflow-wrap-anywhere">
                    {msg.content.length > 0 ? (
                      <span
                        className="markdown"
                        dangerouslySetInnerHTML={{
                          __html: render(msg.content),
                        }}
                      />
                    ) : (
                      <span className="h-6 flex items-center gap-1">
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-200"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-400"></span>
                      </span>
                    )}
                  </p>
                </div>
              </>
            ) : (
              <>
                <UserIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-blue-500 text-white rounded-lg p-4">
                  <p className="min-h-6 overflow-wrap-anywhere">
                    {msg.content}
                  </p>
                </div>
              </>
            )}
          </div>
        ))
      )}
    </div>
  );
}


----- .\llama-3.2-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\llama-3.2-webgpu\src\components\icons\ArrowRightIcon.jsx -----

export default function ArrowRightIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M5 12h14" />
      <path d="m12 5 7 7-7 7" />
    </svg>
  );
}


----- .\llama-3.2-webgpu\src\components\icons\BotIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M12 8V4H8" />
      <rect width="16" height="12" x="4" y="8" rx="2" />
      <path d="M2 14h2" />
      <path d="M20 14h2" />
      <path d="M15 13v2" />
      <path d="M9 13v2" />
    </svg>
  );
}


----- .\llama-3.2-webgpu\src\components\icons\StopIcon.jsx -----

export default function StopIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
      <path
        fill="currentColor"
        d="M9 9.563C9 9.252 9.252 9 9.563 9h4.874c.311 0 .563.252.563.563v4.874c0 .311-.252.563-.563.563H9.564A.562.562 0 0 1 9 14.437V9.564Z"
      />
    </svg>
  );
}


----- .\llama-3.2-webgpu\src\components\icons\UserIcon.jsx -----

export default function UserIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2" />
      <circle cx="12" cy="7" r="4" />
    </svg>
  );
}


----- .\moonshine-web\src\App.jsx -----

import { useRef, useEffect, useState } from "react";
import { Canvas } from "@react-three/fiber";
import { motion } from "motion/react";

import BloomScene from "./components/BloomScene";
import AnimatedMesh from "./components/AnimatedMesh";

import { SAMPLE_RATE } from "./constants";
import { formatDate } from "./utils";

function App() {
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [messages, setMessages] = useState([]);
  const [frequency, setFrequency] = useState(0);
  const worker = useRef(null);

  useEffect(() => {
    // Initialize worker on mount
    worker.current ??= new Worker(new URL("./worker.js", import.meta.url), {
      type: "module",
    });

    // NOTE: Certain browsers handle error messages differently, so to ensure
    // compatibility, we need to handle errors in both `message` and `error` events.
    const onMessage = ({ data }) => {
      if (data.error) {
        return onError(data.error);
      }
      if (data.type === "status") {
        setStatus(data.message);
        setMessages((prev) => [...prev, data]);
      } else {
        setMessages((prev) => [...prev, data]);
      }
    };
    const onError = (error) => setError(error.message);

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessage);
    worker.current.addEventListener("error", onError);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessage);
      worker.current.removeEventListener("error", onError);
    };
  }, []);

  useEffect(() => {
    // https://react.dev/learn/synchronizing-with-effects#fetching-data
    let ignore = false; // Flag to track if the effect is active
    const audioStream = navigator.mediaDevices.getUserMedia({
      audio: {
        channelCount: 1,
        echoCancellation: true,
        autoGainControl: true,
        noiseSuppression: true,
        sampleRate: SAMPLE_RATE,
      },
    });

    let worklet;
    let audioContext;
    let source;
    audioStream
      .then(async (stream) => {
        if (ignore) return; // Exit if the effect has been cleaned up

        audioContext = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: SAMPLE_RATE,
          latencyHint: "interactive",
        });

        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 32;

        // NOTE: In Firefox, the following line may throw an error:
        // "AudioContext.createMediaStreamSource: Connecting AudioNodes from AudioContexts with different sample-rate is currently not supported."
        // See the following bug reports for more information:
        //  - https://bugzilla.mozilla.org/show_bug.cgi?id=1674892
        //  - https://bugzilla.mozilla.org/show_bug.cgi?id=1674892
        source = audioContext.createMediaStreamSource(stream);
        source.connect(analyser);

        const dataArray = new Uint8Array(analyser.frequencyBinCount);

        const getAverageFrequency = () => {
          analyser.getByteFrequencyData(dataArray);
          return (
            dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length
          );
        };

        const updateFrequency = () => {
          const frequency = getAverageFrequency();
          setFrequency(frequency);
          requestAnimationFrame(updateFrequency);
        };
        updateFrequency();

        await audioContext.audioWorklet.addModule(
          new URL("./processor.js", import.meta.url),
        );

        worklet = new AudioWorkletNode(audioContext, "vad-processor", {
          numberOfInputs: 1,
          numberOfOutputs: 0,
          channelCount: 1,
          channelCountMode: "explicit",
          channelInterpretation: "discrete",
        });

        source.connect(worklet);

        worklet.port.onmessage = (event) => {
          const { buffer } = event.data;

          // Dispatch buffer for voice activity detection
          worker.current?.postMessage({ buffer });
        };
      })
      .catch((err) => {
        setError(err.message);
        console.error(err);
      });

    return () => {
      ignore = true; // Mark the effect as cleaned up
      audioStream.then((stream) =>
        stream.getTracks().forEach((track) => track.stop()),
      );
      source?.disconnect();
      worklet?.disconnect();
      audioContext?.close();
    };
  }, []);

  const downloadTranscript = () => {
    const content = messages
      .filter((output) => output.type === "output")
      .map(
        (output) =>
          `${formatDate(output.start)} - ${formatDate(output.end)} | ${output.message}`,
      )
      .join("\n");

    const blob = new Blob([content], { type: "text/plain" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    a.download = "transcript.txt";
    a.click();
    URL.revokeObjectURL(url);
  };

  return (
    <div className="flex flex-col items-center justify-center w-screen supports-[height:100cqh]:h-[100cqh] supports-[height:100svh]:h-[100svh] bg-gray-900">
      <motion.div
        initial={{ opacity: 1, display: "flex" }}
        animate={{ opacity: 0, display: "none" }}
        transition={{ delay: 1.5, duration: 2 }}
        className="p-2 fixed inset-0 flex flex-col items-center justify-center bg-black/90 backdrop-blur-md z-20 text-center w-full h-full"
      >
        <h1 className="text-6xl sm:text-7xl lg:text-8xl text-white font-bold">
          Moonshine Web
        </h1>
        <h2 className="text-2xl text-white">
          Real-time in-browser speech recognition, powered by Transformers.js
        </h2>
      </motion.div>
      {error ? (
        <div className="text-center p-2">
          <div className="text-white text-4xl md:text-5xl mb-1 font-semibold">
            An error occurred
          </div>
          <div className="text-red-300 text-xl">{error}</div>
        </div>
      ) : (
        <>
          <div className="bottom-0 absolute text-center w-full z-10 text-white overflow-hidden pb-8">
            {messages.map(({ type, message, duration }, index) => (
              <motion.div
                key={index}
                initial={{ opacity: 0, y: 25 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ duration: 0.2 }}
                className={`mb-1 ${type === "output" ? "text-5xl" : "text-2xl text-green-300 font-light"}`}
              >
                <motion.div
                  initial={{ opacity: 1 }}
                  animate={
                    duration === "until_next" && index === messages.length - 1
                      ? {}
                      : {
                          opacity: 0,
                          display: "none",
                        }
                  }
                  transition={{
                    delay:
                      duration === "until_next" ? 0 : 1 + message.length / 20,
                    duration: 1,
                  }}
                >
                  {message}
                </motion.div>
              </motion.div>
            ))}
          </div>
          <Canvas camera={{ position: [0, 0, 8] }}>
            <ambientLight intensity={0.5} />
            <BloomScene frequency={frequency} />
            <AnimatedMesh
              ready={status !== null}
              active={status === "recording_start"}
              frequency={frequency}
            />
          </Canvas>
          <div className="absolute bottom-6 right-6 flex flex-col space-y-2 z-10">
            <button
              onClick={() => downloadTranscript()}
              className="w-10 h-10 bg-white rounded-full shadow-md flex items-center justify-center hover:bg-gray-100"
              title="Download Transcript"
            >
              <svg
                className="w-7 h-7 cursor-pointer text-gray-800"
                aria-hidden="true"
                xmlns="http://www.w3.org/2000/svg"
                fill="currentColor"
                viewBox="0 0 24 24"
              >
                <path
                  fillRule="evenodd"
                  d="M13 11.15V4a1 1 0 1 0-2 0v7.15L8.78 8.374a1 1 0 1 0-1.56 1.25l4 5a1 1 0 0 0 1.56 0l4-5a1 1 0 1 0-1.56-1.25L13 11.15Z"
                  clipRule="evenodd"
                />
                <path
                  fillRule="evenodd"
                  d="M9.657 15.874 7.358 13H5a2 2 0 0 0-2 2v4a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2v-4a2 2 0 0 0-2-2h-2.358l-2.3 2.874a3 3 0 0 1-4.685 0ZM17 16a1 1 0 1 0 0 2h.01a1 1 0 1 0 0-2H17Z"
                  clipRule="evenodd"
                />
              </svg>
            </button>

            <button
              onClick={() =>
                window.open(
                  "https://github.com/huggingface/transformers.js-examples/tree/main/moonshine-web",
                  "_blank",
                )
              }
              className="w-10 h-10 cursor-pointer bg-white rounded-full shadow-md flex items-center justify-center hover:bg-gray-100"
              title="Source Code"
            >
              <svg
                className="w-7 h-7 text-gray-800"
                aria-hidden="true"
                xmlns="http://www.w3.org/2000/svg"
                fill="currentColor"
                viewBox="0 0 24 24"
              >
                <path
                  fillRule="evenodd"
                  d="M12.006 2a9.847 9.847 0 0 0-6.484 2.44 10.32 10.32 0 0 0-3.393 6.17 10.48 10.48 0 0 0 1.317 6.955 10.045 10.045 0 0 0 5.4 4.418c.504.095.683-.223.683-.494 0-.245-.01-1.052-.014-1.908-2.78.62-3.366-1.21-3.366-1.21a2.711 2.711 0 0 0-1.11-1.5c-.907-.637.07-.621.07-.621.317.044.62.163.885.346.266.183.487.426.647.71.135.253.318.476.538.655a2.079 2.079 0 0 0 2.37.196c.045-.52.27-1.006.635-1.37-2.219-.259-4.554-1.138-4.554-5.07a4.022 4.022 0 0 1 1.031-2.75 3.77 3.77 0 0 1 .096-2.713s.839-.275 2.749 1.05a9.26 9.26 0 0 1 5.004 0c1.906-1.325 2.74-1.05 2.74-1.05.37.858.406 1.828.101 2.713a4.017 4.017 0 0 1 1.029 2.75c0 3.939-2.339 4.805-4.564 5.058a2.471 2.471 0 0 1 .679 1.897c0 1.372-.012 2.477-.012 2.814 0 .272.18.592.687.492a10.05 10.05 0 0 0 5.388-4.421 10.473 10.473 0 0 0 1.313-6.948 10.32 10.32 0 0 0-3.39-6.165A9.847 9.847 0 0 0 12.007 2Z"
                  clipRule="evenodd"
                />
              </svg>
            </button>
          </div>
        </>
      )}
    </div>
  );
}

export default App;


----- .\moonshine-web\src\constants.js -----

/**
 * Sample rate of the audio.
 * Coindicentally, this is the same for both models (Moonshine and Silero VAD)
 */
export const SAMPLE_RATE = 16000;
export const SAMPLE_RATE_MS = SAMPLE_RATE / 1000;

/**
 * Probabilities ABOVE this value are considered as SPEECH
 */
export const SPEECH_THRESHOLD = 0.3;

/**
 * If current state is SPEECH, and the probability of the next state
 * is below this value, it is considered as NON-SPEECH.
 */
export const EXIT_THRESHOLD = 0.1;

/**
 * After each speech chunk, wait for at least this amount of silence
 * before considering the next chunk as a new speech chunk
 */
export const MIN_SILENCE_DURATION_MS = 400;
export const MIN_SILENCE_DURATION_SAMPLES =
  MIN_SILENCE_DURATION_MS * SAMPLE_RATE_MS;

/**
 * Pad the speech chunk with this amount each side
 */
export const SPEECH_PAD_MS = 80;
export const SPEECH_PAD_SAMPLES = SPEECH_PAD_MS * SAMPLE_RATE_MS;

/**
 * Final speech chunks below this duration are discarded
 */
export const MIN_SPEECH_DURATION_SAMPLES = 250 * SAMPLE_RATE_MS; // 250 ms

/**
 * Maximum duration of audio that can be handled by Moonshine
 */
export const MAX_BUFFER_DURATION = 30;

/**
 * Size of the incoming buffers
 */
export const NEW_BUFFER_SIZE = 512;

/**
 * The number of previous buffers to keep, to ensure the audio is padded correctly
 */
export const MAX_NUM_PREV_BUFFERS = Math.ceil(
  SPEECH_PAD_SAMPLES / NEW_BUFFER_SIZE,
);


----- .\moonshine-web\src\index.css -----

@import url("https://fonts.googleapis.com/css2?family=Poppins:wght@100;200;300;400;500;600;700;800;900&display=swap");
@import "tailwindcss";

* {
  font-family: "Poppins", sans-serif;
}

html {
  overflow: hidden;
}


----- .\moonshine-web\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import "./index.css";
import App from "./App.jsx";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\moonshine-web\src\processor.js -----

const MIN_CHUNK_SIZE = 512;
let globalPointer = 0;
let globalBuffer = new Float32Array(MIN_CHUNK_SIZE);

class VADProcessor extends AudioWorkletProcessor {
  process(inputs, outputs, parameters) {
    const buffer = inputs[0][0];
    if (!buffer) return; // buffer is null when the stream ends

    if (buffer.length > MIN_CHUNK_SIZE) {
      // If the buffer is larger than the minimum chunk size, send the entire buffer
      this.port.postMessage({ buffer });
    } else {
      const remaining = MIN_CHUNK_SIZE - globalPointer;
      if (buffer.length >= remaining) {
        // If the buffer is larger than (or equal to) the remaining space in the global buffer, copy the remaining space
        globalBuffer.set(buffer.subarray(0, remaining), globalPointer);

        // Send the global buffer
        this.port.postMessage({ buffer: globalBuffer });

        // Reset the global buffer and set the remaining buffer
        globalBuffer.fill(0);
        globalBuffer.set(buffer.subarray(remaining), 0);
        globalPointer = buffer.length - remaining;
      } else {
        // If the buffer is smaller than the remaining space in the global buffer, copy the buffer to the global buffer
        globalBuffer.set(buffer, globalPointer);
        globalPointer += buffer.length;
      }
    }

    return true; // Keep the processor alive
  }
}

registerProcessor("vad-processor", VADProcessor);


----- .\moonshine-web\src\utils.js -----

export function formatDate(timestamp) {
  return new Date(timestamp).toLocaleString("zh", {
    hour12: false,
    year: "numeric",
    month: "numeric",
    day: "numeric",
    hour: "numeric",
    minute: "numeric",
    second: "numeric",
    fractionalSecondDigits: 3,
  });
}

export async function supportsWebGPU() {
  try {
    if (!navigator.gpu) return false;
    await navigator.gpu.requestAdapter();
    return true;
  } catch (e) {
    return false;
  }
}


----- .\moonshine-web\src\worker.js -----

import { AutoModel, Tensor, pipeline } from "@huggingface/transformers";
import {
  MAX_BUFFER_DURATION,
  SAMPLE_RATE,
  SPEECH_THRESHOLD,
  EXIT_THRESHOLD,
  SPEECH_PAD_SAMPLES,
  MAX_NUM_PREV_BUFFERS,
  MIN_SILENCE_DURATION_SAMPLES,
  MIN_SPEECH_DURATION_SAMPLES,
} from "./constants";
import { supportsWebGPU } from "./utils";

const device = (await supportsWebGPU()) ? "webgpu" : "wasm";
self.postMessage({ type: "info", message: `Using device: "${device}"` });
self.postMessage({
  type: "info",
  message: "Loading models...",
  duration: "until_next",
});

// Load models
const silero_vad = await AutoModel.from_pretrained(
  "onnx-community/silero-vad",
  {
    config: { model_type: "custom" },
    dtype: "fp32", // Full-precision
  },
).catch((error) => {
  self.postMessage({ error });
  throw error;
});

const DEVICE_DTYPE_CONFIGS = {
  webgpu: {
    encoder_model: "fp32",
    decoder_model_merged: "q4",
  },
  wasm: {
    encoder_model: "fp32",
    decoder_model_merged: "q8",
  },
};
const transcriber = await pipeline(
  "automatic-speech-recognition",
  "onnx-community/moonshine-base-ONNX", // or "onnx-community/whisper-tiny.en",
  {
    device,
    dtype: DEVICE_DTYPE_CONFIGS[device],
  },
).catch((error) => {
  self.postMessage({ error });
  throw error;
});

await transcriber(new Float32Array(SAMPLE_RATE)); // Compile shaders
self.postMessage({ type: "status", status: "ready", message: "Ready!" });

// Transformers.js currently doesn't support simultaneous inference,
// so we need to chain the inference promises.
let inferenceChain = Promise.resolve();

// Global audio buffer to store incoming audio
const BUFFER = new Float32Array(MAX_BUFFER_DURATION * SAMPLE_RATE);
let bufferPointer = 0;

// Initial state for VAD
const sr = new Tensor("int64", [SAMPLE_RATE], []);
let state = new Tensor("float32", new Float32Array(2 * 1 * 128), [2, 1, 128]);

// Whether we are in the process of adding audio to the buffer
let isRecording = false;

/**
 * Perform Voice Activity Detection (VAD)
 * @param {Float32Array} buffer The new audio buffer
 * @returns {Promise<boolean>} `true` if the buffer is speech, `false` otherwise.
 */
async function vad(buffer) {
  const input = new Tensor("float32", buffer, [1, buffer.length]);

  const { stateN, output } = await (inferenceChain = inferenceChain.then((_) =>
    silero_vad({ input, sr, state }),
  ));
  state = stateN; // Update state

  const isSpeech = output.data[0];

  // Use heuristics to determine if the buffer is speech or not
  return (
    // Case 1: We are above the threshold (definitely speech)
    isSpeech > SPEECH_THRESHOLD ||
    // Case 2: We are in the process of recording, and the probability is above the negative (exit) threshold
    (isRecording && isSpeech >= EXIT_THRESHOLD)
  );
}

/**
 * Transcribe the audio buffer
 * @param {Float32Array} buffer The audio buffer
 * @param {Object} data Additional data
 */
const transcribe = async (buffer, data) => {
  const { text } = await (inferenceChain = inferenceChain.then((_) =>
    transcriber(buffer),
  ));
  self.postMessage({ type: "output", buffer, message: text, ...data });
};

// Track the number of samples after the last speech chunk
let postSpeechSamples = 0;
const reset = (offset = 0) => {
  self.postMessage({
    type: "status",
    status: "recording_end",
    message: "Transcribing...",
    duration: "until_next",
  });
  BUFFER.fill(0, offset);
  bufferPointer = offset;
  isRecording = false;
  postSpeechSamples = 0;
};

const dispatchForTranscriptionAndResetAudioBuffer = (overflow) => {
  // Get start and end time of the speech segment, minus the padding
  const now = Date.now();
  const end =
    now - ((postSpeechSamples + SPEECH_PAD_SAMPLES) / SAMPLE_RATE) * 1000;
  const start = end - (bufferPointer / SAMPLE_RATE) * 1000;
  const duration = end - start;
  const overflowLength = overflow?.length ?? 0;

  // Send the audio buffer to the worker
  const buffer = BUFFER.slice(0, bufferPointer + SPEECH_PAD_SAMPLES);

  const prevLength = prevBuffers.reduce((acc, b) => acc + b.length, 0);
  const paddedBuffer = new Float32Array(prevLength + buffer.length);
  let offset = 0;
  for (const prev of prevBuffers) {
    paddedBuffer.set(prev, offset);
    offset += prev.length;
  }
  paddedBuffer.set(buffer, offset);
  transcribe(paddedBuffer, { start, end, duration });

  // Set overflow (if present) and reset the rest of the audio buffer
  if (overflow) {
    BUFFER.set(overflow, 0);
  }
  reset(overflowLength);
};

let prevBuffers = [];
self.onmessage = async (event) => {
  const { buffer } = event.data;

  const wasRecording = isRecording; // Save current state
  const isSpeech = await vad(buffer);

  if (!wasRecording && !isSpeech) {
    // We are not recording, and the buffer is not speech,
    // so we will probably discard the buffer. So, we insert
    // into a FIFO queue with maximum size of PREV_BUFFER_SIZE
    if (prevBuffers.length >= MAX_NUM_PREV_BUFFERS) {
      // If the queue is full, we discard the oldest buffer
      prevBuffers.shift();
    }
    prevBuffers.push(buffer);
    return;
  }

  const remaining = BUFFER.length - bufferPointer;
  if (buffer.length >= remaining) {
    // The buffer is larger than (or equal to) the remaining space in the global buffer,
    // so we perform transcription and copy the overflow to the global buffer
    BUFFER.set(buffer.subarray(0, remaining), bufferPointer);
    bufferPointer += remaining;

    // Dispatch the audio buffer
    const overflow = buffer.subarray(remaining);
    dispatchForTranscriptionAndResetAudioBuffer(overflow);
    return;
  } else {
    // The buffer is smaller than the remaining space in the global buffer,
    // so we copy it to the global buffer
    BUFFER.set(buffer, bufferPointer);
    bufferPointer += buffer.length;
  }

  if (isSpeech) {
    if (!isRecording) {
      // Indicate start of recording
      self.postMessage({
        type: "status",
        status: "recording_start",
        message: "Listening...",
        duration: "until_next",
      });
    }
    // Start or continue recording
    isRecording = true;
    postSpeechSamples = 0; // Reset the post-speech samples
    return;
  }

  postSpeechSamples += buffer.length;

  // At this point we're confident that we were recording (wasRecording === true), but the latest buffer is not speech.
  // So, we check whether we have reached the end of the current audio chunk.
  if (postSpeechSamples < MIN_SILENCE_DURATION_SAMPLES) {
    // There was a short pause, but not long enough to consider the end of a speech chunk
    // (e.g., the speaker took a breath), so we continue recording
    return;
  }

  if (bufferPointer < MIN_SPEECH_DURATION_SAMPLES) {
    // The entire buffer (including the new chunk) is smaller than the minimum
    // duration of a speech chunk, so we can safely discard the buffer.
    reset();
    return;
  }

  dispatchForTranscriptionAndResetAudioBuffer();
};


----- .\moonshine-web\src\components\AnimatedMesh.jsx -----

import { useRef, useMemo } from "react";
import { useFrame } from "@react-three/fiber";
import { IcosahedronGeometry, ShaderMaterial, Clock, Mesh } from "three";

const MIN_WAVE_SIZE = 10;
const AUDIO_SCALE = 0.5;
const MAX_WAVE_SIZE = 60;

const clock = new Clock();

function AnimatedMesh({ ready, active, frequency }) {
  const colors = {
    red: ready ? (active ? 1 : 0) : 0.1,
    green: ready ? (active ? 0 : 1) : 0.1,
    blue: ready ? (active ? 1 : 0) : 0.1,
  };
  const meshRef = useRef();
  const geometry = useMemo(() => new IcosahedronGeometry(3, 20), []);
  const material = useMemo(() => {
    return new ShaderMaterial({
      uniforms: {
        u_time: { value: 0.0 },
        u_frequency: { value: 0.0 },
        u_red: { value: 0.0 },
        u_green: { value: 0.0 },
        u_blue: { value: 0.0 },
      },
      vertexShader: document.getElementById("vertexshader").textContent,
      fragmentShader: document.getElementById("fragmentshader").textContent,
      wireframe: true,
    });
  }, []);

  const uniforms = material.uniforms;

  useFrame(() => {
    const time = clock.getElapsedTime();

    uniforms.u_time.value = time;
    uniforms.u_frequency.value = Math.min(
      MIN_WAVE_SIZE + AUDIO_SCALE * frequency,
      MAX_WAVE_SIZE,
    );
    uniforms.u_red.value = colors.red;
    uniforms.u_green.value = colors.green;
    uniforms.u_blue.value = colors.blue;
  });

  return <primitive object={new Mesh(geometry, material)} ref={meshRef} />;
}

export default AnimatedMesh;


----- .\moonshine-web\src\components\BloomScene.jsx -----

import { useRef, useEffect } from "react";
import { useFrame, extend, useThree } from "@react-three/fiber";
import { Vector2 } from "three";
import { EffectComposer } from "three/examples/jsm/postprocessing/EffectComposer";
import { OutputPass } from "three/examples/jsm/postprocessing/OutputPass";
import { RenderPass } from "three/examples/jsm/postprocessing/RenderPass";
import { UnrealBloomPass } from "three/examples/jsm/postprocessing/UnrealBloomPass";

extend({ EffectComposer, RenderPass, UnrealBloomPass, OutputPass });

function BloomScene({ frequency }) {
  const { gl, scene, camera, size } = useThree();

  const renderPass = useRef();
  const outputPass = useRef();
  const composer = useRef();
  const bloomPass = useRef();

  useEffect(() => {
    // Runs on resize, etc.
    renderPass.current = new RenderPass(scene, camera);
    outputPass.current = new OutputPass();
    composer.current = new EffectComposer(gl);
    bloomPass.current = new UnrealBloomPass(
      new Vector2(size.width, size.height),
      0.2,
      1,
      0,
    );

    composer.current.addPass(renderPass.current);
    composer.current.addPass(bloomPass.current);
    composer.current.addPass(outputPass.current);

    return () => {
      composer.current.removePass(renderPass.current);
      composer.current.removePass(bloomPass.current);
      composer.current.removePass(outputPass.current);
      renderPass.current.dispose();
      outputPass.current.dispose();
      bloomPass.current.dispose();
    };
  }, [gl, scene, camera, size]);

  useEffect(() => {
    composer.current.setSize(size.width, size.height);
  }, [size]);

  useEffect(() => {
    bloomPass.current.strength = 0.2 + frequency / 1000;
  }, [frequency]);

  useFrame(() => {
    composer.current.render();
  }, 1);

  return null;
}

export default BloomScene;


----- .\musicgen-web\src\App.css -----



----- .\musicgen-web\src\App.jsx -----

import { useEffect, useState, useRef } from "react";
import {
  env,
  AutoTokenizer,
  MusicgenForConditionalGeneration,
  BaseStreamer,
} from "@huggingface/transformers";
import { encodeWAV, share } from "./utils.js";

import "./App.css";

env.backends.onnx.wasm.proxy = true;

const MODEL_ID = "Xenova/musicgen-small";

// Adapted from https://huggingface.co/spaces/facebook/MusicGen
const EXAMPLES = [
  "80s pop track with bassy drums and synth",
  "90s rock song with loud guitars and heavy drums",
  "a light and cheerly EDM track, with syncopated drums, aery pads, and strong emotions bpm: 130",
  "A cheerful country song with acoustic guitars",
  "lofi slow bpm electro chill with organic samples",
];

// Enable sharing if running on Hugging Face Spaces
const SHARING_ENABLED = window.location.host.endsWith(".hf.space");

// Streamer to update progress
class CallbackStreamer extends BaseStreamer {
  constructor(callback_fn) {
    super();
    this.callback_fn = callback_fn;
  }

  put(value) {
    return this.callback_fn(value);
  }

  end() {
    return this.callback_fn();
  }
}

// Main App component
const App = () => {
  // Input/output state
  const [textInput, setTextInput] = useState(EXAMPLES[0]);
  const [progress, setProgress] = useState(0);
  const [loadProgress, setLoadProgress] = useState({});
  const [statusText, setStatusText] = useState("Loading model (656MB)...");
  const [result, setResult] = useState(null);
  const audioRef = useRef(null);

  // Model and tokenizer references
  const modelPromise = useRef(null);
  const tokenizerPromise = useRef(null);

  // Generation parameters
  const [guidance_scale, setGuidanceScale] = useState(3);
  const [temperature, setTemperature] = useState(1);
  const [duration, setDuration] = useState(10);

  // Load model and tokenizer on first render
  useEffect(() => {
    modelPromise.current ??= MusicgenForConditionalGeneration.from_pretrained(
      MODEL_ID,
      {
        progress_callback: (data) => {
          if (data.status !== "progress") return;
          setLoadProgress((prev) => ({ ...prev, [data.file]: data }));
        },
        dtype: {
          text_encoder: "q8",
          decoder_model_merged: "q8",
          encodec_decode: "fp32",
        },
        device: "wasm",
      },
    );

    tokenizerPromise.current ??= AutoTokenizer.from_pretrained(MODEL_ID);
  }, []);

  // Update progress bar based on load progress
  useEffect(() => {
    const items = Object.values(loadProgress);
    if (items.length !== 5) return; // 5 files to load
    let loaded = 0;
    let total = 0;
    for (const data of Object.values(loadProgress)) {
      loaded += data.loaded;
      total += data.total;
    }
    const progress = loaded / total;
    setProgress(progress);
    setStatusText(
      progress === 1
        ? "Ready!"
        : `Loading model (${(progress * 100).toFixed()}% of 656MB)...`,
    );
  }, [loadProgress]);

  // Function to handle generating music
  const generateMusic = async () => {
    // Reset audio player and result
    audioRef.current.src = "";
    setResult(null);

    // Get model and tokenizer
    const tokenizer = await tokenizerPromise.current;
    const model = await modelPromise.current;

    // Get number of tokens to match user-specified duration (more intuitive for user)
    // 503 tokens -> 10 seconds generated => ~50 tokens per second
    // https://huggingface.co/docs/transformers/model_doc/musicgen#generation
    const max_length = Math.min(
      Math.max(Math.floor(duration * 50), 1) + 4,
      model.generation_config.max_length ?? 1500,
    );

    // Create a streamer to update progress
    let num_tokens = 0;
    const streamer = new CallbackStreamer((value) => {
      const percent = value === undefined ? 1 : ++num_tokens / max_length;
      setStatusText(`Generating (${(percent * 100).toFixed()}%)...`);
      setProgress(percent);
    });

    // Tokenize input text
    const inputs = tokenizer(textInput);

    // Generate music
    const audio_values = await model.generate({
      // Inputs
      ...inputs,

      // Generation parameters
      max_length,
      guidance_scale,
      temperature,

      // Outputs
      streamer,
    });

    setStatusText("Encoding audio...");

    // Encode audio values to WAV
    const sampling_rate = model.config.audio_encoder.sampling_rate;
    const wav = encodeWAV(audio_values.data, sampling_rate);
    const blob = new Blob([wav], { type: "audio/wav" });
    setResult(blob);

    audioRef.current.src = URL.createObjectURL(blob);
    setStatusText("Done!");
  };

  return (
    <div className="max-w-4xl h-screen mx-auto text-center flex justify-center items-center">
      <div className="container mx-auto p-8">
        <h1 className="text-5xl font-bold mb-2">MusicGen Web</h1>
        <h2 className="text-2xl font-semibold mb-4">
          In-browser text-to-music w/{" "}
          <a
            className="underline"
            href="https://github.com/huggingface/transformers.js"
          >
            ðŸ¤— Transformers.js!
          </a>
        </h2>

        {/* Text input for user */}
        <input
          type="text"
          placeholder="Describe the music to generate..."
          value={textInput}
          onChange={(e) => setTextInput(e.target.value)}
          className="border border-gray-300 p-2 mb-4 w-full rounded"
        />

        {/* Example buttons */}
        <div className="mb-4 flex gap-2 justify-center text-sm">
          {EXAMPLES.map((example, i) => (
            <button
              key={i}
              className="bg-blue-500 hover:bg-blue-400 transition-colors duration-100 text-white px-2 py-2 rounded cursor-pointer"
              onClick={(e) => setTextInput(e.target.innerText)}
            >
              {example}
            </button>
          ))}
        </div>

        {/* Generation parameters */}
        <div className="flex mb-4 justify-center gap-2">
          {/* Duration */}
          <div>
            <label className="block text-sm font-semibold mb-1">Duration</label>
            <input
              type="range"
              min={1}
              max={30}
              value={duration}
              onChange={(e) => setDuration(e.target.value)}
            />
            <p className="text-sm text-center">{`${duration} second${duration > 1 ? "s" : ""}`}</p>
          </div>

          {/* Guidance Scale */}
          <div className="mr-4">
            <label className="block text-sm font-semibold mb-1">
              Guidance Scale
            </label>
            <input
              type="range"
              min={1}
              max={10}
              value={guidance_scale}
              onChange={(e) => setGuidanceScale(e.target.value)}
            />
            <p className="text-sm text-center">{guidance_scale}</p>
          </div>

          {/* Temperature */}
          <div>
            <label className="block text-sm font-semibold mb-1">
              Temperature
            </label>
            <input
              type="range"
              min={0.1}
              max={2}
              step={0.1}
              value={temperature}
              onChange={(e) => setTemperature(e.target.value)}
            />
            <p className="text-sm text-center">{temperature}</p>
          </div>
        </div>

        {/* Button to generate music */}
        <button
          className="mb-4 bg-green-500 hover:bg-green-400 transition-colors duration-100 text-white px-4 py-3 rounded-lg font-semibold cursor-pointer"
          onClick={generateMusic}
        >
          Generate Music
        </button>

        {/* Progress bar */}
        <div className="mb-4">
          <div className="bg-gray-200 h-4 w-full rounded-full">
            <div
              className="bg-blue-500 h-4 rounded-full"
              style={{ width: `${100 * progress}%` }}
            ></div>
          </div>
          <p className="text-sm text-center mt-1">{statusText}</p>
        </div>

        {/* Audio player */}
        {
          <div className="flex justify-center flex-col items-center">
            <audio ref={audioRef} controls type="audio/wav" />
            {SHARING_ENABLED && result && (
              <button
                className="bg-red-500 hover:bg-red-400 transition-colors duration-100 text-white px-2 py-1 my-2 rounded-lg text-sm"
                onClick={async (e) => {
                  e.target.disabled = true;
                  e.target.innerText = "Uploading...";
                  await share(result, {
                    prompt: textInput,
                    duration,
                    guidance_scale,
                    temperature,
                  });
                  e.target.disabled = false;
                  e.target.innerText = "Share";
                }}
              >
                Share
              </button>
            )}
          </div>
        }
      </div>
    </div>
  );
};

export default App;


----- .\musicgen-web\src\index.css -----

@import "tailwindcss";


----- .\musicgen-web\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\musicgen-web\src\utils.js -----

// Adapted from https://www.npmjs.com/package/audiobuffer-to-wav
export function encodeWAV(samples, sampleRate = 16000) {
  let offset = 44;
  const buffer = new ArrayBuffer(offset + samples.length * 4);
  const view = new DataView(buffer);

  /* RIFF identifier */
  writeString(view, 0, "RIFF");
  /* RIFF chunk length */
  view.setUint32(4, 36 + samples.length * 4, true);
  /* RIFF type */
  writeString(view, 8, "WAVE");
  /* format chunk identifier */
  writeString(view, 12, "fmt ");
  /* format chunk length */
  view.setUint32(16, 16, true);
  /* sample format (raw) */
  view.setUint16(20, 3, true);
  /* channel count */
  view.setUint16(22, 1, true);
  /* sample rate */
  view.setUint32(24, sampleRate, true);
  /* byte rate (sample rate * block align) */
  view.setUint32(28, sampleRate * 4, true);
  /* block align (channel count * bytes per sample) */
  view.setUint16(32, 4, true);
  /* bits per sample */
  view.setUint16(34, 32, true);
  /* data chunk identifier */
  writeString(view, 36, "data");
  /* data chunk length */
  view.setUint32(40, samples.length * 4, true);

  for (let i = 0; i < samples.length; ++i, offset += 4) {
    view.setFloat32(offset, samples[i], true);
  }

  return buffer;
}
function writeString(view, offset, string) {
  for (let i = 0; i < string.length; ++i) {
    view.setUint8(offset + i, string.charCodeAt(i));
  }
}

export async function share(body, settings) {
  const response = await fetch("https://huggingface.co/uploads", {
    method: "POST",
    body,
  });
  if (!response.ok)
    throw new Error(`Failed to upload audio: ${response.statusText}`);
  const url = await response.text();

  const params = new URLSearchParams({
    title: `ðŸŽµ ${settings.prompt}`,
    description: `<audio controls src="${url}"></audio>\n${JSON.stringify(settings, null, 2)}`,
  });

  const shareURL = `https://huggingface.co/spaces/Xenova/musicgen-web/discussions/new?${params.toString()}`;
  window.open(shareURL, "_blank");
}


----- .\pglite-semantic-search\src\App.jsx -----

import { getDB, initSchema, countRows, seedDb, search } from "./utils/db";
import { useState, useEffect, useRef, useCallback } from "react";

export default function App() {
  // Keep track of the classification result and the model loading status.
  const [input, setInput] = useState("");
  const [content, setContent] = useState([]);
  const [result, setResult] = useState(null);
  const [ready, setReady] = useState(null);
  const initailizing = useRef(false);

  // Create a reference to the worker object.
  const worker = useRef(null);

  // Set up DB
  const db = useRef(null);
  useEffect(() => {
    const setup = async () => {
      initailizing.current = true;
      db.current = await getDB();
      await initSchema(db.current);
      let count = await countRows(db.current, "embeddings");
      console.log(`Found ${count} rows`);
      if (count === 0) {
        await seedDb(db.current);
        count = await countRows(db.current, "embeddings");
        console.log(`Seeded ${count} rows`);
      }
      // Get Items
      const items = await db.current.query("SELECT content FROM embeddings");
      setContent(items.rows.map((x) => x.content));
    };
    if (!db.current && !initailizing.current) {
      setup();
    }
  }, []);

  // We use the `useEffect` hook to set up the worker as soon as the `App` component is mounted.
  useEffect(() => {
    if (!worker.current) {
      // Create the worker if it does not yet exist.
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = async (e) => {
      switch (e.data.status) {
        case "initiate":
          setReady(false);
          break;
        case "ready":
          setReady(true);
          break;
        case "complete":
          // Cosine similarity search in pgvector
          const searchResults = await search(db.current, e.data.embedding);
          console.log({ searchResults });
          setResult(searchResults.map((x) => x.content));
          break;
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () =>
      worker.current.removeEventListener("message", onMessageReceived);
  });

  const classify = useCallback((text) => {
    if (worker.current) {
      worker.current.postMessage({ text });
    }
  }, []);
  return (
    <main className="flex min-h-screen flex-col items-center justify-center p-12">
      <h1 className="text-5xl font-bold mb-2 text-center">Transformers.js</h1>
      <h2 className="text-2xl mb-4 text-center">
        100% in-browser Semantic Search with{" "}
        <a
          className="underline"
          href="https://huggingface.co/docs/transformers.js"
        >
          Transformers.js
        </a>
        {", "}
        <a className="underline" href="https://github.com/electric-sql/pglite">
          PGlite
        </a>{" "}
        {" + "}
        <a className="underline" href="https://github.com/pgvector/pgvector">
          pgvector!
        </a>
      </h2>
      <p className="text-center">Items in database:</p>
      <pre className="bg-gray-100 p-2 mb-4 rounded">
        {JSON.stringify(content)}
      </pre>
      <form
        onSubmit={(e) => {
          e.preventDefault();
          classify(input);
        }}
      >
        <input
          type="text"
          className="w-full max-w-xs p-2 border border-gray-300 rounded mb-4"
          placeholder="Enter text here"
          onInput={(e) => {
            setResult([]);
            setInput(e.target.value);
          }}
        />
        <button
          type="submit"
          className="bg-blue-500 text-white p-2 mb-4 rounded w-full max-w-xs"
        >
          Semantic Search
        </button>
      </form>

      {ready !== null && (
        <>
          <p className="text-center">Similarity Search results:</p>
          <pre className="bg-gray-100 p-2 rounded">
            {!ready || !result ? "Loading..." : JSON.stringify(result)}
          </pre>
        </>
      )}
    </main>
  );
}


----- .\pglite-semantic-search\src\globals.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --foreground-rgb: 0, 0, 0;
  --background-start-rgb: 214, 219, 220;
  --background-end-rgb: 255, 255, 255;
}

body {
  color: rgb(var(--foreground-rgb));
  background: linear-gradient(
      to bottom,
      transparent,
      rgb(var(--background-end-rgb))
    )
    rgb(var(--background-start-rgb));
}


----- .\pglite-semantic-search\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import App from "./App.jsx";
import "./globals.css";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\pglite-semantic-search\src\worker.js -----

import { pipeline } from "@huggingface/transformers";

// Use the Singleton pattern to enable lazy construction of the pipeline.
class PipelineSingleton {
  static task = "feature-extraction";
  static model = "Supabase/gte-small";
  static instance = null;

  static async getInstance(progress_callback = null) {
    if (this.instance === null) {
      this.instance = pipeline(this.task, this.model, {
        progress_callback,
        dtype: "fp32",
        device: !!navigator.gpu ? "webgpu" : "wasm",
      });
    }
    return this.instance;
  }
}

// Listen for messages from the main thread
self.addEventListener("message", async (event) => {
  // Retrieve the classification pipeline. When called for the first time,
  // this will load the pipeline and save it for future use.
  const classifier = await PipelineSingleton.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  // Actually perform the classification
  const output = await classifier(event.data.text, {
    pooling: "mean",
    normalize: true,
  });

  // Extract the embedding output
  const embedding = Array.from(output.data);

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    embedding,
  });
});


----- .\pglite-semantic-search\src\utils\db.js -----

import { PGlite } from "@electric-sql/pglite";
import { vector } from "@electric-sql/pglite/vector";

let dbInstance;

export async function getDB() {
  if (dbInstance) {
    return dbInstance;
  }
  const metaDb = new PGlite("idb://supa-semantic-search", {
    extensions: {
      vector,
    },
  });
  await metaDb.waitReady;
  dbInstance = metaDb;
  return metaDb;
}

export const initSchema = async (db) => {
  return await db.exec(`
    create extension if not exists vector;
    -- drop table if exists embeddings; -- Uncomment this line to reset the database
    create table if not exists embeddings (
      id bigint primary key generated always as identity,
      content text not null,
      embedding vector (384)
    );
    
    create index on embeddings using hnsw (embedding vector_ip_ops);
  `);
};

export const countRows = async (db, table) => {
  const res = await db.query(`SELECT COUNT(*) FROM ${table};`);
  return res.rows[0].count;
};

// Cosine similarity search in pgvector
export const search = async (
  db,
  embedding,
  match_threshold = 0.8,
  limit = 3,
) => {
  const res = await db.query(
    `
    select * from embeddings

    -- The inner product is negative, so we negate match_threshold
    where embeddings.embedding <#> $1 < $2

    -- Our embeddings are normalized to length 1, so cosine similarity
    -- and inner product will produce the same query results.
    -- Using inner product which can be computed faster.
    --
    -- For the different distance functions, see https://github.com/pgvector/pgvector
    order by embeddings.embedding <#> $1
    limit $3;
    `,
    [JSON.stringify(embedding), -Number(match_threshold), Number(limit)],
  );
  return res.rows;
};

export const seedDb = async (db) => {
  return await db.exec(`
    insert into embeddings (content, embedding) values
      ('Bed', '[-0.006822244,-0.0073390524,0.040399525,0.000736064,-0.009085903,0.03851906,0.040384453,0.046808533,0.042115353,0.0016632339,-0.0034933984,-0.08830318,0.010509381,0.058803126,-0.001959153,-0.015982354,0.0057063154,0.013070074,-0.047947004,0.049101133,0.04977712,-0.01529147,0.02679988,-0.016667083,0.02259381,-0.019300632,-0.022183467,-0.0524223,-0.05392063,-0.15442617,-0.016726814,-0.0046356786,-0.0033007693,-0.038389646,0.0034641414,-0.019414157,0.00530319,0.059465468,0.014079502,0.029850144,0.038306333,0.01192391,0.004093596,-0.08163046,-0.012357966,-0.006009865,-0.013621683,-0.02337859,0.08280816,0.010447533,0.03683234,-0.028687134,0.01815019,-0.008677378,0.042089492,0.05065854,0.07357729,0.011352438,0.034780625,0.09188656,0.017418442,0.064832725,-0.20041808,0.10360221,0.0671062,0.018502504,-0.09739072,-0.00019951521,0.022652537,0.06734662,-0.027538104,-0.016570838,0.015677454,0.053861156,0.048691154,-0.054969806,0.04112108,-0.04718161,0.025326118,-0.010980256,-0.007515454,-0.007403816,-0.042940833,-0.013452556,-0.028590562,-0.0272049,-0.018576223,-0.019168183,0.012477406,-0.010323304,-0.025437333,0.026618486,-0.04006101,0.052304097,-0.029081466,-0.020374913,-0.0058097104,0.0049184063,-0.025047028,0.24331976,-0.028509228,0.064583465,0.0582664,-0.008838904,0.029010272,-0.042640913,-0.023065887,0.0009429337,-0.03458494,0.031521663,-0.024115514,0.022468511,0.017324978,-0.021598602,0.057364237,-0.03622212,0.06522789,0.03643067,0.019802976,-0.03882662,-0.024954926,-0.00922944,0.026700335,0.0022999484,0.021236164,-0.1017225,0.034700826,0.10996454,0.015752304,-0.010611508,0.016026804,-0.008537629,-0.025187723,-0.011628215,-0.012928679,0.050661393,0.027768211,0.012645961,-0.014378526,-0.03122859,-0.054974407,-0.11988388,0.010854659,-0.113561414,-0.05754862,0.04890203,-0.074521095,0.042206623,-0.047778264,0.027690561,-0.02492779,0.04351375,-0.0042421916,-0.013825208,0.029148908,0.024689274,-0.0011065236,0.010749134,-0.02179892,0.030476885,0.005788956,-0.011307971,-0.029803434,0.047130115,-0.016205732,-0.07745564,-0.06069879,0.012738268,0.008780021,-0.04605315,0.05029989,0.051564094,-0.06556095,0.0097619705,0.07852359,0.03755774,6.385107e-05,-0.025080299,-0.00807334,0.002826086,0.029596226,-0.06858373,-0.059509,0.042350702,0.008429371,-0.056518193,-0.0038923442,0.0019655668,0.042615306,0.03156147,0.020491784,0.025043039,-0.014860392,-0.04517383,-0.05519179,-0.067039765,-0.01814679,-0.02211169,0.011379715,-0.038663853,0.074394055,-0.0033453933,-0.01922349,0.03422448,0.03382803,0.06018736,-0.040258747,-0.016005179,0.04973765,0.00047837297,-0.0032964202,-0.018018955,0.059821125,-0.02552337,-0.050962154,0.033526286,0.0015766671,0.0008125402,0.055109553,0.035740905,0.06790532,-0.10467817,-0.06608197,-0.22082354,0.04529764,0.04231412,-0.06357316,0.06083306,-0.03584259,-0.045060553,-0.034747567,-0.011515003,0.023540512,0.092857406,-0.015673747,-0.037912108,0.03791529,-0.027737431,0.075638145,0.064396635,0.00039346732,-0.054100823,-0.04585783,-0.013354809,-0.00029281477,-0.03531705,-0.013202513,0.025100702,-0.022423482,0.22803828,0.021944992,0.062074866,-0.033807088,0.063471064,0.058585603,-0.034941155,-0.153415,0.05968843,0.043823935,0.02648161,-0.035211023,-0.027928075,-0.08642302,-0.03428572,0.0782574,0.0048829336,-0.07674112,-0.014943502,-0.0026991852,-0.051111754,-0.02639702,-0.028578043,0.04409947,0.016471509,-0.0153481895,0.05080286,0.035896562,-0.026176613,-0.00410023,-0.054971956,0.0072062816,-0.010886078,0.03109176,0.0062575233,-0.059884343,0.03391246,-0.021350292,0.028362656,-0.03287903,-0.02909052,-0.054189354,0.040938597,-0.043358773,-0.0062902463,0.038029272,-0.017994935,-0.02783497,0.059119657,0.04149558,0.049971994,0.012019358,-0.034788415,-0.034991883,0.037606157,-0.07293067,0.00878965,0.020748338,0.006832928,0.014310711,0.0025819188,-0.026033923,0.063590325,-0.023910472,0.026275495,0.006421037,-0.025870698,-0.03395694,0.013277425,-0.017819595,-0.27904364,0.042438418,-0.040410627,0.060995445,-0.03024428,0.023125576,0.026820898,0.07209522,-0.045738053,0.021097308,0.018815981,0.048715215,-0.0140842525,0.01586625,-0.00600374,0.057657298,0.07749236,-0.04184785,0.04569934,-0.017640881,-0.03221387,0.040331382,0.23157857,-0.022834923,0.043734368,0.015736328,-0.018165922,0.04427944,0.031467006,0.0013029817,0.016038246,0.032582633,0.087903075,-0.0579502,0.034907967,0.056975555,-0.061623715,0.03723524,0.037153512,-0.059985436,-0.048163902,-0.005239113,-0.03370352,-0.024089405,0.082422175,-0.0325414,-0.021374922,-0.05650002,0.00010377556,-0.046514522,-0.061216045,-0.016242305,-0.02899706,0.020795047,0.03967546,0.003870585,-0.024333557,-0.022179967,-0.046025176,-0.026946675,-0.02249182,-0.07122435,-0.042746153,0.010575632,-0.0070871795]'),
      ('Car', '[-0.013675096,0.027324528,0.06942244,0.0013266953,-0.020453496,0.061512772,0.07301235,0.04290585,0.0023934839,-0.0186373,0.012499963,-0.093231946,0.028258853,0.07755444,0.025344418,-0.013358756,0.023283212,0.027902517,-0.032629456,-0.018587576,0.022121359,-0.01931578,-0.020892585,-0.07198824,0.0230901,0.058177892,-0.020588726,-0.020963376,-0.05223263,-0.13351677,-0.020541323,-0.034604687,0.05377937,0.015278223,-0.059128493,-0.054362193,-0.06885301,0.028906692,-0.052341137,0.01598179,0.027373223,0.015316855,-0.07678603,-0.053560868,-0.013568181,-0.0028432393,0.0071754334,0.020171847,0.07315126,-0.07514821,0.034036174,-0.017486328,0.014351431,0.004913182,-0.03727606,0.022707878,0.04719973,0.012032776,0.04482492,0.024352394,0.022039536,0.0637522,-0.22861159,0.055107087,0.07447692,0.0291307,-0.041218065,-0.040351745,0.047432993,0.054303292,-0.023342311,-0.016348358,0.0017851957,0.08988982,0.02769922,-0.049907178,-0.027979424,-0.042212185,-0.020770581,-0.021024825,0.029763328,-0.053642657,-0.066037476,-0.026011113,-0.010575339,-0.047937546,0.012322384,-0.056541797,0.029350977,0.014430247,-0.045942552,0.02386634,0.0029833463,0.0141003905,-0.05075455,-0.014572922,-0.008882929,-0.032623354,-0.050203778,0.26171875,-0.021519234,0.049280297,0.04079181,-0.028314121,-0.013227541,-0.012881829,-0.035419036,0.013379328,-0.030239435,0.04186359,0.017268283,-0.000105469044,0.0125124715,-0.082406625,0.034267884,-0.016504455,0.038922243,0.06633839,0.006251055,0.00037995656,-0.001207575,0.02512416,0.02109219,-0.02958561,0.0013164878,-0.103339866,0.04863308,0.11800187,0.034086462,0.037300617,0.056644727,-0.037418414,-0.061853807,-0.022136258,-0.019294702,0.017083716,0.012703508,0.015861439,-0.022639215,-0.007921711,-0.0544705,-0.12781276,-0.01872742,-0.123899914,-0.037989907,0.039830185,-0.05202418,0.05241696,-0.056093507,0.0136191305,-0.026056947,0.048660703,-0.0140545815,-0.0027107827,0.026319232,0.0021281333,0.04325188,0.058733158,-0.04723551,0.0404873,-0.034551904,-0.03205584,-0.053433474,0.06742306,-0.01232912,-0.08600591,-0.017429007,0.029419081,0.0026076722,0.013392961,0.03677887,0.031694457,-0.0720888,0.051425304,0.08350261,0.0315462,-0.004771196,0.034244526,-0.010683566,0.0025684177,0.028690292,0.010577877,-0.052667093,0.06373304,0.059627198,-0.019951789,-0.052819874,-0.0048726257,0.013962882,0.054299265,0.040887468,0.061755426,0.007373504,-0.0075591686,-0.062904865,-0.007323224,-0.016505266,-0.033703998,0.0016645732,-0.021243243,0.037611052,-0.020470953,-0.023737112,0.039098095,-0.0017488332,-0.038300857,-0.045891576,-0.016960371,0.022000503,0.043613117,-0.05976287,-0.0041191196,0.0744647,-0.0018894378,-0.03981402,0.012990376,-0.045962907,0.04994432,0.028742177,0.05744724,0.0009102008,-0.08285415,-0.03267054,-0.2337071,0.0072381077,-0.032779843,-0.024238953,0.041009203,-0.05101638,0.03943452,-0.02855554,0.04525813,0.046112273,0.05932983,-0.022565039,-0.009743857,0.061256465,-0.03037518,0.05372682,-0.010801566,0.01609313,-0.0061583905,0.042392224,-0.021107364,0.0141626885,-0.047603372,-0.057255138,0.040890433,-0.011722571,0.24564229,0.027296212,0.019367144,-0.016961388,0.06517703,0.043743007,-0.036653895,-0.055930916,0.013116146,-0.016756361,0.0140022235,-0.002970313,-0.070724085,-0.057791047,-0.013434504,0.029526964,-0.00938867,-0.05977423,0.00040445486,-0.014813439,-0.029890385,0.009259244,-0.027084075,0.041800242,0.06727153,-0.0014996758,-0.00021284568,0.016413646,0.0063286186,-0.019270584,-0.06787658,0.029184967,0.0017431147,0.0158079,-0.019558495,-0.04962892,0.017410701,-0.034534313,0.019038094,0.006610072,-0.04343159,-0.031567868,-0.009124138,-0.036709126,-0.015360762,0.10293102,-0.010821659,-0.015772903,0.07758905,-0.015194392,0.060196437,0.010410943,-0.021493688,-0.06867113,0.0474313,-0.017747577,0.042234425,0.05141044,0.057146773,0.006755126,0.049838375,-0.043111365,0.0029671746,-0.026369067,0.03549969,0.0017129052,-0.028317036,-0.05324385,0.039201375,-0.017168764,-0.2772981,0.04778095,-0.0069068763,0.06153518,-0.041531466,-0.0171643,0.013447191,0.03850731,-0.07578842,-0.029888023,0.012730036,0.03311835,0.059252515,0.040407978,-0.0048704417,0.043922015,0.05727722,-0.019122923,0.06540934,-0.008338455,0.010299639,0.053547956,0.20795242,-0.013018652,0.056953296,0.025129227,-0.018750485,0.008330987,0.02540996,-0.0045585097,0.038341187,0.035392124,0.0776931,-0.032675825,0.01839246,0.008414822,-0.07650357,0.07460887,0.015999481,-0.0143431565,-0.03775232,0.073541395,-0.029160045,-0.033645444,0.05769379,-0.037698034,-0.022761138,-0.066397525,0.013602807,0.005004625,-0.012167106,-0.0090219155,-0.07229144,-0.007818199,0.055839863,-0.023012608,-0.058479514,-0.023438398,0.007111039,-0.039270334,-0.007022913,-0.03873911,-0.016563078,0.015775386,0.025728563]'),
      ('Train', '[0.008390516,-0.0316401,0.059414063,0.004530007,0.015912173,0.04257927,0.010396404,-0.0041149696,-0.016684545,-0.051473897,0.013823704,-0.105521135,0.016059747,0.06659557,-0.0019725815,0.011045265,0.013066779,0.036172472,-0.043464396,-0.0027326334,0.008696494,-0.04963063,-0.05605746,-0.064520374,0.0071908,0.025355445,-0.050698947,-0.038638834,-0.06742357,-0.12853827,-0.0057572243,-0.052277446,0.019270279,-0.03003436,-0.035267383,-0.019119024,-0.022066116,0.054565392,0.021611325,0.034875706,-0.0043622344,0.014108078,-0.028517855,-0.08079088,-0.019493984,-0.034040015,-0.011598695,0.0022550072,0.0621561,-0.03721019,0.047613103,-0.04455849,0.03295865,-0.0057227607,0.0075348215,0.04554701,0.04224485,0.02329181,0.05805944,0.063204974,-0.031965736,0.048258748,-0.24342363,0.07309152,0.032815777,0.002095711,-0.059097853,-0.015610364,0.004590883,0.066063076,-0.017212633,0.021194763,0.007554856,0.033827275,0.014187226,-0.047569692,0.017875925,-0.017424196,-0.015125725,-0.005130941,0.0029060335,-0.050865818,-0.07343196,-0.0048736315,0.020944353,0.00077292154,0.032206904,-0.05818478,0.02801747,0.017108278,-0.03247216,-0.0065188087,-0.015342016,0.026298964,-0.049516425,-0.005976409,0.01856195,0.0074172006,-0.075506,0.23088165,-0.021115338,0.05222904,0.023529256,0.043604445,0.019573972,-0.026877966,-0.04359997,-0.0046444032,-0.02207065,-0.040870916,0.009780713,-0.025193287,0.024433471,-0.024275476,0.009968814,0.047776353,0.06775558,0.01283433,-0.052475818,0.008022442,-0.0013314247,-0.020480726,0.023044059,-0.028667847,0.0060666455,-0.09205764,0.048958115,0.122405164,0.019911205,0.022110125,0.050222058,0.0015760588,-0.078412116,-0.0021023818,-0.011218934,0.025069945,0.03324381,-0.0006424969,-0.043864336,0.0087585915,-0.05289486,-0.092548065,0.03227165,-0.1332425,-0.072707884,0.03601174,-0.029322848,0.04662253,-0.023045791,0.009186527,-0.04339159,-0.004015751,-0.002390284,-0.03830812,-0.003402778,0.042021804,0.039244883,0.05749609,-0.056686126,-0.007919699,-0.053717893,-0.016760027,-0.06469656,0.07116849,-0.015925482,-0.095182456,-0.019137817,0.011446995,0.022712586,-0.065081745,0.016669888,0.03609432,-0.022235183,0.019483658,0.06453548,0.0013194638,-0.042232916,0.012842303,0.0008133578,0.016946876,0.054184176,-0.061540544,-0.064576425,0.05788913,0.043063283,-0.017957158,-0.00660539,-0.043557957,0.00932838,0.04721575,0.025036044,0.05511638,0.012167107,-0.073137976,-0.05228407,-0.027518198,-0.020088682,-0.04512229,-0.0067941146,-0.038086385,0.026816027,-0.0076302565,-0.030978065,0.011591283,0.010688236,0.062038578,-0.034754917,-0.029522672,0.023187192,0.029613974,-0.03722577,-0.012008098,0.102696836,-0.007421274,-0.027096916,0.02145292,0.026860205,0.06578597,0.01368912,0.040054448,0.04191872,-0.038178593,-0.063167796,-0.22596447,0.06565988,0.04722377,-0.016424103,0.05646301,-0.045248922,0.017223613,0.004723246,0.03433359,0.057964306,0.107756704,-0.040196985,-0.005384249,0.02173068,-0.021337867,0.053607292,0.027281692,0.039454717,0.015027626,0.02693812,-0.003631183,0.048921067,-0.019490749,-0.09182499,0.02951052,0.0186495,0.23003499,0.06202278,0.07585443,-0.06069743,0.0526535,0.02085982,-0.018095134,-0.1384808,0.041433737,-0.026623845,0.07243412,-0.0029134983,-0.0069605466,-0.019240782,-0.018164543,0.029909529,0.01608419,-0.061474107,-0.013469153,-0.029401183,-0.054659907,-0.014086127,-0.028915925,-0.0069666984,0.041868016,-0.013776472,0.039377537,0.028841918,0.015001087,-0.03712765,-0.067391664,0.031860482,-0.041481916,0.037242766,0.0051513095,-0.024946364,0.02921327,-0.038750958,0.037937738,0.025684254,-0.050058123,-0.00971873,0.058333874,-0.020295596,-0.043949526,0.08710765,-0.0069736685,-0.047693014,0.070027076,0.017611913,0.009996941,0.025605714,-0.014843967,-0.0095715,0.03362786,-0.027780171,0.034262065,0.054104224,0.015189927,0.00031930834,0.08402685,-0.020675372,0.05230347,0.00073445373,0.0203266,-0.010717519,-0.017641345,-0.049389035,0.03801163,-0.04896717,-0.2916533,0.02326856,0.030424621,0.057537735,-0.03546315,0.009951743,0.021315183,0.04614926,-0.01757108,-0.025332054,0.047435746,-0.004156035,0.070101835,0.006551285,0.014095617,0.035963845,0.09428639,-0.011006988,0.043139383,-0.028828816,-0.010830479,0.052022237,0.17928188,0.013676716,0.048481107,0.020455515,-0.03715131,0.0030906487,0.014748529,-0.026029117,-0.016452625,0.011987294,0.085855335,-0.012302411,0.037723247,0.0425674,-0.055454608,0.1011866,0.028467475,-0.030435871,-0.061414395,-0.017736189,-0.03107398,-0.06507749,0.09065245,-0.021177005,-0.016372962,-0.038777813,-0.00053287647,-0.031771254,-0.023859626,0.00845833,-0.0049761753,0.020201217,0.013904311,0.069836564,-0.023459928,-0.09671563,-0.0576696,-0.027228847,0.012631596,-0.07193221,-0.028516661,0.023131175,-0.0030631186]'),
      ('Cat', '[-0.018450698,-0.043701697,0.02752752,0.017822262,-0.02586187,0.028728843,0.087862626,0.09188629,0.04397456,0.019820007,-0.011478988,-0.124914564,0.022496833,0.0789779,0.034404784,-0.0018655953,0.0054757334,0.015263809,-0.057259317,0.036637075,0.023102136,-0.0009833422,-0.04593577,-0.039027687,0.03276482,0.02076609,-0.06291743,-0.03474475,-0.061686616,-0.13878247,-0.031647447,-0.041279692,0.020428147,-0.008723568,0.002749398,-0.029738247,-0.02957134,0.03317581,-0.048957366,0.07942621,0.023558771,-0.014116848,-0.039913174,-0.029095618,-0.024157254,-0.0028368456,-0.002660644,-0.024854222,0.047447667,-0.04207809,0.0380363,-0.053787,0.020617802,0.022927182,0.001773245,0.02898622,0.10264574,0.021385299,0.06101385,0.043469504,-0.0060370597,0.072553396,-0.22571594,0.08177761,0.059967242,0.011605421,-0.059498515,-0.01614959,0.030732593,0.047745787,-0.034631897,0.0045268857,0.008692242,0.06986321,0.062867224,-0.04319344,0.044571005,-0.008040988,-0.007348434,0.0011111188,-0.040033247,-0.013782392,-0.05935072,-0.031029223,-0.025947869,-0.0116114365,0.009943911,-0.04495425,0.039871536,-0.007157675,-0.081404544,0.028082607,0.0002421545,0.0007344536,-0.058870003,-0.03995512,0.02585252,0.005922959,-0.06805756,0.23782046,-0.030170966,0.021609697,0.06485757,0.011116049,0.033888217,-0.021842483,-0.013968073,-0.025392892,-0.017921817,0.028079357,0.025580177,-0.01690473,0.013204528,-0.058830313,0.051414166,0.0056296475,0.042037394,0.010937367,0.03082752,0.017728599,-0.021105066,0.009503924,0.014986898,-0.010639325,0.037019223,-0.08803974,0.027080009,0.089853294,0.022677682,0.03509845,0.062564544,-0.042244546,-0.050651427,0.0051593627,-0.021133807,0.014681318,0.011123185,0.01954461,-0.011593424,-0.048611607,-0.06442908,-0.10776765,-0.0038715384,-0.113250785,-0.0013805063,0.054094467,-0.038793158,0.05252635,-0.02806175,0.028057585,0.00038797883,0.0248274,-0.053739987,0.02358995,0.009615382,0.0459994,0.028359659,0.043745056,-0.039010525,0.04654124,-0.0046896823,-0.046589084,-0.03633851,0.046189457,-0.013919748,-0.058477134,-0.039088316,-0.030274736,-0.03725408,-0.0074797496,0.05141904,0.03529245,-0.05534556,0.029533856,0.022124233,0.030466082,-0.0037198325,0.018125841,0.005400052,-0.0031701697,0.052214794,-0.03613148,-0.06176965,0.07701529,0.04650194,-0.040947113,-0.048056263,-0.018562285,-0.039730188,0.011365426,0.030509792,0.03808681,0.025926674,-0.07370392,-0.014984852,-0.060279783,-0.008129943,0.008534002,0.035145674,-0.023089841,0.0450473,-0.040078282,-0.067101896,0.054339316,0.018377861,-0.00052205054,-0.022381525,0.022282446,0.032699667,-0.004516875,-0.023932414,0.011830043,0.044468176,0.008677854,-0.03468146,0.007745217,-0.018356947,0.05176401,0.04161891,0.0716399,0.043909244,-0.097493835,0.004900012,-0.22551504,0.010759602,0.025497086,-0.057824843,0.03173267,-0.052233122,0.036974248,-0.015762206,0.07025136,0.031058954,0.057086878,0.0040403972,0.0186622,0.076763235,-0.05272903,0.07411572,0.030511323,-0.021784903,0.019807672,0.01646691,-0.03355217,0.039836943,-0.016108982,-0.081623204,0.02503345,0.023985993,0.23101605,0.09275506,0.036845215,-0.012754107,0.0072520603,0.02663752,-0.08013605,-0.122672945,0.029188603,0.047271322,0.029502878,-0.021710804,-0.057916477,-0.033290174,-0.04561619,0.024318889,0.011353906,-0.060632486,0.015315554,-0.017688273,-0.04047208,0.030335227,-0.041746728,0.027059378,0.05104814,-0.024603844,0.0070659635,0.0025729148,0.00940215,-0.024860522,-0.07034691,0.005896322,-0.0069140918,0.01501812,-0.006486526,-0.09685372,-0.0012098794,-0.01242476,0.05278896,-0.0022264088,-0.024227154,-0.012343209,0.023066515,-0.06185465,-0.032793436,0.09016447,0.026167199,-0.051579386,0.023778731,0.02196033,0.011613299,-0.0028875927,0.007213804,-0.077984005,0.04114566,-0.042612676,0.030693077,0.018442027,0.018042548,-0.016633302,0.023955721,0.0065748747,0.048678786,-0.060312748,0.049683105,0.031068243,-0.035726856,-0.04870947,0.009337585,0.00096612197,-0.28237557,0.01650157,0.030606303,0.0030061172,-0.043637928,0.012786108,0.0047959434,-0.018053751,-0.057656318,-0.032992836,0.035492945,0.040239785,0.03308174,0.030363094,-0.0074734306,0.031687234,0.042816583,-0.02827239,0.025122454,-0.017633557,0.05130058,0.019577276,0.26412004,-0.03937151,-0.005285997,-0.026237419,-0.031395175,-0.023909623,0.050147958,-0.021485727,0.008363846,0.020979265,0.05898941,-0.0369355,-0.009970978,0.058567666,-0.053051967,0.07510308,0.007790953,0.0030001537,-0.018105373,-0.0024175737,-0.03822598,-0.054223735,0.07783055,-0.049844712,-0.047537003,-0.016613906,0.014751358,0.005427777,-0.044944577,0.0005177255,-0.036851205,0.035434917,0.0031816638,0.017624011,-0.0002383347,-0.027522223,-0.04817965,-0.02104149,-0.024004659,-0.025815494,-0.0032800396,0.046702977,0.059542835]'),
      ('Dog', '[-0.03398106,-0.04587913,0.05834977,-0.012510896,-0.020966545,0.024606809,0.06725803,0.04614885,0.021671712,0.0028234783,0.024777666,-0.13095815,0.050201364,0.04163673,-0.0073551466,-0.007788209,0.02349543,0.045571007,-0.101046406,0.026413035,0.006283169,-0.029844765,-0.018307583,-0.067260414,0.0071734344,0.040144853,-0.022258174,-0.035064206,-0.038951498,-0.10791707,0.012508958,-0.029608397,0.01737361,-0.0048551173,-0.0137116825,-0.021252109,0.016257621,0.030800447,-0.010203598,0.05779155,0.03410912,-0.014027831,-0.06014225,-0.0737464,-0.00296503,-0.02995635,-0.048784822,-0.04322933,0.04992729,-0.054626375,0.020585917,-0.02869352,0.027270662,0.028178928,0.023035508,0.027372012,0.07144182,0.027457833,0.006989782,0.026272165,0.020677378,0.1046711,-0.18369348,0.10478375,0.06709782,0.031876415,-0.080784984,-0.0318658,0.0068611726,0.04296069,-0.014369754,0.025120066,0.0064320723,0.07627709,0.009385011,-0.046717513,0.02942915,-0.011798346,0.0024966802,-0.0011040273,-0.01837801,-0.024898572,-0.056603655,-0.0049853246,-0.018484253,-0.047932703,0.005165957,-0.07251493,0.015559436,0.06264581,-0.039077364,-0.02403875,0.013444196,0.014436731,-0.056333177,-0.015642889,0.021741007,-0.013120397,-0.033021655,0.20736612,-0.06262082,0.042408913,0.063680306,-0.047230907,0.07333769,-0.0058781966,-0.019491186,0.001866874,-0.04809916,0.038134225,0.004091026,-0.02022125,0.0114840185,-0.066682085,0.03792327,-0.00015499585,0.027070496,0.049177293,-0.0024667126,-0.01676147,0.005352208,0.03228254,0.020419393,-0.023925079,0.027801491,-0.08195105,0.02263024,0.1127984,-0.01773959,0.013458586,0.023056736,-0.0405365,-0.019467505,-0.007438755,0.014741418,0.019221041,-0.002969519,0.0063020405,0.0050163977,-0.019422691,-0.042474356,-0.11976022,-0.0031393368,-0.09801484,-0.0063687935,0.054927953,-0.034576423,0.018661322,-0.06695973,0.018426672,-0.041533604,0.012827635,-0.0216872,0.029161893,0.035417113,0.014812438,-0.0005086224,0.047981005,-0.033724055,0.02964084,-0.011219084,-0.030708553,-0.05478525,0.061196856,-0.01044451,-0.06580756,-0.071939655,0.028023727,0.019572273,-0.0066324784,0.03371536,0.03889294,-0.043447196,0.023165302,0.02749916,0.036996223,-0.017910937,0.0034960753,-0.017619846,-0.01682099,0.07389422,-0.022559106,-0.075324185,0.068171635,0.039252345,-0.01228445,-0.019303845,0.0061936406,0.0069038905,0.05486719,0.021220824,0.06319413,-0.00862464,-0.07810823,-0.06715591,0.01653162,-0.074353494,-0.0055483994,-0.00432403,-0.029376648,0.044267535,-0.02766178,0.015305978,0.041949403,0.05040478,0.03711134,-0.006646276,-0.024122963,0.04387694,0.042191654,0.0040298956,0.026687108,0.026858974,5.975888e-05,-0.042856377,0.02719497,-0.015184606,0.017931797,0.051872507,0.040260497,0.034908403,-0.10133825,-0.035800662,-0.21634497,0.026843634,-0.0035379566,-0.026145559,0.03874843,-0.0072357524,0.03432515,-0.01789197,0.05520025,0.056067273,0.06292914,-0.0063916366,-0.0250232,0.029002182,0.0048547084,0.09028735,0.063608915,-0.013629017,0.007837473,0.030949071,-0.0305201,-0.016705323,-0.029566336,-0.041564044,0.020007666,-0.029416997,0.24585131,0.071520805,0.027776815,-0.010154821,0.019156493,0.026060665,-0.05332598,-0.117831856,0.05576698,0.0611754,0.023893401,-0.03997265,-0.042899795,-0.053757526,-0.03657568,0.07664591,-0.032604933,-0.07631975,-0.024680613,0.0060682655,-0.0018964773,0.012901344,-0.042611003,0.02288525,0.031216914,-0.034108605,0.022518663,0.01629285,-0.029234616,-0.015426959,-0.034897357,-0.021540005,0.0015016403,0.033357877,0.0284842,-0.092373185,0.02815136,-0.034993514,0.037112426,-0.023349721,-0.03801536,-0.03301411,0.007879143,-0.037546758,-0.010365928,0.07326524,0.048820004,-0.030361317,0.056599807,-0.006202554,0.05078459,-0.046470087,-0.022375962,-0.027263056,0.04430895,-0.066588804,0.048946578,-0.008054631,0.056988873,-0.022817664,0.05487547,-0.00850563,0.06884496,-0.056825235,0.013956117,-2.8598519e-05,-0.008391312,-0.07403139,-0.02054572,0.00740269,-0.28785047,0.035514876,0.018872637,0.016709069,-0.022526452,0.04546725,0.0036777765,0.010296626,-0.07546051,-0.045812447,0.010187619,0.027719049,0.031793963,-0.013236095,-0.020754311,0.03627473,0.011115101,0.014774947,0.04622976,-0.029250441,0.020287355,0.017993618,0.24138238,-0.07568645,0.07751866,0.012238281,-0.041065767,0.051694304,0.04204503,-0.028650045,0.07694258,-0.021129774,0.10736998,-0.045513943,0.010340816,0.046329513,-0.046585277,0.09469313,-0.012459904,-0.055962075,-0.015606321,0.033765897,0.0049535953,-0.07884165,0.08479913,-0.035221465,-0.04052622,-0.02334385,0.022412961,0.017682377,-0.044773374,-0.06094265,-0.044896785,0.022461157,0.039271295,0.01423678,0.011286046,-0.05645138,-0.055751275,-0.06402218,-0.0047324076,-0.059572853,-0.025109097,0.03716784,0.05244787]'),
      ('Apple', '[-0.01854126,-0.015314187,0.008172714,-0.06233388,0.0151762,0.00095683464,0.046682406,0.036243673,0.014862187,-5.8190515e-05,-0.004585996,-0.061831452,0.0136447605,0.017847551,0.048936155,0.031905275,0.0249166,-0.01723699,-0.074055605,0.0044720536,0.009445661,-0.03197354,0.016330265,-0.05798763,-0.0005444175,0.03345155,-0.06665412,-0.031260885,-0.047729123,-0.15240389,-0.0369158,-0.061498255,0.056001555,-0.00027440977,-0.00062431605,-0.037229452,-0.049931858,0.040858846,-0.031204186,0.03931569,0.05703049,-0.009987068,-0.036490448,-0.018589024,-0.028742589,-0.016836036,-0.024929224,-0.024623353,0.058277316,-0.02126311,0.055800628,-0.016453594,0.023494028,0.011939725,0.006316923,0.05045838,0.052253675,0.0018598125,0.057757597,0.064271055,0.021462984,-0.00598809,-0.20524058,0.098078206,0.07923511,-0.0030400828,-0.0042570103,-0.048181806,-0.0057368493,0.028005809,-0.027782597,0.036583826,0.056569118,0.06672127,0.0072273863,-0.010364808,0.037651714,-0.058841214,-0.0156052,-0.007403201,0.004738195,-0.045108624,-0.043373324,-0.010229558,-0.015398024,-0.027500607,-0.013934623,-0.06390856,0.07244502,-0.03525932,-0.09858975,-0.017686404,0.032276656,0.013345139,-0.07269245,-0.0112332925,0.062022205,-0.0022355681,-0.067187116,0.25026688,-0.03435671,0.0287123,0.054986972,-0.059890773,0.027863948,-0.042722933,-0.01700282,-0.019195212,-0.025022611,0.019158779,0.031251986,-0.01125425,0.03872539,-0.020698383,0.030452145,0.014281089,0.029007183,0.047993574,0.01674457,-0.016619984,-0.017566299,0.061561186,0.029382374,-0.01851396,-0.0062958095,-0.09839809,0.020334389,0.1269149,0.0069205724,0.016504155,0.08306016,-0.060554363,-0.07056581,-0.021461489,0.049540065,0.047961283,-0.018516554,-0.022690471,0.044891648,-0.04310284,-0.078540124,-0.07875601,0.012700777,-0.09502175,-0.022846391,0.0629089,-0.015902612,0.046939265,-0.07262387,0.02601966,-0.005264863,0.04895392,-0.01572132,-0.0102630425,0.015897341,0.053458296,0.03574174,0.04539862,-0.035877213,0.03533559,-0.042159036,-0.0087875,-0.049615107,0.07554764,0.011191862,-0.113151096,-0.017492991,0.043413684,0.002327505,0.0073730038,0.02350307,0.035559066,-0.0059042843,0.05666708,0.044166975,-0.00055669114,-0.036611453,-0.0014730084,0.007564431,-0.011042368,0.05057629,-0.052485,-0.044374328,0.009869594,0.08320784,-0.023855116,0.015617343,-0.030314686,0.07130435,0.03665859,0.00899261,0.05550256,-0.054251857,0.0035008327,-0.046595573,-0.042936403,-0.011023383,0.018025259,0.026859136,-0.029143373,0.049876533,-0.0034112562,-0.041306548,0.055204038,0.019626688,-0.016427225,-0.04123679,0.0017773646,0.046740886,0.012245985,-0.033460714,0.021917453,0.07956837,-0.028794741,-0.08975127,-0.0037019434,-0.00012320856,0.03852602,0.053377513,0.040480774,0.0314433,-0.090866566,-0.081535764,-0.2508107,0.007842401,-0.0103528835,-0.017066024,0.031441532,-0.040664397,0.017248066,-0.034469314,0.016174091,0.04561676,0.05384791,-0.00558743,0.018127529,0.07188737,-0.029698055,0.04265685,0.06104204,0.012198512,-0.0036226169,0.044303454,-0.028414544,0.045141805,-0.028411133,-0.04631903,0.030073958,-0.0515857,0.220293,0.04642099,0.018873105,-0.011720311,0.032426845,0.046402935,-0.03572881,-0.09112599,-0.003913931,0.06326886,0.008865028,-0.041372094,-0.06256474,-0.028682407,-0.040730156,0.047156896,-0.0039689573,-0.05302654,-0.003632981,-0.07732629,-0.01206058,-0.019614374,-0.028614756,-0.011921634,0.05150594,-0.015722616,0.035769034,0.012905213,0.04353579,-0.038506903,-0.12106876,-0.009226098,-0.019487629,0.004457581,-0.014951816,-0.045989387,0.001073027,-0.009420541,0.02853069,0.0024750652,-0.028357819,-0.01819474,0.003554601,-0.090667546,0.004385141,0.08669419,0.017099937,-0.033926927,0.07534967,0.030821681,0.04540305,-0.018767703,-0.037650675,-0.05459591,0.038266014,0.024052124,0.047373742,0.037806187,0.017990775,0.026818357,0.07041108,-0.012912107,0.009049243,-0.06140867,0.026061615,-0.0029175973,-0.0034775157,-0.038097065,0.046520837,0.0017449996,-0.24222106,0.061534416,0.014094019,0.03329052,-0.05666281,-0.0035613445,0.005507172,0.044907637,-0.011295333,0.04069986,0.0023404267,-0.017250648,0.055625506,-0.040635873,0.01642882,0.03416647,0.06721072,-0.081086,0.04945331,-0.037871443,0.022263754,0.02852677,0.21389903,-0.088772774,0.0179931,-0.008538751,-0.02611542,0.043828104,0.0054317755,-0.009777816,0.035113182,-0.007311843,0.07082955,-0.021270387,0.0066470113,0.054239165,-0.06441797,0.038601365,0.0214487,-0.025159193,-0.03238237,0.02582966,-0.079627365,-0.011419387,0.0626605,-0.024815738,-0.03032111,-0.05392778,0.01915368,0.01430781,-0.015028325,-0.050955776,-0.0047285086,0.033127528,0.016017374,-0.016613198,-0.051912326,0.015621767,-0.06378826,-0.056303553,0.040947903,-0.033308335,-0.016811416,0.07314832,0.052192166]'),
      ('Boat', '[0.003451573,-0.03132442,0.041924234,-0.07151062,0.005656667,0.014033464,0.038280405,0.07405814,0.002665217,-0.020811431,0.02797703,-0.082384676,-0.021522641,0.049683884,0.0051554316,0.05113881,0.011191923,0.08603509,-0.04581075,0.027721966,0.06524101,-0.011254213,0.005719363,-0.06456224,-0.0067095314,0.027434144,-0.03998245,-0.018337745,-0.03024962,-0.12786528,-0.007468822,-0.07965858,0.002841196,-0.026761381,-0.002710397,-0.032941174,-0.01157222,0.01835951,0.025763508,0.017918503,0.01716959,0.04313931,-0.06198612,-0.019062353,-0.045892436,-0.047470637,-0.025586972,0.015134751,0.07648262,-0.04904084,0.013845007,-0.031671703,0.027743243,0.03567014,0.044218108,0.024208087,0.053735733,0.013186276,0.0055202967,0.042314656,0.05348394,0.024783261,-0.22115476,0.08963771,0.013096708,0.044419244,-0.017883042,-0.03910393,0.013514059,0.016817786,-0.052290566,0.017970363,0.042357013,0.10046304,0.027752733,-0.061662477,0.006513319,-0.037006002,0.027272867,0.027006257,0.0142964665,-0.06360955,-0.021836504,-0.029386729,0.003337552,-0.017336056,0.029699676,-0.026593758,0.03526048,0.0039346362,-0.057597876,-0.029524093,-0.01730882,0.014263184,-0.08440352,-0.00041559874,-0.004121953,-0.007147526,-0.066180535,0.21969096,-0.026675701,0.04207458,0.061170112,-0.024306577,0.044024996,-0.02159782,-0.052115727,-0.013863495,-0.04703005,-0.0007212677,0.004346511,-0.014467664,0.00843103,-0.013691538,-0.0072375066,-0.019222913,0.018029733,0.03152813,-0.030557722,-0.029202802,-0.014806717,-0.0122162895,0.05467705,-0.011990872,0.029487923,-0.057703514,0.08271271,0.105192035,0.007058811,-0.005331917,0.001044727,-0.027939413,-0.03841933,-0.039499894,-0.03354715,0.0008674375,0.0028215437,-0.0063504563,-0.0013052999,-0.01605963,-0.08247868,-0.12754385,-0.00035439446,-0.1135988,-0.043434717,0.038759027,0.002496258,0.03332463,-0.021302817,0.03943982,-0.017751513,0.04280311,0.0033914526,-0.023666987,0.018136116,-0.014481841,-0.019508364,0.07897855,-0.043303143,0.018218957,0.006419149,-0.03464293,-0.03316714,0.039618928,0.0014868038,-0.13087988,-0.0063449526,0.04696695,-0.030625926,-0.005813282,-0.01341627,0.05716793,-0.016758876,0.05393186,0.0777393,0.033460576,0.03254571,0.01969365,0.018621385,-0.009796354,0.050934583,-0.015461227,-0.054977313,0.071182206,0.06265597,-0.042554602,-0.02824994,-0.029791066,-0.0006974022,0.028558008,-0.0026959486,0.050376188,0.00981151,-0.026680889,-0.023037922,-0.030151734,-0.002973705,-0.055360682,0.0035032611,-0.049384903,0.075852595,-0.0074333963,-0.02718678,0.053259887,0.025845353,0.009015108,-0.017371554,-0.024869643,-0.013716606,0.07490163,-0.06361879,-0.0013744798,0.059992343,-0.015204999,-0.0040881964,-0.006926333,0.031526998,0.046630118,0.011222064,0.057870165,0.017545212,-0.118089885,-0.038867403,-0.21836688,0.017069299,0.013897999,-0.020572908,0.029985098,-0.011952629,0.015152697,-0.007566515,0.018216064,0.059099764,0.082925186,-0.041308656,-0.05755257,0.04316516,-0.03456597,0.04691071,0.03894865,-0.0013005713,0.0054288697,0.020903084,-0.0019502229,0.068574,-0.012678534,-0.023653781,0.05061183,-0.015460681,0.23320328,0.0540902,0.03415941,-0.03924938,0.0067807026,0.045057893,-0.026635995,-0.07698709,0.032159913,0.053191483,0.05704703,-0.021299265,-0.008298184,-0.03543195,-0.0070491335,0.04338257,-0.016497841,-0.073704265,-0.044688262,-0.040218312,-0.03207327,-0.0046671163,-0.023711322,0.0096596815,0.065637104,-0.02979161,0.033204455,0.026951917,-0.0056020273,-0.028967276,-0.07344016,0.009490252,-0.034891546,0.021181162,0.016997669,-0.01901137,3.6806174e-05,-0.05220573,0.03200216,-0.021272521,-0.030422227,-0.05363538,0.043928806,-0.08376253,-0.037302095,0.097358674,0.024045696,-0.017719422,0.081304744,0.047614243,0.06464407,-0.032756194,0.018565826,-0.044159252,0.040204857,-0.041234825,0.023949105,0.002779979,0.04253179,-0.015129703,0.022819744,-0.03602023,0.026503049,-0.03500936,0.021883395,-0.013415038,-0.025728086,-0.037960876,0.037986066,0.021162277,-0.31060043,0.031734128,-0.0022458911,0.042481646,-0.028866054,0.030208766,-0.0133145405,0.054347847,-0.073503956,0.062051143,0.005826847,0.049297802,0.030884407,-0.01688786,0.017160615,0.063027896,0.07313963,-0.09637417,0.051601037,-0.043145347,0.000817891,0.028763952,0.19526124,-0.041790325,0.0726707,-0.024687497,-0.0013156298,0.045472197,0.0010359284,0.015232671,-0.027874617,0.014591427,0.13244624,-0.039644428,0.020199,0.060292866,-0.05322153,0.03625645,0.017529424,-0.07401374,-0.036324993,0.025423175,-0.0046287975,0.0029855266,0.0834278,0.0056974366,-0.028152516,-0.091103025,-0.006525143,0.022312194,-0.03248805,-0.02484882,-0.049474973,0.010650054,0.08801606,9.901745e-05,-0.021977311,-0.03979849,-0.04118816,-0.019186275,-0.01652988,-0.068953894,-0.019286314,0.003947064,0.024725446]'),
      ('Mouse', '[-0.040377103,-0.022667758,0.040733587,-0.045248877,0.004549785,0.01920968,0.09860386,0.06337809,-0.014576411,-0.016547957,0.0043267845,-0.08727463,0.001282113,0.034992572,0.0030495469,-0.005252943,-0.030733703,0.028543305,-0.05340802,0.02134803,0.009926888,-0.041723635,-0.014660007,-0.08119687,-0.006314594,0.0060803755,-0.047911707,-0.05091576,-0.06703744,-0.12893848,-0.028729504,-0.08311065,0.02901125,-0.032763172,-0.016858876,-0.027138188,-0.0064179595,0.051359415,-0.036937356,0.0038320909,0.042311396,-0.0014249359,-0.035213996,-0.03742079,-0.0014664575,-0.06288964,-0.008702811,-0.07035538,0.060513493,-0.022340205,0.040802304,-0.061406795,0.031208003,0.0147732375,-0.003349924,0.06889083,0.05477272,0.015846154,0.026901418,0.053733137,0.0089147,0.039884243,-0.189687,0.11407487,0.06078271,0.01646154,-0.010769416,-0.032409113,0.0042888965,0.046524018,-0.026228016,0.034576327,0.016356785,0.041987386,0.016909229,-0.048567224,0.0051092454,-0.027669614,0.027950207,0.032748174,-0.0155055905,-0.010912961,-0.023338877,-0.035056524,-0.027904602,-0.014183707,-0.019453976,-0.04282588,0.042652052,-0.021886067,-0.060644917,0.002340001,-0.011412922,-0.02175079,-0.0628676,-0.020284368,0.028365277,-0.005381609,-0.06597562,0.2276179,-0.07204836,0.019287534,0.041096047,-0.008528844,0.053176265,-0.018938553,-0.024173811,0.004013408,-0.039964236,0.03459345,0.010588582,-0.012078534,0.04525789,0.040328942,0.056859862,-0.041445985,0.034563567,-0.0031516568,0.0024751956,0.02141297,0.03361259,0.02350541,0.01680798,-0.0155210495,0.053421624,-0.053505804,0.012133244,0.12967996,0.003512417,-0.0025171235,0.046476725,-0.021781605,-0.05417786,0.020584816,0.020197248,0.02124402,0.015991636,0.022962242,-0.0012403561,-0.019248111,-0.06310508,-0.08227882,0.020538509,-0.109149516,-0.007844497,0.061679017,0.008426173,-0.0001732398,-0.049548004,0.050724845,0.004618454,0.03646706,-0.01484994,0.0039252373,-0.001403416,0.0664071,0.044730082,0.056839943,-0.034704044,-0.004730838,-0.023692578,-0.041346386,-0.04952889,0.056637034,0.015446278,-0.052159145,-0.023077182,0.055139318,-0.008341107,-0.023459954,0.033917475,0.0136266155,-0.04708014,-0.02063184,0.010280838,-0.02531588,-0.030564599,-0.025114458,-0.013762015,-0.010960947,0.09357507,-0.03643072,-0.042957693,0.05346348,0.04329152,-0.031103479,0.021322457,-0.01522813,0.0045661125,0.0537702,0.03209306,0.021443458,0.026066622,-0.089546315,-0.034763128,-0.055409517,-0.0222944,-0.025723536,0.017936995,-0.04824363,0.060549878,-0.037808303,-0.02620794,0.055600144,0.022436464,0.042776663,0.01612469,-0.02875058,0.051428806,0.017573178,-0.034467857,-0.0077900384,0.07513873,-0.019162938,-0.06555969,-0.014434028,0.047275715,0.02736343,0.080187805,0.0610768,0.037875254,-0.05696084,-0.05987416,-0.19478716,0.017523922,0.02172921,-0.03269391,0.021612154,-0.023794834,0.038002275,-0.006948219,0.07060778,0.035000026,0.08414426,-0.03226489,0.005960691,0.019508546,-0.07250312,0.07385267,0.06994085,-0.0026967837,0.0037788237,-0.02082316,-0.022966592,0.033302892,-0.02538656,-0.051392157,0.01707558,-0.008380637,0.2414746,0.124691986,0.015465072,-0.0077904454,0.020838035,0.030270401,-0.017751623,-0.094773516,0.053050213,0.08594688,0.021123288,-0.02162361,-0.029895138,0.0005249808,-0.06582099,0.01703007,-0.024236333,-0.09378091,-0.026403174,-0.025289156,-0.037038643,0.0076490627,-0.022255246,0.028706118,0.02795358,-0.034890853,0.00032028902,-0.019587833,-0.038501434,-0.01807254,-0.06457782,-0.0017888489,-0.0064566033,0.021368807,0.0021709246,-0.061586574,-0.010555151,-0.044502374,0.033785746,-0.007822122,0.007395981,-0.03746442,0.064690515,-0.0509958,-0.041122936,0.08854608,0.053750817,-0.021349406,0.05115994,0.017185207,0.024272623,0.029618526,-0.035643056,-0.047882646,0.042593274,-0.0027154651,0.039761305,0.060229745,0.027780939,-0.034311175,0.045842495,-0.01091128,0.053926248,-0.0791733,-0.01993803,-0.009639867,-0.042199083,-0.055512805,-0.011045383,0.0026950783,-0.31714323,0.03781929,0.03368966,0.03852715,-0.011875669,0.0084579475,-0.011901843,0.011003005,-0.108379245,0.005903558,0.045323938,0.042683903,-0.040204156,0.031824812,0.01460879,0.0362605,0.023287812,-0.013861633,0.040758185,-0.07728433,0.044756882,0.03642765,0.24435243,-0.036999114,0.032329634,0.028380118,-0.024428038,-0.016580353,0.042322036,-0.035527807,0.0089394795,-0.019262306,0.101632625,-0.04122436,0.03479015,0.07982272,-0.05526763,0.07851163,0.008479268,-0.008245814,-0.021768505,0.02535241,-0.031347126,-0.047938507,0.10146795,-0.034345087,-0.047936875,-0.050744783,0.0011701895,4.4137156e-05,-0.06095025,-0.0023069785,-0.03405786,0.013980694,0.018927611,-0.023111442,0.025687002,-0.026432892,-0.008920023,0.0053275074,0.031403854,-0.016360547,0.003918425,0.007261948,0.041972917]'),
      ('Chair', '[0.007242612,-0.008890708,0.07087478,-0.017218666,0.010161964,0.02288103,0.0628255,0.11424202,0.0042939316,-8.577153e-05,0.011476879,-0.062698215,-0.008032744,0.055627514,0.031709522,0.008646851,-0.007677212,0.066087514,-0.03393286,0.04595943,0.026118021,-0.022798017,-0.018182075,-0.05310286,0.00035249838,0.023295226,-0.042976543,-0.05135346,-0.03338082,-0.14254245,-0.014448951,-0.082290724,0.044443995,-0.03974422,-0.005926467,-0.049466074,-0.023638904,0.027757877,0.022480648,0.053556327,0.047165435,-0.004361453,-0.050542057,-0.0437012,-0.02560832,-0.06319627,-0.0041195815,0.003607597,0.052995015,0.011868532,-0.023403196,-0.03835523,-0.011448239,0.022041226,0.029417036,0.050992206,0.0633451,-0.023109978,0.00216341,0.034917675,0.057824627,0.018065536,-0.24170217,0.11006612,0.03759417,0.036411434,-0.052734427,-0.08213202,0.026438877,0.023384342,-0.0010665334,0.02193495,0.04822074,0.057032634,0.032533765,-0.06519445,-0.011998926,-0.07312999,0.04362863,0.029194133,0.036719177,-0.014692358,-0.047481794,-0.0066692936,-0.0130425235,-0.033006106,-0.006729974,-0.055479433,0.002909906,0.009901007,-0.054927614,0.016737197,-0.0039022777,0.012947416,-0.095253274,-0.03701531,0.009167639,0.0016094614,-0.074405804,0.23805863,-0.07722793,0.053594265,0.05661687,0.013474817,0.012748461,-0.043693267,-0.0058534956,-0.03260329,-0.046950907,0.0037631558,-0.031769305,-0.017883668,-0.0041399263,-0.00987623,-0.030830177,0.019493125,0.015751151,0.02815844,-0.035440337,-0.045221068,0.03741029,0.034405444,0.04950322,0.0002810536,0.039881516,-0.09062843,0.06339841,0.13453153,0.045081846,0.064796284,0.06558472,-0.037522722,-0.0140108,-0.02632353,0.00747574,0.015750928,0.0083326,0.012081866,0.004607462,-0.03037953,-0.004366012,-0.11747248,-0.04063107,-0.056524236,-0.03673815,0.06279229,-0.0609879,0.015854241,-0.045741193,0.050263777,-0.0073641776,0.050540153,-0.04290963,0.014485639,-0.0139276795,0.022182146,-0.012923014,0.036970016,-0.0050614392,0.0010354925,-0.023141287,-0.051632244,-0.018497547,0.09570567,-0.007901496,-0.09501473,-0.017568301,0.03681976,0.013184938,-0.029150061,0.044263743,0.030326407,-0.062202446,0.03592422,0.14171353,0.061264616,-0.0113557875,0.0057085683,-0.0042014834,0.024679331,0.09886817,-0.022956898,-0.08608185,0.03893355,0.039992895,-0.02475844,0.005292053,-0.008077331,0.023394594,0.03322915,0.0014964573,0.036820974,-0.08104848,-0.040368747,-0.023672812,-0.04888999,-0.017661298,0.0004353547,0.046766445,-0.02462088,0.05855902,0.019459412,-0.092742875,0.07434688,0.00040293016,-0.008316873,-0.022810865,-0.007958628,0.0145595735,0.046529282,-0.035352007,0.017510511,0.018891828,-0.03154253,-0.016596705,0.011242443,0.03959193,0.033566765,0.028399244,0.0078011975,0.045376554,-0.07623077,-0.039943192,-0.17767033,0.04873167,-0.0020410172,-0.04009756,0.024814129,0.007918742,0.024190389,-0.00033496512,0.013061196,0.008428592,0.04138094,-0.0040136375,-0.010298606,0.018205566,-0.05673821,0.09389307,0.041287653,-0.04741975,-0.009544984,-0.01219063,-0.006107758,0.04731182,-0.054440457,-0.02459403,0.032760493,-0.016143644,0.20824662,0.0641648,0.016682435,-0.037340675,0.042529665,0.0018572406,-0.010167863,-0.14444397,0.06076707,0.027983412,0.013329789,-0.094144866,-0.053174745,-0.023026194,-0.024418438,0.038233172,0.013938331,-0.03727014,-0.027715338,-0.053909518,-0.058724325,-0.023147698,-0.05653502,0.024166629,0.042693656,-0.07007864,0.027310245,0.035349548,-0.009547592,0.0021897648,-0.039186116,0.01033775,-0.019588446,0.043451525,-0.060066592,-0.03307617,-0.0066904244,-0.03603856,0.04852917,-0.00236953,0.010736449,-0.008823226,0.048167307,-0.06029884,-0.02008656,0.04934105,-0.02165272,-0.0059321085,0.06101121,0.0050238813,0.036402144,-0.02407015,0.023990652,-0.017778596,0.0436243,-0.015234602,0.0013730796,0.032770347,0.04894707,-0.019903826,0.027210288,0.007320456,0.0031876478,-0.055667188,-0.0072730477,0.028816042,-0.036349706,-0.00014394987,0.041932836,0.011812606,-0.28528613,0.04374173,-0.02590061,0.036650803,-0.0054236376,0.044467915,-0.03377718,0.026663015,-0.057349887,-0.009933792,0.05871865,0.066987924,0.01848101,0.035349075,-0.017123345,0.09152398,0.03281407,-0.027747098,0.024043055,-0.07519967,-0.008642383,0.012063249,0.1965554,-0.04898195,0.068870105,0.023952631,-0.019946015,0.004611634,-0.011829971,0.004210717,0.007021141,-0.002664805,0.09861566,-0.07022641,0.02679062,0.08935795,-0.030601857,0.049988363,-0.00043722984,-0.07530143,-0.02624388,0.0027576822,-0.00384283,-0.0011950951,0.09351181,0.000937468,-0.04877977,-0.008898081,-0.0072512534,0.035405826,-0.037256964,0.017189763,-0.026388798,0.03487121,0.0046409904,-0.004795937,-0.012733615,-0.023830233,-0.0358013,-0.043804348,-0.029517788,-0.046758275,-0.06881492,0.024177445,0.0143103]'),
      ('Tomato', '[-0.029869203,0.015828134,-0.024596874,0.020861035,0.03365765,0.005016581,0.060061164,0.055231918,0.006352899,0.004938967,-0.010154029,-0.079970695,0.07321542,0.07790107,0.033254772,-0.004294707,0.01585883,0.059918594,-0.037038,0.0012428203,0.051965587,-0.020879416,-0.016188672,-0.054822158,0.05654682,0.01551287,-0.038257234,-0.0013387762,-0.06285542,-0.15906854,-0.017457722,-0.031295694,0.039125703,-0.033283714,0.010947399,-0.040234644,-0.034372844,0.0104622785,-0.025485123,0.04994066,0.036814038,0.017452875,-0.023161633,-0.036298722,-0.07051053,-0.01720321,-0.050520383,-0.008748737,0.045904037,-0.034201797,0.00838936,-0.02913385,-0.026715593,-0.0019239038,0.030497734,0.05315972,0.0737804,-0.012085407,0.008660171,0.042524733,0.05538341,0.000746007,-0.20466295,0.06342115,0.06135094,0.031504415,-0.069241084,-0.031363465,0.02054631,0.06331636,-0.023490809,0.03283903,0.016335465,0.07762305,0.02926262,-0.031864528,0.007902017,-0.051381256,-3.054966e-05,0.0069825146,-0.0072099287,-0.057942085,-0.021548394,-0.020484764,0.0034414476,-0.0009200981,-0.025876729,-0.0011384323,0.030854544,0.013149443,-0.06754705,0.025895027,0.013232211,0.0026189121,-0.082489476,-0.016675644,-0.0011329504,-0.031126892,-0.07112445,0.24384846,-0.046597656,0.0059918556,0.05970376,-0.018256843,-0.006963375,-0.013897323,0.0015030024,-0.020435717,0.019298133,0.052937556,-0.017333444,-0.018564422,0.009297942,-0.054324653,0.03786996,-0.032721665,0.06951459,0.01771603,-0.028982867,0.0077923154,-0.00830094,0.029445156,0.029822096,-0.012998489,0.03462648,-0.060357843,-0.010533851,0.1293935,0.034346506,0.050578557,0.09663329,-0.01276127,-0.033567615,-0.0064450526,0.0058356086,0.0059343786,0.01880375,0.02080349,0.011490754,-0.051762104,-0.058268107,-0.10148158,-0.028299917,-0.12878123,-0.056659013,0.0792677,-0.06189865,0.07475942,-0.015977787,0.033134293,-0.028377857,0.070312105,-0.033728436,0.0008066307,0.0011054198,0.036089025,0.005808116,0.0150892725,0.015652863,0.023343807,-0.067047544,-0.021238264,-0.033493955,0.049286805,0.061130386,-0.1302799,-0.034686644,0.03194993,0.017306728,-0.022859355,0.07234756,0.06707331,-0.038075157,-0.036348555,0.029766655,0.025726654,-0.008922685,0.023629867,-0.0069144396,-0.0069140885,0.056218613,0.0060891095,-0.018740704,0.039786205,0.044703834,-0.05122165,-0.030560166,-0.019477822,0.025642106,0.047365725,-0.0292419,0.020383202,-0.018561209,0.0019795808,-0.07156357,-0.07546085,-0.033361394,0.014745907,0.043143313,-0.08493165,0.040821496,8.2503364e-05,-0.037970517,0.07551236,0.018706819,-0.008236447,0.012222742,-0.0072385306,0.03870738,0.009308826,-0.053195536,0.005780943,0.061434455,0.022424093,-0.095846556,-0.027069451,0.047961123,0.05197209,0.057374045,0.035580248,0.029196136,-0.10977645,-0.057589736,-0.1870228,0.043391757,0.035963323,-0.014210672,0.05607454,0.0082466435,0.019204104,-0.05721462,0.040878456,0.022721449,0.024591066,0.020865226,0.0049236715,0.015667483,-0.021398226,0.037637185,0.06469617,0.012518049,0.017760469,0.0019964725,-0.020859785,-0.040002,-0.033094212,-0.0699783,0.023698682,-0.07017367,0.24296242,0.080658525,0.009871246,-0.040088497,-0.0053918683,0.04648372,-0.02816283,-0.110829785,0.029916579,0.030329235,0.024077553,-0.049767897,-0.04330244,-0.023295995,-0.004359568,-0.03932452,-0.02428935,-0.09337115,0.034868468,-0.002945901,-0.020691164,0.018511932,0.014854298,0.04256989,0.023097772,0.001148896,0.057300646,0.013074834,-0.012571415,0.020001471,-0.10127031,0.032281198,-0.04176389,0.016812032,-0.019411203,-0.05776911,0.029127091,-0.067681596,0.067657456,0.0037067574,0.0054313703,-0.06507483,0.02452655,-0.023523247,-0.0115845855,0.07426925,0.0059831166,-0.032276228,0.041309595,0.029319493,0.031122738,-0.02010134,-0.03379396,-0.068257384,0.07548506,-0.00865319,0.020672757,0.018169515,0.02121715,-0.010468761,0.030261477,-0.039439406,0.03985935,-0.03133109,0.0016366037,-0.0125524895,-0.041070357,-0.011944829,0.04043416,-0.06700538,-0.28739145,0.05875665,0.009824386,0.023446443,-0.025512964,-0.0033332668,-0.002571084,0.041617453,-0.09641374,-0.0148532195,0.029931597,0.00049597985,0.056677457,0.006903832,-0.0062901466,0.05687495,0.03539329,0.0054870755,0.029720528,-0.02696151,0.050539803,0.033945065,0.21645951,-0.046155605,-0.00011607572,0.037222028,-0.00097053614,0.010392147,0.016402123,0.013145822,0.014141131,0.051504795,0.080597855,-0.043332655,0.010268469,0.023539314,-0.028266096,0.060939495,-0.007246181,-0.016179414,-0.07213172,-0.0051829983,-0.047173,-0.043231044,0.07690072,-0.035430543,-0.018886589,-0.071139365,0.00027390203,0.024421537,-0.05540805,0.0030416052,-0.03133615,0.020766012,0.007597287,-0.024713233,-0.018646253,-0.02164263,-0.04884827,-0.011035427,0.0315883,-0.045343455,-0.0052071656,0.046982065,0.03005724]'),
      ('Desk', '[-0.017075958,-0.010888746,0.080291696,-0.025692422,0.023058051,0.011809025,0.052361254,0.06862456,0.023607824,-0.009210255,0.04033431,-0.053230878,0.08069261,0.046486624,0.009520946,0.0037151624,-0.027851786,0.022258898,-0.03552146,0.0049876287,0.016760446,-0.0354698,-0.041900143,-0.08992244,-0.0030455913,-0.0043795966,-0.062581964,-0.06256854,-0.04892184,-0.14111088,0.0026656375,-0.031744845,0.05466585,-0.018847726,0.019207526,-0.06257882,-0.007204764,0.02333366,-0.029454706,0.027162952,-0.014566991,-0.0097501725,-0.05109416,-0.049618334,-0.009138151,-0.049895912,-0.002367935,-0.045575418,0.04331254,0.0060089612,0.010464706,-0.06699024,0.011478289,0.04059331,0.0084988475,0.040767603,0.046229202,0.0055651255,0.034506306,0.071188055,0.050099786,0.0064023295,-0.22729196,0.11016682,0.063853145,0.002075671,-0.058538996,-0.04469632,0.025434878,0.054608773,-0.03046769,-0.0023969284,0.023868104,0.04704296,0.05093148,-0.07893697,-0.016868979,-0.023819225,0.04966335,0.012613918,0.0002760624,-0.0237792,-0.02210674,-0.0025413414,-0.051818397,-0.010000192,-0.0056564524,-0.024465935,0.034022514,-0.0039295424,-0.08148384,0.044355,-0.059270795,0.006314524,-0.07335388,-0.05289436,0.029089374,0.003093686,-0.053854994,0.24911241,-0.062527195,0.03473907,0.056096543,-0.034448277,0.008506664,-0.062420454,0.012665839,-0.008595196,-0.064310096,0.02994306,-0.034977753,0.016273487,0.0093974555,-0.00015720105,0.038714644,0.0036081353,0.053009037,-0.0008864092,-0.013851306,-0.039496463,0.014128448,0.017951578,0.009349462,0.01042504,0.051630985,-0.053956885,0.02876148,0.11510076,0.0455511,0.012667353,0.0806707,0.0070800376,0.025727438,0.041150693,0.007863412,0.031984337,0.013170205,-0.009395292,0.014573138,-0.03485805,-0.03764348,-0.07997719,-0.011784456,-0.09563226,-0.03400148,0.109441295,-0.04900478,0.02535317,-0.04422044,0.0062985034,-0.0076054796,0.03937887,0.030961435,0.017655462,-0.012246824,0.00020564783,0.021475356,0.049180325,-0.012257939,-0.02592291,-0.0014702714,-0.015490445,-0.05669335,0.05330947,0.010385874,-0.102429666,-0.030609999,0.0714415,0.011635493,0.0022937357,0.009192694,0.01323513,-0.046253715,0.042666234,0.09617933,0.038035154,-0.0074128667,-0.032752965,0.0076832944,0.02399374,0.06469999,-0.033205263,-0.060129013,0.04173046,0.06158138,-0.03689033,0.004568745,-0.023324467,0.031645365,0.07746148,0.0059857927,0.037316162,0.0029132718,-0.016107306,-0.05434655,-0.06491648,-0.023832045,-0.050689403,-0.010082951,-0.05112477,0.09912774,-0.025389507,-0.051846053,0.05495424,0.033441294,0.0037791673,0.009748631,-0.0019748393,0.04403737,0.009415996,-0.060215756,0.018110497,0.058251932,-0.006824225,-0.009257916,-0.041744713,0.015736148,0.03218059,0.060505457,0.012099181,0.034636773,-0.052649904,-0.079120345,-0.20813292,0.066237904,-0.020957587,0.0073129637,0.034559604,0.011299093,0.03411265,-0.0021747819,0.03185566,0.0070963744,0.07857288,-0.008124623,-0.017663067,0.0016398929,-0.039167996,0.061877865,0.06713014,0.018810991,-0.026776476,-0.046341814,-0.005355074,0.028902428,-0.06398502,-0.029270144,0.02508191,-0.02530654,0.21773459,0.0339529,0.015731804,-0.03964237,0.07042959,-0.02501626,-0.022183988,-0.14884305,0.065215595,0.10670705,-0.00037880713,-0.06864392,-0.028297037,-0.024357636,-0.050164282,0.01834488,-0.031103658,-0.084248856,-0.028994896,-0.015948653,-0.03755552,-0.015801996,-0.033609822,0.02573379,0.02479709,-0.03685521,-0.0046613724,0.034090023,-0.010260922,0.015709793,-0.062157243,0.01375377,-0.04164558,-0.004064993,-0.0070441877,-0.05030921,0.0043502874,-0.01210114,0.052284397,0.026279854,-0.008008602,0.012525424,0.04647556,-0.039444804,-0.0002497322,0.043502543,0.022106966,0.009966737,0.061893284,0.010174843,0.034406714,0.016847465,-0.04216487,-0.028892005,0.009899105,-0.025332458,0.027274324,0.037005823,0.017847693,0.015657736,0.06033117,-0.030242763,0.060493562,-0.06003715,-0.008263403,-0.010291895,-0.034526054,-0.052242614,0.028519461,0.00037531398,-0.29202363,0.051917017,-0.0124211,0.048688818,-0.03532362,-0.015612858,-0.053210825,0.06197344,-0.07399723,0.013094931,0.0069343266,0.016719852,-0.022996183,0.015213123,-0.0123770265,0.04051931,0.06230749,-0.00079691666,0.028561795,-0.04758113,0.03337342,0.050868753,0.2105322,-0.0056460854,0.047387015,0.018683372,0.015859153,-8.0802485e-05,0.017905135,0.01635564,0.020528011,-0.007790339,0.051299676,-0.04791672,0.030732168,0.055217195,-0.034426298,0.06178515,0.0012667197,-0.03904972,-0.049328256,0.048351344,-0.0860337,-0.015718967,0.09275064,-0.025560671,-0.019069614,-0.08066873,0.04960441,-0.023049746,-0.056898046,0.0032385543,-0.051040743,0.009549142,0.020795956,-0.018794646,0.015302576,-0.048582405,-0.013162642,-0.05900879,-0.00052222173,-0.053530484,0.013752464,0.015265108,0.039229322]'),
      ('Banana', '[0.019635703,-0.016662825,0.0438706,-0.05679909,0.065090984,0.021733282,0.06932404,0.05716613,-0.028375698,0.033598874,0.03070026,-0.12938243,0.047556713,0.009217823,0.043147042,0.022466118,-0.004610096,0.018666554,-0.0574933,6.868109e-05,0.05209691,-0.014084585,-0.012451078,-0.035549372,0.04206527,0.037208725,-0.050204195,-0.035145547,-0.010901805,-0.15682638,-0.005470431,-0.021524418,0.015910674,-0.04949034,0.0051853815,0.014806365,-0.0056085135,0.03450337,-0.056007136,0.023609918,0.07185804,-0.014022289,-0.016194828,-0.108324386,-0.044947192,-0.046223115,-0.02189368,-0.019143095,0.06842713,-0.009386807,0.0113680465,-0.043105036,-0.017536012,0.03985226,-0.026393633,0.038680777,0.10853933,0.020019624,-0.0026814984,0.023394262,0.075863995,0.019038668,-0.19950384,0.055614036,0.028766343,0.010403027,-0.04593247,-0.015267849,0.010628882,0.066665724,-0.014128945,0.03466571,0.025084144,0.065656714,0.017702313,-0.032993343,0.05914002,-0.105294295,-0.005326594,0.04990947,-0.039132904,-0.02830107,-0.08071914,-0.028800204,-0.020271,-0.049152274,0.0043607694,-0.047078244,0.05794935,0.01023668,-0.025150688,0.00729757,0.05313274,0.007297015,-0.093589135,-0.014347521,0.031550843,-0.018207908,-0.033484485,0.23968731,-0.04008758,0.0060827625,-0.004287125,-0.04060174,0.035998467,-0.013882029,-0.015104646,-0.04700737,0.015394095,0.02893912,0.031648815,-0.002771979,0.03083158,-0.05707041,0.010538449,-0.053470287,0.06256131,0.011704445,-0.014541999,-0.01051473,-0.006733189,0.044093613,0.013339935,-0.06267326,0.0428632,-0.07038315,-0.009143276,0.09994883,0.004987607,0.026885895,0.08034262,-0.033635627,-0.022487551,-0.030024724,0.026934387,0.029255223,0.027208928,0.0069896323,-0.046474732,-0.058649674,-0.07008608,-0.06753458,-0.034869235,-0.120450824,-0.010866406,0.08480637,-0.05114118,0.06559932,-0.031219408,0.032882344,-0.017922925,0.033752162,-0.03199418,0.01667962,0.037241977,0.022630231,0.045894135,0.030753318,-0.02215518,0.017770028,-0.025049254,-0.045379672,-0.06098087,0.063464515,0.020640565,-0.10424734,-0.00786076,0.028407086,0.012444685,-0.010966217,0.025792638,0.031150766,-0.09807386,0.009220468,0.07690304,0.031107485,-0.02323454,0.01232304,0.027339907,-0.009314799,0.025466112,-0.0526805,0.011169639,0.026503928,0.053522177,-0.026159566,-0.02141847,0.0077685686,0.068012245,0.043427285,0.025749903,0.015305993,-0.05753994,0.036382552,-0.037798975,-0.06504522,-0.025704943,-0.034619145,0.009529093,-0.039157882,0.05972188,-0.029291982,-0.054928947,0.0015608502,0.019036693,0.0087293945,-0.0043398645,0.017392809,0.02264325,0.025433283,-0.04307134,-0.0022087728,0.07539596,-0.031026198,-0.08422478,0.015684372,-0.007135754,0.07095444,0.05106952,0.021192841,0.022940988,-0.09845196,-0.054315202,-0.2283184,0.00039206562,0.020285372,-0.0032002586,0.09146159,0.020653537,0.027857816,-0.03427893,0.042690206,-0.013986325,0.04072902,0.016655613,-0.015795486,0.09504197,-0.022981595,0.030391654,0.059060454,-0.0011297392,0.0129476525,0.024110911,0.0024470065,0.017639639,-0.0011270575,-0.06595389,0.016953548,-0.013562799,0.23597829,0.10829628,0.00782811,-0.038939714,-0.0005555627,-0.002915367,-0.015536974,-0.10943432,0.026033664,0.020181289,0.0003686194,-0.04886056,-0.02366949,-0.0001586763,-0.03300311,0.040830493,-0.024658168,-0.07623268,0.0075789504,-0.024747914,-0.011904173,0.00024992827,-0.035677873,0.0060291006,0.029552713,-0.020987893,-0.01081308,-0.00033254165,0.01623525,-0.035739012,-0.08761928,0.0008644093,-0.0059358636,0.017304605,0.006209267,-0.050949544,0.032538358,-0.032607634,0.01767997,0.016854625,0.0251009,-0.056566466,0.029434936,-0.027132805,-0.0037773196,0.08622786,-0.024845831,-0.0149036255,0.0677935,0.035698667,0.053244714,-0.023172028,-0.057150465,-0.042664234,0.016723031,-0.0110892,0.027086386,0.040617514,0.021721961,-0.008825345,0.09014068,-0.00031725666,0.037772376,-0.028894745,0.0071449205,-0.00014647323,-0.020653443,-0.054506272,0.037730232,-0.02952679,-0.27268472,0.06581913,-0.025963487,0.046704344,-0.023291914,-0.001648131,0.064198926,0.03256141,-0.07444727,0.03312911,0.029869812,0.0011079342,0.02101518,-0.018955339,0.018415285,0.04355975,0.02111472,-0.060182165,0.05878127,-0.028283918,0.029465748,0.003601499,0.2091422,-0.05559134,-0.00097313384,0.0067268955,-0.013934751,0.028899418,0.0306436,-0.018715024,0.027255507,0.034688573,0.111619085,-0.0074516083,-0.0008301801,0.052996,-0.018112244,0.0149507355,0.030463047,-0.018259967,-0.038920976,0.0023088527,-0.044930156,-0.04536979,0.07041295,-0.025604915,0.008208011,-0.07095862,0.019948348,-0.008449301,-0.016477905,-0.046026208,-0.0107074315,0.0250523,0.05894703,-0.050161943,-0.026808146,-0.038142174,-0.027797315,-0.069310926,0.022376642,-0.04382796,-0.019629879,0.019454641,0.026503637]');
  `);
};


----- .\phi-3.5-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import Chat from "./components/Chat";
import ArrowRightIcon from "./components/icons/ArrowRightIcon";
import StopIcon from "./components/icons/StopIcon";
import Progress from "./components/Progress";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;
const STICKY_SCROLL_THRESHOLD = 120;
const EXAMPLES = [
  "Give me some tips to improve my time management skills.",
  "What is the difference between AI and ML?",
  "Write python code to compute the nth fibonacci number.",
];

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const textareaRef = useRef(null);
  const chatContainerRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);
  const [isRunning, setIsRunning] = useState(false);

  // Inputs and outputs
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState([]);
  const [tps, setTps] = useState(null);
  const [numTokens, setNumTokens] = useState(null);

  function onEnter(message) {
    setMessages((prev) => [...prev, { role: "user", content: message }]);
    setTps(null);
    setIsRunning(true);
    setInput("");
  }

  function onInterrupt() {
    // NOTE: We do not set isRunning to false here because the worker
    // will send a 'complete' message when it is done.
    worker.current.postMessage({ type: "interrupt" });
  }

  useEffect(() => {
    resizeInput();
  }, [input]);

  function resizeInput() {
    if (!textareaRef.current) return;

    const target = textareaRef.current;
    target.style.height = "auto";
    const newHeight = Math.min(Math.max(target.scrollHeight, 24), 200);
    target.style.height = `${newHeight}px`;
  }

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    if (!worker.current) {
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
      worker.current.postMessage({ type: "check" }); // Do a feature check
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "start":
          {
            // Start generation
            setMessages((prev) => [
              ...prev,
              { role: "assistant", content: "" },
            ]);
          }
          break;

        case "update":
          {
            // Generation update: update the output text.
            // Parse messages
            const { output, tps, numTokens } = e.data;
            setTps(tps);
            setNumTokens(numTokens);
            setMessages((prev) => {
              const cloned = [...prev];
              const last = cloned.at(-1);
              cloned[cloned.length - 1] = {
                ...last,
                content: last.content + output,
              };
              return cloned;
            });
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsRunning(false);
          break;

        case "error":
          setError(e.data.data);
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
    };
  }, []);

  // Send the messages to the worker thread whenever the `messages` state changes.
  useEffect(() => {
    if (messages.filter((x) => x.role === "user").length === 0) {
      // No user messages yet: do nothing.
      return;
    }
    if (messages.at(-1).role === "assistant") {
      // Do not update if the last message is from the assistant
      return;
    }
    setTps(null);
    worker.current.postMessage({ type: "generate", data: messages });
  }, [messages, isRunning]);

  useEffect(() => {
    if (!chatContainerRef.current || !isRunning) return;
    const element = chatContainerRef.current;
    if (
      element.scrollHeight - element.scrollTop - element.clientHeight <
      STICKY_SCROLL_THRESHOLD
    ) {
      element.scrollTop = element.scrollHeight;
    }
  }, [messages, isRunning]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto items justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {status === null && messages.length === 0 && (
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[300px] text-center">
            <img
              src="logo.png"
              width="85%"
              height="auto"
              className="block"
            ></img>
            <h1 className="text-4xl font-bold mb-1">Phi-3.5 WebGPU</h1>
            <h2 className="font-semibold">
              A private and powerful AI chatbot
              <br />
              that runs locally in your browser.
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            <p className="max-w-[514px] mb-4">
              <br />
              You are about to load{" "}
              <a
                href="onnx-community/Phi-3.5-mini-instruct-onnx-web"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                Phi-3.5-mini-instruct
              </a>
              , a 3.82 billion parameter LLM that is optimized for inference on
              the web. Once downloaded, the model (2.3&nbsp;GB) will be cached
              and reused when you revisit the page.
              <br />
              <br />
              Everything runs directly in your browser using{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning your conversations aren&#39;t sent
              to a server. You can even disconnect from the internet after the
              model has loaded!
              <br />
              Want to learn more? Check out the demo's source code on{" "}
              <a
                href="https://github.com/huggingface/transformers.js-examples/tree/main/phi-3.5-webgpu"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                GitHub
              </a>
              !
            </p>

            {error && (
              <div className="text-red-500 text-center mb-2">
                <p className="mb-1">
                  Unable to load model due to the following error:
                </p>
                <p className="text-sm">{error}</p>
              </div>
            )}

            <button
              className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 disabled:cursor-not-allowed select-none"
              onClick={() => {
                worker.current.postMessage({ type: "load" });
                setStatus("loading");
              }}
              disabled={status !== null || error !== null}
            >
              Load model
            </button>
          </div>
        </div>
      )}
      {status === "loading" && (
        <>
          <div className="w-full max-w-[500px] text-left mx-auto p-4 bottom-0 mt-auto">
            <p className="text-center mb-1">{loadingMessage}</p>
            {progressItems.map(({ file, progress, total }, i) => (
              <Progress
                key={i}
                text={file}
                percentage={progress}
                total={total}
              />
            ))}
          </div>
        </>
      )}

      {status === "ready" && (
        <div
          ref={chatContainerRef}
          className="overflow-y-auto scrollbar-thin w-full flex flex-col items-center h-full"
        >
          <Chat messages={messages} />
          {messages.length === 0 && (
            <div>
              {EXAMPLES.map((msg, i) => (
                <div
                  key={i}
                  className="m-1 border dark:border-gray-600 rounded-md p-2 bg-gray-100 dark:bg-gray-700 cursor-pointer"
                  onClick={() => onEnter(msg)}
                >
                  {msg}
                </div>
              ))}
            </div>
          )}
          <p className="text-center text-sm min-h-6 text-gray-500 dark:text-gray-300">
            {tps && messages.length > 0 && (
              <>
                {!isRunning && (
                  <span>
                    Generated {numTokens} tokens in{" "}
                    {(numTokens / tps).toFixed(2)} seconds&nbsp;&#40;
                  </span>
                )}
                {
                  <>
                    <span className="font-medium text-center mr-1 text-black dark:text-white">
                      {tps.toFixed(2)}
                    </span>
                    <span className="text-gray-500 dark:text-gray-300">
                      tokens/second
                    </span>
                  </>
                }
                {!isRunning && (
                  <>
                    <span className="mr-1">&#41;.</span>
                    <span
                      className="underline cursor-pointer"
                      onClick={() => {
                        worker.current.postMessage({ type: "reset" });
                        setMessages([]);
                      }}
                    >
                      Reset
                    </span>
                  </>
                )}
              </>
            )}
          </p>
        </div>
      )}

      <div className="mt-2 border dark:bg-gray-700 rounded-lg w-[600px] max-w-[80%] max-h-[200px] mx-auto relative mb-3 flex">
        <textarea
          ref={textareaRef}
          className="scrollbar-thin w-[550px] dark:bg-gray-700 px-3 py-4 rounded-lg bg-transparent border-none outline-none text-gray-800 disabled:text-gray-400 dark:text-gray-200 placeholder-gray-500 dark:placeholder-gray-400 disabled:placeholder-gray-200 resize-none disabled:cursor-not-allowed"
          placeholder="Type your message..."
          type="text"
          rows={1}
          value={input}
          disabled={status !== "ready"}
          title={status === "ready" ? "Model is ready" : "Model not loaded yet"}
          onKeyDown={(e) => {
            if (
              input.length > 0 &&
              !isRunning &&
              e.key === "Enter" &&
              !e.shiftKey
            ) {
              e.preventDefault(); // Prevent default behavior of Enter key
              onEnter(input);
            }
          }}
          onInput={(e) => setInput(e.target.value)}
        />
        {isRunning ? (
          <div className="cursor-pointer" onClick={onInterrupt}>
            <StopIcon className="h-8 w-8 p-1 rounded-md text-gray-800 dark:text-gray-100 absolute right-3 bottom-3" />
          </div>
        ) : input.length > 0 ? (
          <div className="cursor-pointer" onClick={() => onEnter(input)}>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-800 dark:bg-gray-100 text-white dark:text-black rounded-md absolute right-3 bottom-3`}
            />
          </div>
        ) : (
          <div>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-200 dark:bg-gray-600 text-gray-50 dark:text-gray-800 rounded-md absolute right-3 bottom-3`}
            />
          </div>
        )}
      </div>

      <p className="text-xs text-gray-400 text-center mb-3">
        Disclaimer: Generated content may be inaccurate or false.
      </p>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\phi-3.5-webgpu\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;

@layer utilities {
  .scrollbar-thin::-webkit-scrollbar {
    @apply w-2;
  }

  .scrollbar-thin::-webkit-scrollbar-track {
    @apply rounded-full bg-gray-100 dark:bg-gray-700;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb {
    @apply rounded-full bg-gray-300 dark:bg-gray-600;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb:hover {
    @apply bg-gray-500;
  }

  .animation-delay-200 {
    animation-delay: 200ms;
  }
  .animation-delay-400 {
    animation-delay: 400ms;
  }

  .overflow-wrap-anywhere {
    overflow-wrap: anywhere;
  }
}


----- .\phi-3.5-webgpu\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\phi-3.5-webgpu\src\worker.js -----

import {
  AutoTokenizer,
  AutoModelForCausalLM,
  TextStreamer,
  InterruptableStoppingCriteria,
} from "@huggingface/transformers";

/**
 * This class uses the Singleton pattern to enable lazy-loading of the pipeline
 */
class TextGenerationPipeline {
  static model_id = "onnx-community/Phi-3.5-mini-instruct-onnx-web";

  static async getInstance(progress_callback = null) {
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= AutoModelForCausalLM.from_pretrained(this.model_id, {
      // dtype: "q4",
      dtype: "q4f16",
      device: "webgpu",
      use_external_data_format: true,
      progress_callback,
    });

    return Promise.all([this.tokenizer, this.model]);
  }
}

const stopping_criteria = new InterruptableStoppingCriteria();

let past_key_values_cache = null;
async function generate(messages) {
  // Retrieve the text-generation pipeline.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance();

  const inputs = tokenizer.apply_chat_template(messages, {
    add_generation_prompt: true,
    return_dict: true,
  });

  let startTime;
  let numTokens = 0;
  let tps;
  const token_callback_function = () => {
    startTime ??= performance.now();

    if (numTokens++ > 0) {
      tps = (numTokens / (performance.now() - startTime)) * 1000;
    }
  };
  const callback_function = (output) => {
    self.postMessage({
      status: "update",
      output,
      tps,
      numTokens,
    });
  };

  const streamer = new TextStreamer(tokenizer, {
    skip_prompt: true,
    skip_special_tokens: true,
    callback_function,
    token_callback_function,
  });

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  const { past_key_values, sequences } = await model.generate({
    ...inputs,
    // TODO: Enable once model is fixed
    // past_key_values: past_key_values_cache,

    // Sampling
    do_sample: true,
    top_k: 3,
    temperature: 0.2,

    max_new_tokens: 1024,
    streamer,
    stopping_criteria,
    return_dict_in_generate: true,
  });
  past_key_values_cache = past_key_values;

  const decoded = tokenizer.batch_decode(sequences, {
    skip_special_tokens: true,
  });

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: decoded,
  });
}

async function check() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) {
      throw new Error("WebGPU is not supported (no adapter found)");
    }
  } catch (e) {
    self.postMessage({
      status: "error",
      data: e.toString(),
    });
  }
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  self.postMessage({
    status: "loading",
    data: "Compiling shaders and warming up model...",
  });

  // Run model with dummy input to compile shaders
  const inputs = tokenizer("a");
  await model.generate({ ...inputs, max_new_tokens: 1 });
  self.postMessage({ status: "ready" });
}
// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "check":
      check();
      break;

    case "load":
      load();
      break;

    case "generate":
      stopping_criteria.reset();
      generate(data);
      break;

    case "interrupt":
      stopping_criteria.interrupt();
      break;

    case "reset":
      past_key_values_cache = null;
      stopping_criteria.reset();
      break;
  }
});


----- .\phi-3.5-webgpu\src\components\Chat.css -----

@scope (.markdown) {
  /* Code blocks */
  pre {
    margin: 0.5rem 0;
    white-space: break-spaces;
  }

  code {
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
    font-size: 0.9em;
  }

  pre,
  code {
    background-color: #f2f2f2;
  }

  @media (prefers-color-scheme: dark) {
    pre,
    code {
      background-color: #333;
    }
  }

  pre:has(code) {
    padding: 1rem 0.5rem;
  }

  pre > code {
    padding: 0;
  }

  /* Headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-weight: 600;
    line-height: 1.2;
  }

  h1 {
    font-size: 2em;
    margin: 1rem 0;
  }

  h2 {
    font-size: 1.5em;
    margin: 0.83rem 0;
  }

  h3 {
    font-size: 1.25em;
    margin: 0.67rem 0;
  }

  h4 {
    font-size: 1em;
    margin: 0.5rem 0;
  }

  h5 {
    font-size: 0.875em;
    margin: 0.33rem 0;
  }

  h6 {
    font-size: 0.75em;
    margin: 0.25rem 0;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6:first-child {
    margin-top: 0;
  }

  /* Unordered List */
  ul {
    list-style-type: disc;
    margin-left: 1.5rem;
  }

  /* Ordered List */
  ol {
    list-style-type: decimal;
    margin-left: 1.5rem;
  }

  /* List Items */
  li {
    margin: 0.25rem 0;
  }

  p:not(:first-child) {
    margin-top: 0.75rem;
  }

  p:not(:last-child) {
    margin-bottom: 0.75rem;
  }

  ul > li {
    margin-left: 1rem;
  }

  /* Table */
  table,
  th,
  td {
    border: 1px solid lightgray;
    padding: 0.25rem;
  }

  @media (prefers-color-scheme: dark) {
    table,
    th,
    td {
      border: 1px solid #f2f2f2;
    }
  }
}


----- .\phi-3.5-webgpu\src\components\Chat.jsx -----

import { marked } from "marked";
import DOMPurify from "dompurify";

import BotIcon from "./icons/BotIcon";
import UserIcon from "./icons/UserIcon";

import "./Chat.css";
import { useEffect } from "react";

function render(text) {
  return DOMPurify.sanitize(marked.parse(text));
}

export default function Chat({ messages }) {
  const empty = messages.length === 0;

  useEffect(() => {
    window.MathJax.typeset();
  }, [messages]);

  return (
    <div
      className={`flex-1 p-6 max-w-[960px] w-full ${empty ? "flex flex-col items-center justify-end" : "space-y-4"}`}
    >
      {empty ? (
        <div className="text-xl">Ready!</div>
      ) : (
        messages.map((msg, i) => (
          <div key={`message-${i}`} className="flex items-start space-x-4">
            {msg.role === "assistant" ? (
              <>
                <BotIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-gray-200 dark:bg-gray-700 rounded-lg p-4">
                  <p className="min-h-6 text-gray-800 dark:text-gray-200 overflow-wrap-anywhere">
                    {msg.content.length > 0 ? (
                      <span
                        className="markdown"
                        dangerouslySetInnerHTML={{
                          __html: render(msg.content),
                        }}
                      />
                    ) : (
                      <span className="h-6 flex items-center gap-1">
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-200"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-400"></span>
                      </span>
                    )}
                  </p>
                </div>
              </>
            ) : (
              <>
                <UserIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-blue-500 text-white rounded-lg p-4">
                  <p className="min-h-6 overflow-wrap-anywhere">
                    {msg.content}
                  </p>
                </div>
              </>
            )}
          </div>
        ))
      )}
    </div>
  );
}


----- .\phi-3.5-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\phi-3.5-webgpu\src\components\icons\ArrowRightIcon.jsx -----

export default function ArrowRightIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M5 12h14" />
      <path d="m12 5 7 7-7 7" />
    </svg>
  );
}


----- .\phi-3.5-webgpu\src\components\icons\BotIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M12 8V4H8" />
      <rect width="16" height="12" x="4" y="8" rx="2" />
      <path d="M2 14h2" />
      <path d="M20 14h2" />
      <path d="M15 13v2" />
      <path d="M9 13v2" />
    </svg>
  );
}


----- .\phi-3.5-webgpu\src\components\icons\StopIcon.jsx -----

export default function StopIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
      <path
        fill="currentColor"
        d="M9 9.563C9 9.252 9.252 9 9.563 9h4.874c.311 0 .563.252.563.563v4.874c0 .311-.252.563-.563.563H9.564A.562.562 0 0 1 9 14.437V9.564Z"
      />
    </svg>
  );
}


----- .\phi-3.5-webgpu\src\components\icons\UserIcon.jsx -----

export default function UserIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2" />
      <circle cx="12" cy="7" r="4" />
    </svg>
  );
}


----- .\realtime-whisper-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import { AudioVisualizer } from "./components/AudioVisualizer";
import Progress from "./components/Progress";
import { LanguageSelector } from "./components/LanguageSelector";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;

const WHISPER_SAMPLING_RATE = 16_000;
const MAX_AUDIO_LENGTH = 30; // seconds
const MAX_SAMPLES = WHISPER_SAMPLING_RATE * MAX_AUDIO_LENGTH;

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const recorderRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);

  // Inputs and outputs
  const [text, setText] = useState("");
  const [tps, setTps] = useState(null);
  const [language, setLanguage] = useState("en");

  // Processing
  const [recording, setRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [chunks, setChunks] = useState([]);
  const [stream, setStream] = useState(null);
  const audioContextRef = useRef(null);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    if (!worker.current) {
      // Create the worker if it does not yet exist.
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          recorderRef.current?.start();
          break;

        case "start":
          {
            // Start generation
            setIsProcessing(true);

            // Request new data from the recorder
            recorderRef.current?.requestData();
          }
          break;

        case "update":
          {
            // Generation update: update the output text.
            const { tps } = e.data;
            setTps(tps);
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsProcessing(false);
          setText(e.data.output);
          break;
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
    };
  }, []);

  useEffect(() => {
    if (recorderRef.current) return; // Already set

    if (navigator.mediaDevices.getUserMedia) {
      navigator.mediaDevices
        .getUserMedia({ audio: true })
        .then((stream) => {
          setStream(stream);

          recorderRef.current = new MediaRecorder(stream);
          audioContextRef.current = new AudioContext({
            sampleRate: WHISPER_SAMPLING_RATE,
          });

          recorderRef.current.onstart = () => {
            setRecording(true);
            setChunks([]);
          };
          recorderRef.current.ondataavailable = (e) => {
            if (e.data.size > 0) {
              setChunks((prev) => [...prev, e.data]);
            } else {
              // Empty chunk received, so we request new data after a short timeout
              setTimeout(() => {
                recorderRef.current.requestData();
              }, 25);
            }
          };

          recorderRef.current.onstop = () => {
            setRecording(false);
          };
        })
        .catch((err) => console.error("The following error occurred: ", err));
    } else {
      console.error("getUserMedia not supported on your browser!");
    }

    return () => {
      recorderRef.current?.stop();
      recorderRef.current = null;
    };
  }, []);

  useEffect(() => {
    if (!recorderRef.current) return;
    if (!recording) return;
    if (isProcessing) return;
    if (status !== "ready") return;

    if (chunks.length > 0) {
      // Generate from data
      const blob = new Blob(chunks, { type: recorderRef.current.mimeType });

      const fileReader = new FileReader();

      fileReader.onloadend = async () => {
        const arrayBuffer = fileReader.result;
        const decoded =
          await audioContextRef.current.decodeAudioData(arrayBuffer);
        let audio = decoded.getChannelData(0);
        if (audio.length > MAX_SAMPLES) {
          // Get last MAX_SAMPLES
          audio = audio.slice(-MAX_SAMPLES);
        }

        worker.current.postMessage({
          type: "generate",
          data: { audio, language },
        });
      };
      fileReader.readAsArrayBuffer(blob);
    } else {
      recorderRef.current?.requestData();
    }
  }, [status, recording, isProcessing, chunks, language]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[400px] text-center">
            <img
              src="logo.png"
              width="50%"
              height="auto"
              className="block"
            ></img>
            <h1 className="text-4xl font-bold mb-1">Whisper WebGPU</h1>
            <h2 className="text-xl font-semibold">
              Real-time in-browser speech recognition
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            {status === null && (
              <>
                <p className="max-w-[480px] mb-4">
                  <br />
                  You are about to load{" "}
                  <a
                    href="https://huggingface.co/onnx-community/whisper-base"
                    target="_blank"
                    rel="noreferrer"
                    className="font-medium underline"
                  >
                    whisper-base
                  </a>
                  , a 73 million parameter speech recognition model that is
                  optimized for inference on the web. Once downloaded, the model
                  (~200&nbsp;MB) will be cached and reused when you revisit the
                  page.
                  <br />
                  <br />
                  Everything runs directly in your browser using{" "}
                  <a
                    href="https://huggingface.co/docs/transformers.js"
                    target="_blank"
                    rel="noreferrer"
                    className="underline"
                  >
                    ðŸ¤—&nbsp;Transformers.js
                  </a>{" "}
                  and ONNX Runtime Web, meaning no data is sent to a server. You
                  can even disconnect from the internet after the model has
                  loaded!
                </p>

                <button
                  className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 disabled:cursor-not-allowed select-none"
                  onClick={() => {
                    worker.current.postMessage({ type: "load" });
                    setStatus("loading");
                  }}
                  disabled={status !== null}
                >
                  Load model
                </button>
              </>
            )}

            <div className="w-[500px] p-2">
              <AudioVisualizer className="w-full rounded-lg" stream={stream} />
              {status === "ready" && (
                <div className="relative">
                  <p className="w-full h-[80px] overflow-y-auto overflow-wrap-anywhere border rounded-lg p-2">
                    {text}
                  </p>
                  {tps && (
                    <span className="absolute bottom-0 right-0 px-1">
                      {tps.toFixed(2)} tok/s
                    </span>
                  )}
                </div>
              )}
            </div>
            {status === "ready" && (
              <div className="relative w-full flex justify-center">
                <LanguageSelector
                  language={language}
                  setLanguage={(e) => {
                    recorderRef.current?.stop();
                    setLanguage(e);
                    recorderRef.current?.start();
                  }}
                />
                <button
                  className="border rounded-lg px-2 absolute right-2"
                  onClick={() => {
                    recorderRef.current?.stop();
                    recorderRef.current?.start();
                  }}
                >
                  Reset
                </button>
              </div>
            )}
            {status === "loading" && (
              <div className="w-full max-w-[500px] text-left mx-auto p-4">
                <p className="text-center">{loadingMessage}</p>
                {progressItems.map(({ file, progress, total }, i) => (
                  <Progress
                    key={i}
                    text={file}
                    percentage={progress}
                    total={total}
                  />
                ))}
              </div>
            )}
          </div>
        </div>
      }
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\realtime-whisper-webgpu\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;

@layer utilities {
  .scrollbar-thin::-webkit-scrollbar {
    @apply w-2;
  }

  .scrollbar-thin::-webkit-scrollbar-track {
    @apply rounded-full bg-gray-100 dark:bg-gray-700;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb {
    @apply rounded-full bg-gray-300 dark:bg-gray-600;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb:hover {
    @apply bg-gray-500;
  }

  .animation-delay-200 {
    animation-delay: 200ms;
  }
  .animation-delay-400 {
    animation-delay: 400ms;
  }

  .overflow-wrap-anywhere {
    overflow-wrap: anywhere;
  }
}


----- .\realtime-whisper-webgpu\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\realtime-whisper-webgpu\src\worker.js -----

import {
  AutoTokenizer,
  AutoProcessor,
  WhisperForConditionalGeneration,
  TextStreamer,
  full,
} from "@huggingface/transformers";

const MAX_NEW_TOKENS = 64;

/**
 * This class uses the Singleton pattern to ensure that only one instance of the model is loaded.
 */
class AutomaticSpeechRecognitionPipeline {
  static model_id = "onnx-community/whisper-base";
  static tokenizer = null;
  static processor = null;
  static model = null;

  static async getInstance(progress_callback = null) {
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id, {
      progress_callback,
    });
    this.processor ??= AutoProcessor.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= WhisperForConditionalGeneration.from_pretrained(
      this.model_id,
      {
        dtype: {
          encoder_model: "fp32", // 'fp16' works too
          decoder_model_merged: "q4", // or 'fp32' ('fp16' is broken)
        },
        device: "webgpu",
        progress_callback,
      },
    );

    return Promise.all([this.tokenizer, this.processor, this.model]);
  }
}

let processing = false;
async function generate({ audio, language }) {
  if (processing) return;
  processing = true;

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  // Retrieve the text-generation pipeline.
  const [tokenizer, processor, model] =
    await AutomaticSpeechRecognitionPipeline.getInstance();

  let startTime;
  let numTokens = 0;
  let tps;
  const token_callback_function = () => {
    startTime ??= performance.now();

    if (numTokens++ > 0) {
      tps = (numTokens / (performance.now() - startTime)) * 1000;
    }
  };
  const callback_function = (output) => {
    self.postMessage({
      status: "update",
      output,
      tps,
      numTokens,
    });
  };

  const streamer = new TextStreamer(tokenizer, {
    skip_prompt: true,
    skip_special_tokens: true,
    callback_function,
    token_callback_function,
  });

  const inputs = await processor(audio);

  const outputs = await model.generate({
    ...inputs,
    max_new_tokens: MAX_NEW_TOKENS,
    language,
    streamer,
  });

  const decoded = tokenizer.batch_decode(outputs, {
    skip_special_tokens: true,
  });

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: decoded,
  });
  processing = false;
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [tokenizer, processor, model] =
    await AutomaticSpeechRecognitionPipeline.getInstance((x) => {
      // We also add a progress callback to the pipeline so that we can
      // track model loading.
      self.postMessage(x);
    });

  self.postMessage({
    status: "loading",
    data: "Compiling shaders and warming up model...",
  });

  // Run model with dummy input to compile shaders
  await model.generate({
    input_features: full([1, 80, 3000], 0.0),
    max_new_tokens: 1,
  });
  self.postMessage({ status: "ready" });
}

// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "load":
      load();
      break;

    case "generate":
      generate(data);
      break;
  }
});


----- .\realtime-whisper-webgpu\src\components\AudioVisualizer.jsx -----

import { useRef, useCallback, useEffect } from "react";

export function AudioVisualizer({ stream, ...props }) {
  const canvasRef = useRef(null);

  const visualize = useCallback((stream) => {
    const audioContext = new (window.AudioContext ||
      window.webkitAudioContext)();
    const source = audioContext.createMediaStreamSource(stream);
    const analyser = audioContext.createAnalyser();
    analyser.fftSize = 2048;
    source.connect(analyser);

    const canvas = canvasRef.current;
    const canvasCtx = canvas.getContext("2d");
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    const drawVisual = () => {
      requestAnimationFrame(drawVisual);
      analyser.getByteTimeDomainData(dataArray);

      canvasCtx.fillStyle = "rgb(255, 255, 255)";
      canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

      canvasCtx.lineWidth = 2;
      canvasCtx.strokeStyle = "rgb(0, 0, 0)";
      canvasCtx.beginPath();

      const sliceWidth = (canvas.width * 1.0) / bufferLength;

      let x = 0;
      for (let i = 0; i < bufferLength; ++i) {
        const v = dataArray[i] / 128.0;
        const y = (v * canvas.height) / 2;

        if (i === 0) {
          canvasCtx.moveTo(x, y);
        } else {
          canvasCtx.lineTo(x, y);
        }

        x += sliceWidth;
      }

      canvasCtx.lineTo(canvas.width, canvas.height / 2);
      canvasCtx.stroke();
    };

    drawVisual();
  }, []);

  useEffect(() => {
    stream && visualize(stream);
  }, [visualize, stream]);
  return <canvas {...props} width={720} height={240} ref={canvasRef}></canvas>;
}


----- .\realtime-whisper-webgpu\src\components\LanguageSelector.jsx -----

function titleCase(str) {
  str = str.toLowerCase();
  return (str.match(/\w+.?/g) || [])
    .map((word) => {
      return word.charAt(0).toUpperCase() + word.slice(1);
    })
    .join("");
}

// List of supported languages:
// https://help.openai.com/en/articles/7031512-whisper-api-faq
// https://github.com/openai/whisper/blob/248b6cb124225dd263bb9bd32d060b6517e067f8/whisper/tokenizer.py#L79
const LANGUAGES = {
  en: "english",
  zh: "chinese",
  de: "german",
  es: "spanish/castilian",
  ru: "russian",
  ko: "korean",
  fr: "french",
  ja: "japanese",
  pt: "portuguese",
  tr: "turkish",
  pl: "polish",
  ca: "catalan/valencian",
  nl: "dutch/flemish",
  ar: "arabic",
  sv: "swedish",
  it: "italian",
  id: "indonesian",
  hi: "hindi",
  fi: "finnish",
  vi: "vietnamese",
  he: "hebrew",
  uk: "ukrainian",
  el: "greek",
  ms: "malay",
  cs: "czech",
  ro: "romanian/moldavian/moldovan",
  da: "danish",
  hu: "hungarian",
  ta: "tamil",
  no: "norwegian",
  th: "thai",
  ur: "urdu",
  hr: "croatian",
  bg: "bulgarian",
  lt: "lithuanian",
  la: "latin",
  mi: "maori",
  ml: "malayalam",
  cy: "welsh",
  sk: "slovak",
  te: "telugu",
  fa: "persian",
  lv: "latvian",
  bn: "bengali",
  sr: "serbian",
  az: "azerbaijani",
  sl: "slovenian",
  kn: "kannada",
  et: "estonian",
  mk: "macedonian",
  br: "breton",
  eu: "basque",
  is: "icelandic",
  hy: "armenian",
  ne: "nepali",
  mn: "mongolian",
  bs: "bosnian",
  kk: "kazakh",
  sq: "albanian",
  sw: "swahili",
  gl: "galician",
  mr: "marathi",
  pa: "punjabi/panjabi",
  si: "sinhala/sinhalese",
  km: "khmer",
  sn: "shona",
  yo: "yoruba",
  so: "somali",
  af: "afrikaans",
  oc: "occitan",
  ka: "georgian",
  be: "belarusian",
  tg: "tajik",
  sd: "sindhi",
  gu: "gujarati",
  am: "amharic",
  yi: "yiddish",
  lo: "lao",
  uz: "uzbek",
  fo: "faroese",
  ht: "haitian creole/haitian",
  ps: "pashto/pushto",
  tk: "turkmen",
  nn: "nynorsk",
  mt: "maltese",
  sa: "sanskrit",
  lb: "luxembourgish/letzeburgesch",
  my: "myanmar/burmese",
  bo: "tibetan",
  tl: "tagalog",
  mg: "malagasy",
  as: "assamese",
  tt: "tatar",
  haw: "hawaiian",
  ln: "lingala",
  ha: "hausa",
  ba: "bashkir",
  jw: "javanese",
  su: "sundanese",
};
export function LanguageSelector({ language, setLanguage }) {
  const handleLanguageChange = (event) => {
    setLanguage(event.target.value);
  };

  const names = Object.values(LANGUAGES).map(titleCase);

  return (
    <select
      className="border rounded-lg p-2 max-w-[100px]"
      value={language}
      onChange={handleLanguageChange}
    >
      {Object.keys(LANGUAGES).map((key, i) => (
        <option key={key} value={key}>
          {names[i]}
        </option>
      ))}
    </select>
  );
}


----- .\realtime-whisper-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\remove-background-webgpu\src\App.jsx -----

import { useState, useCallback, useEffect, useRef } from "react";
import { useDropzone } from "react-dropzone";
import {
  env,
  AutoModel,
  AutoProcessor,
  RawImage,
} from "@huggingface/transformers";

import JSZip from "jszip";
import { saveAs } from "file-saver";

export default function App() {
  const [images, setImages] = useState([]);
  const [processedImages, setProcessedImages] = useState([]);
  const [isProcessing, setIsProcessing] = useState(false);
  const [isDownloadReady, setIsDownloadReady] = useState(false);
  const [isLoading, setIsLoading] = useState(true);
  const [error, setError] = useState(null);

  const modelRef = useRef(null);
  const processorRef = useRef(null);

  useEffect(() => {
    (async () => {
      try {
        if (!navigator.gpu) {
          throw new Error("WebGPU is not supported in this browser.");
        }
        const model_id = "Xenova/modnet";
        env.backends.onnx.wasm.proxy = false;
        modelRef.current ??= await AutoModel.from_pretrained(model_id, {
          device: "webgpu",
        });
        processorRef.current ??= await AutoProcessor.from_pretrained(model_id);
      } catch (err) {
        setError(err);
      }
      setIsLoading(false);
    })();
  }, []);

  const onDrop = useCallback((acceptedFiles) => {
    setImages((prevImages) => [
      ...prevImages,
      ...acceptedFiles.map((file) => URL.createObjectURL(file)),
    ]);
  }, []);

  const {
    getRootProps,
    getInputProps,
    isDragActive,
    isDragAccept,
    isDragReject,
  } = useDropzone({
    onDrop,
    accept: {
      "image/*": [".jpeg", ".jpg", ".png"],
    },
  });

  const removeImage = (index) => {
    setImages((prevImages) => prevImages.filter((_, i) => i !== index));
    setProcessedImages((prevProcessed) =>
      prevProcessed.filter((_, i) => i !== index),
    );
  };

  const processImages = async () => {
    setIsProcessing(true);
    setProcessedImages([]);

    const model = modelRef.current;
    const processor = processorRef.current;

    for (let i = 0; i < images.length; ++i) {
      // Load image
      const img = await RawImage.fromURL(images[i]);

      // Pre-process image
      const { pixel_values } = await processor(img);

      // Predict alpha matte
      const { output } = await model({ input: pixel_values });

      const maskData = (
        await RawImage.fromTensor(output[0].mul(255).to("uint8")).resize(
          img.width,
          img.height,
        )
      ).data;

      // Create new canvas
      const canvas = document.createElement("canvas");
      canvas.width = img.width;
      canvas.height = img.height;
      const ctx = canvas.getContext("2d");

      // Draw original image output to canvas
      ctx.drawImage(img.toCanvas(), 0, 0);

      // Update alpha channel
      const pixelData = ctx.getImageData(0, 0, img.width, img.height);
      for (let i = 0; i < maskData.length; ++i) {
        pixelData.data[4 * i + 3] = maskData[i];
      }
      ctx.putImageData(pixelData, 0, 0);
      setProcessedImages((prevProcessed) => [
        ...prevProcessed,
        canvas.toDataURL("image/png"),
      ]);
    }

    setIsProcessing(false);
    setIsDownloadReady(true);
  };

  const downloadAsZip = async () => {
    const zip = new JSZip();
    const promises = images.map(
      (image, i) =>
        new Promise((resolve) => {
          const canvas = document.createElement("canvas");
          const ctx = canvas.getContext("2d");

          const img = new Image();
          img.src = processedImages[i] || image;

          img.onload = () => {
            canvas.width = img.width;
            canvas.height = img.height;
            ctx.drawImage(img, 0, 0);
            canvas.toBlob((blob) => {
              if (blob) {
                zip.file(`image-${i + 1}.png`, blob);
              }
              resolve(null);
            }, "image/png");
          };
        }),
    );

    await Promise.all(promises);

    const content = await zip.generateAsync({ type: "blob" });
    saveAs(content, "images.zip");
  };

  const clearAll = () => {
    setImages([]);
    setProcessedImages([]);
    setIsDownloadReady(false);
  };

  const copyToClipboard = async (url) => {
    try {
      // Fetch the image from the URL and convert it to a Blob
      const response = await fetch(url);
      const blob = await response.blob();

      // Create a clipboard item with the image blob
      const clipboardItem = new ClipboardItem({ [blob.type]: blob });

      // Write the clipboard item to the clipboard
      await navigator.clipboard.write([clipboardItem]);

      console.log("Image copied to clipboard");
    } catch (err) {
      console.error("Failed to copy image: ", err);
    }
  };

  const downloadImage = (url) => {
    const link = document.createElement("a");
    link.href = url;
    link.download = "image.png";
    document.body.appendChild(link);
    link.click();
    document.body.removeChild(link);
  };

  if (error) {
    return (
      <div className="min-h-screen bg-black text-white flex items-center justify-center">
        <div className="text-center">
          <h2 className="text-4xl mb-2">ERROR</h2>
          <p className="text-xl max-w-[500px]">{error.message}</p>
        </div>
      </div>
    );
  }

  if (isLoading) {
    return (
      <div className="min-h-screen bg-black text-white flex items-center justify-center">
        <div className="text-center">
          <div className="inline-block animate-spin rounded-full h-8 w-8 border-t-2 border-b-2 border-white mb-4"></div>
          <p className="text-lg">Loading background removal model...</p>
        </div>
      </div>
    );
  }

  return (
    <div className="min-h-screen bg-black text-white p-8">
      <div className="max-w-4xl mx-auto">
        <h1 className="text-4xl font-bold mb-2 text-center">
          Remove Background WebGPU
        </h1>
        <h2 className="text-lg font-semibold mb-2 text-center">
          In-browser background removal, powered by{" "}
          <a
            className="underline"
            target="_blank"
            href="https://github.com/xenova/transformers.js"
          >
            ðŸ¤— Transformers.js
          </a>
        </h2>
        <div className="flex justify-center mb-8 gap-8">
          <a
            className="underline"
            target="_blank"
            href="https://github.com/huggingface/transformers.js-examples/blob/main/LICENSE"
          >
            License (Apache 2.0)
          </a>
          <a
            className="underline"
            target="_blank"
            href="https://huggingface.co/Xenova/modnet"
          >
            Model (MODNet)
          </a>
          <a
            className="underline"
            target="_blank"
            href="https://github.com/huggingface/transformers.js-examples/tree/main/remove-background-webgpu/"
          >
            Code (GitHub)
          </a>
        </div>
        <div
          {...getRootProps()}
          className={`p-8 mb-8 border-2 border-dashed rounded-lg text-center cursor-pointer transition-colors duration-300 ease-in-out
            ${isDragAccept ? "border-green-500 bg-green-900/20" : ""}
            ${isDragReject ? "border-red-500 bg-red-900/20" : ""}
            ${isDragActive ? "border-blue-500 bg-blue-900/20" : "border-gray-700 hover:border-blue-500 hover:bg-blue-900/10"}
          `}
        >
          <input {...getInputProps()} className="hidden" />
          <p className="text-lg mb-2">
            {isDragActive
              ? "Drop the images here..."
              : "Drag and drop some images here"}
          </p>
          <p className="text-sm text-gray-400">or click to select files</p>
        </div>
        <div className="flex flex-col items-center gap-4 mb-8">
          <button
            onClick={processImages}
            disabled={isProcessing || images.length === 0}
            className="px-6 py-3 bg-blue-600 text-white rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 focus:ring-offset-black disabled:bg-gray-700 disabled:cursor-not-allowed transition-colors duration-200 text-lg font-semibold"
          >
            {isProcessing ? "Processing..." : "Process"}
          </button>
          <div className="flex gap-4">
            <button
              onClick={downloadAsZip}
              disabled={!isDownloadReady}
              className="px-3 py-1 bg-green-600 text-white rounded-md hover:bg-green-700 focus:outline-none focus:ring-2 focus:ring-green-500 focus:ring-offset-2 focus:ring-offset-black disabled:bg-gray-700 disabled:cursor-not-allowed transition-colors duration-200 text-sm"
            >
              Download as ZIP
            </button>
            <button
              onClick={clearAll}
              className="px-3 py-1 bg-red-600 text-white rounded-md hover:bg-red-700 focus:outline-none focus:ring-2 focus:ring-red-500 focus:ring-offset-2 focus:ring-offset-black transition-colors duration-200 text-sm"
            >
              Clear All
            </button>
          </div>
        </div>
        <div className="grid grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-6">
          {images.map((src, index) => (
            <div key={index} className="relative group">
              <img
                src={processedImages[index] || src}
                alt={`Image ${index + 1}`}
                className="rounded-lg object-cover w-full h-48"
              />
              {processedImages[index] && (
                <div className="absolute inset-0 bg-black bg-opacity-70 opacity-0 group-hover:opacity-100 transition-opacity duration-300 rounded-lg flex items-center justify-center">
                  <button
                    onClick={() =>
                      copyToClipboard(processedImages[index] || src)
                    }
                    className="mx-2 px-3 py-1 bg-white text-gray-900 rounded-md hover:bg-gray-200 transition-colors duration-200 text-sm"
                    aria-label={`Copy image ${index + 1} to clipboard`}
                  >
                    Copy
                  </button>
                  <button
                    onClick={() => downloadImage(processedImages[index] || src)}
                    className="mx-2 px-3 py-1 bg-white text-gray-900 rounded-md hover:bg-gray-200 transition-colors duration-200 text-sm"
                    aria-label={`Download image ${index + 1}`}
                  >
                    Download
                  </button>
                </div>
              )}
              <button
                onClick={() => removeImage(index)}
                className="absolute top-2 right-2 bg-black bg-opacity-50 text-white w-6 h-6 rounded-full flex items-center justify-center opacity-0 group-hover:opacity-100 transition-opacity duration-300 hover:bg-opacity-70"
                aria-label={`Remove image ${index + 1}`}
              >
                &#x2715;
              </button>
            </div>
          ))}
        </div>
      </div>
    </div>
  );
}


----- .\remove-background-webgpu\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;


----- .\remove-background-webgpu\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\semantic-image-search-web\src\App.jsx -----

import { useState, useEffect, useCallback, useRef } from "react";
import { Modal } from "./components/Modal";
import { SearchBar } from "./components/SearchBar";
import { ImageGrid } from "./components/ImageGrid";

export default function Home() {
  // Application state
  const [ready, setReady] = useState(null);
  const [images, setImages] = useState(null);
  const [currentImage, setCurrentImage] = useState(null);

  // Create a reference to the worker object.
  const worker = useRef(null);

  // We use the `useEffect` hook to set up the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    worker.current ??= new Worker(new URL("./worker.js", import.meta.url), {
      type: "module",
    });
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "initiate":
          setReady(false);
          break;
        case "ready":
          setReady(true);
          break;
        case "complete":
          setImages(e.data.output);
          break;
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () =>
      worker.current.removeEventListener("message", onMessageReceived);
  });

  const search = useCallback((text) => {
    if (worker.current) {
      worker.current.postMessage({ text });
    }
  }, []);

  return (
    <main className="mx-auto max-w-[1960px] p-4 relative">
      <Modal currentImage={currentImage} setCurrentImage={setCurrentImage} />
      <SearchBar search={search} />
      {ready === false && (
        <div className="z-10 fixed top-0 left-0 w-full h-full bg-black bg-opacity-50 flex items-center justify-center">
          <div className="text-white text-2xl font-bold">
            Loading model and database...
          </div>
        </div>
      )}
      <ImageGrid images={images} setCurrentImage={setCurrentImage} />
    </main>
  );
}


----- .\semantic-image-search-web\src\index.css -----

@import url("https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&family=Poppins:wght@100;200;300;400;500;600;700;800;900&display=swap");
@import "tailwindcss";

* {
  font-family: "Inter", sans-serif;
}

:root {
  --foreground-rgb: 255, 255, 255;
  --background-start-rgb: 0, 0, 0;
  --background-end-rgb: 0, 0, 0;
}

body {
  color: rgb(var(--foreground-rgb));
  background: linear-gradient(
      to bottom,
      transparent,
      rgb(var(--background-end-rgb))
    )
    rgb(var(--background-start-rgb));
}


----- .\semantic-image-search-web\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import "./index.css";
import App from "./App.jsx";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\semantic-image-search-web\src\utils.js -----

import { decode } from "blurhash";

const SIZE = 32;

export function blurHashToDataURL(hash) {
  if (!hash) return undefined;

  const pixels = decode(hash, SIZE, SIZE);

  const canvas = document.createElement("canvas");
  canvas.width = SIZE;
  canvas.height = SIZE;

  const ctx = canvas.getContext("2d");
  const imageData = ctx.createImageData(SIZE, SIZE);
  imageData.data.set(pixels);
  ctx.putImageData(imageData, 0, 0);

  return canvas.toDataURL();
}

function downloadData(url, filename) {
  // Create an anchor element with the data URL as the href attribute
  const downloadLink = document.createElement("a");
  downloadLink.href = url;

  // Set the download attribute to specify the desired filename for the downloaded image
  downloadLink.download = filename;

  // Trigger the download
  downloadLink.click();

  // Clean up: remove the anchor element from the DOM
  downloadLink.remove();
}

export function downloadImage(url, filename) {
  fetch(url, {
    headers: new Headers({
      Origin: location.origin,
    }),
    mode: "cors",
  })
    .then((response) => response.blob())
    .then((blob) => {
      let blobUrl = window.URL.createObjectURL(blob);
      downloadData(blobUrl, filename);
    })
    .catch((e) => console.error(e));
}

// Adapted from https://github.com/xenova/transformers.js/blob/c367f9d68b809bbbf81049c808bf6d219d761d23/src/utils/hub.js#L330
export async function getCachedFile(url) {
  let cache;
  try {
    cache = await caches.open("image-database");
    const cachedResponse = await cache.match(url);
    if (cachedResponse) {
      return await cachedResponse.arrayBuffer();
    }
  } catch (e) {
    console.warn("Unable to open cache", e);
  }

  // No cache, or cache failed to open. Fetch the file.
  const response = await fetch(url);
  const buffer = await response.arrayBuffer();

  if (cache) {
    try {
      // NOTE: We use `new Response(buffer, ...)` instead of `response.clone()` to handle LFS files
      await cache.put(
        url,
        new Response(buffer, {
          headers: response.headers,
        }),
      );
    } catch (e) {
      console.warn("Unable to cache file", e);
    }
  }

  return buffer;
}

export async function getCachedJSON(url) {
  let buffer = await getCachedFile(url);

  let decoder = new TextDecoder("utf-8");
  let jsonData = decoder.decode(buffer);

  return JSON.parse(jsonData);
}


----- .\semantic-image-search-web\src\worker.js -----

import {
  AutoTokenizer,
  CLIPTextModelWithProjection,
  cos_sim,
} from "@huggingface/transformers";
import { getCachedFile, getCachedJSON } from "./utils.js";

const EMBED_DIM = 512;

class ApplicationSingleton {
  static model_id = "Xenova/clip-vit-base-patch16";
  static BASE_URL =
    "https://huggingface.co/datasets/Xenova/semantic-image-search-assets/resolve/main/";

  static tokenizer = null;
  static text_model = null;
  static metadata = null;
  static embeddings = null;

  static async getInstance(progress_callback = null) {
    // Load tokenizer and text model
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id, {
      progress_callback,
    });
    this.text_model ??= CLIPTextModelWithProjection.from_pretrained(
      this.model_id,
      { progress_callback },
    );
    this.metadata ??= getCachedJSON(this.BASE_URL + "image-embeddings.json");
    this.embeddings ??= new Promise((resolve, reject) => {
      getCachedFile(this.BASE_URL + "image-embeddings_25k-512-32bit.bin")
        .then((buffer) => {
          resolve(new Float32Array(buffer));
        })
        .catch(reject);
    });

    return Promise.all([
      this.tokenizer,
      this.text_model,
      this.metadata,
      this.embeddings,
    ]);
  }
}

function cosineSimilarity(query_embeds, database_embeds) {
  const numDB = database_embeds.length / EMBED_DIM;
  const similarityScores = new Array(numDB);

  for (let i = 0; i < numDB; ++i) {
    const startOffset = i * EMBED_DIM;
    const dbVector = database_embeds.slice(
      startOffset,
      startOffset + EMBED_DIM,
    );

    similarityScores[i] = cos_sim(query_embeds, dbVector);
  }

  return similarityScores;
}

// Listen for messages from the main thread
self.addEventListener("message", async (event) => {
  // Get the tokenizer, model, metadata, and embeddings. When called for the first time,
  // this will load the files and cache them for future use.
  const [tokenizer, text_model, metadata, embeddings] =
    await ApplicationSingleton.getInstance(self.postMessage);

  // Send the output back to the main thread
  self.postMessage({ status: "ready" });

  // Run tokenization
  const text_inputs = tokenizer(event.data.text, {
    padding: true,
    truncation: true,
  });

  // Compute embeddings
  const { text_embeds } = await text_model(text_inputs);

  // Compute similarity scores
  const scores = cosineSimilarity(text_embeds.data, embeddings);

  // Make a copy of the metadata
  let output = metadata.slice(0);

  // Add scores to output
  for (let i = 0; i < metadata.length; ++i) {
    output[i].score = scores[i];
  }

  // Sort by score
  output.sort((a, b) => b.score - a.score);

  // Get top 100 results
  output = output.slice(0, 100);

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: output,
  });
});


----- .\semantic-image-search-web\src\components\Image.jsx -----

import { useState, useEffect, useRef } from "react";

export default function Image({
  alt,
  className,
  style,
  blurDataURL,
  src,
  width,
  height,
  objectFit,
}) {
  const [isLoaded, setIsLoaded] = useState(false);
  const [isVisible, setIsVisible] = useState(false);
  const imgRef = useRef(null);

  useEffect(() => {
    const observer = new IntersectionObserver(
      (entries) => {
        entries.forEach((entry) => {
          if (entry.isIntersecting) {
            setIsVisible(true);
            observer.unobserve(entry.target);
          }
        });
      },
      { threshold: 0.1 },
    );

    if (imgRef.current) {
      observer.observe(imgRef.current);
    }

    return () => {
      if (imgRef.current) {
        observer.unobserve(imgRef.current);
      }
    };
  }, []);

  const aspectRatio = width && height ? (height / width) * 100 : null;

  return (
    <div
      style={{
        width: "100%",
        height: "100%",
        paddingTop: aspectRatio ? `${aspectRatio}%` : "auto",
        position: "relative",
        overflow: "hidden",
        ...style,
      }}
      className={className}
      ref={imgRef}
    >
      {/* Placeholder Blur Image */}
      {!isLoaded && blurDataURL && (
        <img
          src={blurDataURL}
          alt={alt}
          style={{
            position: "absolute",
            top: 0,
            left: 0,
            width: "100%",
            height: "100%",
            filter: "blur(20px)",
            transform: "scale(1.1)",
          }}
        />
      )}
      {/* Actual Image */}
      {isVisible && (
        <img
          src={src}
          alt={alt}
          width={width}
          height={height}
          style={{
            display: isLoaded ? "block" : "none",
            position: "absolute",
            top: 0,
            left: 0,
            width: "100%",
            height: "100%",
            objectFit: objectFit,
          }}
          onLoad={() => setIsLoaded(true)}
        />
      )}
    </div>
  );
}


----- .\semantic-image-search-web\src\components\ImageGrid.jsx -----

import Image from "./Image";
import { blurHashToDataURL } from "../utils.js";

export function ImageGrid({ images, setCurrentImage }) {
  return (
    <div className="columns-2 gap-4 sm:columns-3 xl:columns-4 2xl:columns-5">
      {images &&
        images.map(({ id, url, ar, blur }) => (
          <div
            key={id}
            href={`https://unsplash.com/photos/${id}`}
            className="after:content group cursor-pointer relative mb-4 block w-full after:pointer-events-none after:absolute after:inset-0 after:rounded-lg after:shadow-highlight"
            onClick={() => {
              setCurrentImage({ id, url, ar, blur });
            }}
          >
            <Image
              alt=""
              className="transform rounded-lg brightness-90 transition will-change-auto group-hover:brightness-110"
              style={{ transform: "translate3d(0, 0, 0)" }}
              placeholder="blur"
              blurDataURL={blurHashToDataURL(blur)}
              src={`https://images.unsplash.com/${url}?auto=format&fit=crop&w=480&q=80`}
              width={480}
              height={480 / ar}
            />
          </div>
        ))}
    </div>
  );
}


----- .\semantic-image-search-web\src\components\Modal.jsx -----

"use client";

import Image from "./Image";
import { downloadImage } from "../utils.js";

export function Modal({ currentImage, setCurrentImage }) {
  const photo_url = currentImage
    ? `https://unsplash.com/photos/${currentImage.id}`
    : null;
  const photo_image_url = currentImage
    ? `https://images.unsplash.com/${currentImage.url}?auto=format&fit=crop&w=480&q=80`
    : null;
  return (
    <div
      className="fixed inset-0 z-30 backdrop-blur-2xl w-full h-full bg-black top-0 left-0 transition"
      style={{
        backgroundColor: `rgba(0, 0, 0, ${currentImage ? 0.8 : 0})`,
        opacity: currentImage ? 1 : 0,
        pointerEvents: currentImage ? "auto" : "none",
      }}
    >
      {currentImage && (
        <>
          <Image
            alt=""
            className="transform rounded-lg transition will-change-auto"
            style={{ transform: "translate3d(0, 0, 0)" }}
            objectFit="contain"
            src={photo_image_url}
          />
          <div className="absolute top-0 left-0 flex items-center gap-2 p-3 text-white">
            <button
              onClick={() => setCurrentImage(null)}
              className="rounded-full bg-black/50 p-2 text-white/75 backdrop-blur-lg transition hover:bg-black/75 hover:text-white cursor-pointer"
            >
              <svg
                xmlns="http://www.w3.org/2000/svg"
                fill="none"
                viewBox="0 0 24 24"
                strokeWidth="1.5"
                stroke="currentColor"
                aria-hidden="true"
                className="h-5 w-5"
              >
                <path
                  strokeLinecap="round"
                  strokeLinejoin="round"
                  d="M6 18L18 6M6 6l12 12"
                ></path>
              </svg>
            </button>
          </div>
          <div className="absolute top-0 right-0 flex items-center gap-2 p-3 text-white">
            <a
              href={photo_url}
              className="rounded-full bg-black/50 p-2 text-white/75 backdrop-blur-lg transition hover:bg-black/75 hover:text-white"
              target="_blank"
              title="View on Unsplash"
              rel="noreferrer"
            >
              <svg
                xmlns="http://www.w3.org/2000/svg"
                fill="none"
                viewBox="0 0 24 24"
                strokeWidth="1.5"
                stroke="currentColor"
                aria-hidden="true"
                className="h-5 w-5"
              >
                <path
                  strokeLinecap="round"
                  strokeLinejoin="round"
                  d="M13.5 6H5.25A2.25 2.25 0 003 8.25v10.5A2.25 2.25 0 005.25 21h10.5A2.25 2.25 0 0018 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"
                ></path>
              </svg>
            </a>
            <button
              onClick={() =>
                downloadImage(photo_image_url, `${currentImage.id}.png`)
              }
              className="rounded-full bg-black/50 p-2 text-white/75 backdrop-blur-lg transition hover:bg-black/75 hover:text-white cursor-pointer"
              title="Download"
            >
              <svg
                xmlns="http://www.w3.org/2000/svg"
                fill="none"
                viewBox="0 0 24 24"
                strokeWidth="1.5"
                stroke="currentColor"
                aria-hidden="true"
                className="h-5 w-5"
              >
                <path
                  strokeLinecap="round"
                  strokeLinejoin="round"
                  d="M3 16.5v2.25A2.25 2.25 0 005.25 21h13.5A2.25 2.25 0 0021 18.75V16.5M16.5 12L12 16.5m0 0L7.5 12m4.5 4.5V3"
                ></path>
              </svg>
            </button>
          </div>
        </>
      )}
    </div>
  );
}


----- .\semantic-image-search-web\src\components\SearchBar.jsx -----

"use client";

export function SearchBar({ search }) {
  return (
    <form
      onSubmit={(e) => {
        e.preventDefault();
        const formData = new FormData(e.target);
        const text = formData.get("text");
        search(text);
      }}
      className="relative mb-2"
    >
      <div className="absolute inset-y-0 left-0 flex items-center pl-3 pointer-events-none">
        <svg
          className="w-4 h-4 text-gray-500 dark:text-gray-400"
          aria-hidden="true"
          xmlns="http://www.w3.org/2000/svg"
          fill="none"
          viewBox="0 0 20 20"
        >
          <path
            stroke="currentColor"
            strokeLinecap="round"
            strokeLinejoin="round"
            strokeWidth="2"
            d="m19 19-4-4m0-7A7 7 0 1 1 1 8a7 7 0 0 1 14 0Z"
          />
        </svg>
      </div>
      <input
        type="search"
        name="text"
        id="default-search"
        className="block w-full p-4 pl-10 text-sm text-gray-900 border border-gray-300 rounded-lg bg-gray-50 focus:ring-blue-500 focus:border-blue-500 dark:bg-gray-700 dark:border-gray-600 dark:placeholder-gray-400 dark:text-white dark:focus:ring-blue-500 dark:focus:border-blue-500"
        placeholder="Search for images..."
        required
      />
      <button
        type="submit"
        className="text-white absolute right-2.5 bottom-2.5 bg-blue-700 hover:bg-blue-800 focus:ring-4 focus:outline-none focus:ring-blue-300 font-medium rounded-lg text-sm px-4 py-2 dark:bg-blue-600 dark:hover:bg-blue-700 dark:focus:ring-blue-800 cursor-pointer"
      >
        Search
      </button>
    </form>
  );
}


----- .\smollm-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import Chat from "./components/Chat";
import ArrowRightIcon from "./components/icons/ArrowRightIcon";
import StopIcon from "./components/icons/StopIcon";
import Progress from "./components/Progress";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;
const STICKY_SCROLL_THRESHOLD = 120;
const EXAMPLES = [
  "Give me some tips to improve my time management skills.",
  "What is the difference between AI and ML?",
  "Write python code to compute the nth fibonacci number.",
];

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const textareaRef = useRef(null);
  const chatContainerRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);
  const [isRunning, setIsRunning] = useState(false);

  // Inputs and outputs
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState([]);
  const [tps, setTps] = useState(null);
  const [numTokens, setNumTokens] = useState(null);

  function onEnter(message) {
    setMessages((prev) => [...prev, { role: "user", content: message }]);
    setTps(null);
    setIsRunning(true);
    setInput("");
  }

  function onInterrupt() {
    // NOTE: We do not set isRunning to false here because the worker
    // will send a 'complete' message when it is done.
    worker.current.postMessage({ type: "interrupt" });
  }

  useEffect(() => {
    resizeInput();
  }, [input]);

  function resizeInput() {
    if (!textareaRef.current) return;

    const target = textareaRef.current;
    target.style.height = "auto";
    const newHeight = Math.min(Math.max(target.scrollHeight, 24), 200);
    target.style.height = `${newHeight}px`;
  }

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    if (!worker.current) {
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
      worker.current.postMessage({ type: "check" }); // Do a feature check
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "start":
          {
            // Start generation
            setMessages((prev) => [
              ...prev,
              { role: "assistant", content: "" },
            ]);
          }
          break;

        case "update":
          {
            // Generation update: update the output text.
            // Parse messages
            const { output, tps, numTokens } = e.data;
            setTps(tps);
            setNumTokens(numTokens);
            setMessages((prev) => {
              const cloned = [...prev];
              const last = cloned.at(-1);
              cloned[cloned.length - 1] = {
                ...last,
                content: last.content + output,
              };
              return cloned;
            });
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsRunning(false);
          break;

        case "error":
          setError(e.data.data);
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
    };
  }, []);

  // Send the messages to the worker thread whenever the `messages` state changes.
  useEffect(() => {
    if (messages.filter((x) => x.role === "user").length === 0) {
      // No user messages yet: do nothing.
      return;
    }
    if (messages.at(-1).role === "assistant") {
      // Do not update if the last message is from the assistant
      return;
    }
    setTps(null);
    worker.current.postMessage({ type: "generate", data: messages });
  }, [messages, isRunning]);

  useEffect(() => {
    if (!chatContainerRef.current || !isRunning) return;
    const element = chatContainerRef.current;
    if (
      element.scrollHeight - element.scrollTop - element.clientHeight <
      STICKY_SCROLL_THRESHOLD
    ) {
      element.scrollTop = element.scrollHeight;
    }
  }, [messages, isRunning]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto items justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {status === null && messages.length === 0 && (
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[320px] text-center">
            <img
              src="logo.png"
              width="80%"
              height="auto"
              className="block"
            ></img>
            <h1 className="text-4xl font-bold mb-1">SmolLM2 WebGPU</h1>
            <h2 className="font-semibold">
              A blazingly fast and powerful AI chatbot that runs locally in your
              browser.
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            <p className="max-w-[480px] mb-4">
              <br />
              You are about to load{" "}
              <a
                href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                SmolLM2-1.7B-Instruct
              </a>
              , a 1.7B parameter LLM optimized for in-browser inference.
              Everything runs entirely in your browser with{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning no data is sent to a server. Once
              loaded, it can even be used offline. The source code for the demo
              is available on{" "}
              <a
                href="https://github.com/huggingface/transformers.js-examples/tree/main/smollm-webgpu"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                GitHub
              </a>
              .
            </p>

            {error && (
              <div className="text-red-500 text-center mb-2">
                <p className="mb-1">
                  Unable to load model due to the following error:
                </p>
                <p className="text-sm">{error}</p>
              </div>
            )}

            <button
              className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 disabled:cursor-not-allowed select-none"
              onClick={() => {
                worker.current.postMessage({ type: "load" });
                setStatus("loading");
              }}
              disabled={status !== null || error !== null}
            >
              Load model
            </button>
          </div>
        </div>
      )}
      {status === "loading" && (
        <>
          <div className="w-full max-w-[500px] text-left mx-auto p-4 bottom-0 mt-auto">
            <p className="text-center mb-1">{loadingMessage}</p>
            {progressItems.map(({ file, progress, total }, i) => (
              <Progress
                key={i}
                text={file}
                percentage={progress}
                total={total}
              />
            ))}
          </div>
        </>
      )}

      {status === "ready" && (
        <div
          ref={chatContainerRef}
          className="overflow-y-auto scrollbar-thin w-full flex flex-col items-center h-full"
        >
          <Chat messages={messages} />
          {messages.length === 0 && (
            <div>
              {EXAMPLES.map((msg, i) => (
                <div
                  key={i}
                  className="m-1 border dark:border-gray-600 rounded-md p-2 bg-gray-100 dark:bg-gray-700 cursor-pointer"
                  onClick={() => onEnter(msg)}
                >
                  {msg}
                </div>
              ))}
            </div>
          )}
          <p className="text-center text-sm min-h-6 text-gray-500 dark:text-gray-300">
            {tps && messages.length > 0 && (
              <>
                {!isRunning && (
                  <span>
                    Generated {numTokens} tokens in{" "}
                    {(numTokens / tps).toFixed(2)} seconds&nbsp;&#40;
                  </span>
                )}
                {
                  <>
                    <span className="font-medium text-center mr-1 text-black dark:text-white">
                      {tps.toFixed(2)}
                    </span>
                    <span className="text-gray-500 dark:text-gray-300">
                      tokens/second
                    </span>
                  </>
                }
                {!isRunning && (
                  <>
                    <span className="mr-1">&#41;.</span>
                    <span
                      className="underline cursor-pointer"
                      onClick={() => {
                        worker.current.postMessage({ type: "reset" });
                        setMessages([]);
                      }}
                    >
                      Reset
                    </span>
                  </>
                )}
              </>
            )}
          </p>
        </div>
      )}

      <div className="mt-2 border dark:bg-gray-700 rounded-lg w-[600px] max-w-[80%] max-h-[200px] mx-auto relative mb-3 flex">
        <textarea
          ref={textareaRef}
          className="scrollbar-thin w-[550px] dark:bg-gray-700 px-3 py-4 rounded-lg bg-transparent border-none outline-none text-gray-800 disabled:text-gray-400 dark:text-gray-200 placeholder-gray-500 dark:placeholder-gray-400 disabled:placeholder-gray-200 resize-none disabled:cursor-not-allowed"
          placeholder="Type your message..."
          type="text"
          rows={1}
          value={input}
          disabled={status !== "ready"}
          title={status === "ready" ? "Model is ready" : "Model not loaded yet"}
          onKeyDown={(e) => {
            if (
              input.length > 0 &&
              !isRunning &&
              e.key === "Enter" &&
              !e.shiftKey
            ) {
              e.preventDefault(); // Prevent default behavior of Enter key
              onEnter(input);
            }
          }}
          onInput={(e) => setInput(e.target.value)}
        />
        {isRunning ? (
          <div className="cursor-pointer" onClick={onInterrupt}>
            <StopIcon className="h-8 w-8 p-1 rounded-md text-gray-800 dark:text-gray-100 absolute right-3 bottom-3" />
          </div>
        ) : input.length > 0 ? (
          <div className="cursor-pointer" onClick={() => onEnter(input)}>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-800 dark:bg-gray-100 text-white dark:text-black rounded-md absolute right-3 bottom-3`}
            />
          </div>
        ) : (
          <div>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-200 dark:bg-gray-600 text-gray-50 dark:text-gray-800 rounded-md absolute right-3 bottom-3`}
            />
          </div>
        )}
      </div>

      <p className="text-xs text-gray-400 text-center mb-3">
        Disclaimer: Generated content may be inaccurate or false.
      </p>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\smollm-webgpu\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;

@layer utilities {
  .scrollbar-thin::-webkit-scrollbar {
    @apply w-2;
  }

  .scrollbar-thin::-webkit-scrollbar-track {
    @apply rounded-full bg-gray-100 dark:bg-gray-700;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb {
    @apply rounded-full bg-gray-300 dark:bg-gray-600;
  }

  .scrollbar-thin::-webkit-scrollbar-thumb:hover {
    @apply bg-gray-500;
  }

  .animation-delay-200 {
    animation-delay: 200ms;
  }
  .animation-delay-400 {
    animation-delay: 400ms;
  }

  .overflow-wrap-anywhere {
    overflow-wrap: anywhere;
  }
}


----- .\smollm-webgpu\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\smollm-webgpu\src\worker.js -----

import {
  AutoTokenizer,
  AutoModelForCausalLM,
  TextStreamer,
  InterruptableStoppingCriteria,
} from "@huggingface/transformers";

/**
 * Helper function to perform feature detection for WebGPU
 */
// let fp16_supported = false;
async function check() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) {
      throw new Error("WebGPU is not supported (no adapter found)");
    }
    // fp16_supported = adapter.features.has("shader-f16")
  } catch (e) {
    self.postMessage({
      status: "error",
      data: e.toString(),
    });
  }
}

/**
 * This class uses the Singleton pattern to enable lazy-loading of the pipeline
 */
class TextGenerationPipeline {
  static model_id = "HuggingFaceTB/SmolLM2-1.7B-Instruct";

  static async getInstance(progress_callback = null) {
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= AutoModelForCausalLM.from_pretrained(this.model_id, {
      dtype: "q4f16", // TODO: use "q4" as fallback when fixed
      device: "webgpu",
      progress_callback,
    });

    return Promise.all([this.tokenizer, this.model]);
  }
}

const stopping_criteria = new InterruptableStoppingCriteria();

let past_key_values_cache = null;
async function generate(messages) {
  // Retrieve the text-generation pipeline.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance();

  const inputs = tokenizer.apply_chat_template(messages, {
    add_generation_prompt: true,
    return_dict: true,
  });

  let startTime;
  let numTokens = 0;
  let tps;
  const token_callback_function = () => {
    startTime ??= performance.now();

    if (numTokens++ > 0) {
      tps = (numTokens / (performance.now() - startTime)) * 1000;
    }
  };
  const callback_function = (output) => {
    self.postMessage({
      status: "update",
      output,
      tps,
      numTokens,
    });
  };

  const streamer = new TextStreamer(tokenizer, {
    skip_prompt: true,
    skip_special_tokens: true,
    callback_function,
    token_callback_function,
  });

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  const { past_key_values, sequences } = await model.generate({
    ...inputs,
    // TODO: Add back when fixed
    // past_key_values: past_key_values_cache,

    // Sampling
    // do_sample: true,
    // top_k: 3,
    // temperature: 0.2,

    max_new_tokens: 1024,
    streamer,
    stopping_criteria,
    return_dict_in_generate: true,
  });
  past_key_values_cache = past_key_values;

  const decoded = tokenizer.batch_decode(sequences, {
    skip_special_tokens: true,
  });

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: decoded,
  });
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  self.postMessage({
    status: "loading",
    data: "Compiling shaders and warming up model...",
  });

  // Run model with dummy input to compile shaders
  const inputs = tokenizer("a");
  await model.generate({ ...inputs, max_new_tokens: 1 });
  self.postMessage({ status: "ready" });
}
// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "check":
      check();
      break;

    case "load":
      load();
      break;

    case "generate":
      stopping_criteria.reset();
      generate(data);
      break;

    case "interrupt":
      stopping_criteria.interrupt();
      break;

    case "reset":
      past_key_values_cache = null;
      stopping_criteria.reset();
      break;
  }
});


----- .\smollm-webgpu\src\components\Chat.css -----

@scope (.markdown) {
  /* Code blocks */
  pre {
    margin: 0.5rem 0;
    white-space: break-spaces;
  }

  code {
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
    font-size: 0.9em;
  }

  pre,
  code {
    background-color: #f2f2f2;
  }

  @media (prefers-color-scheme: dark) {
    pre,
    code {
      background-color: #333;
    }
  }

  pre:has(code) {
    padding: 1rem 0.5rem;
  }

  pre > code {
    padding: 0;
  }

  /* Headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-weight: 600;
    line-height: 1.2;
  }

  h1 {
    font-size: 2em;
    margin: 1rem 0;
  }

  h2 {
    font-size: 1.5em;
    margin: 0.83rem 0;
  }

  h3 {
    font-size: 1.25em;
    margin: 0.67rem 0;
  }

  h4 {
    font-size: 1em;
    margin: 0.5rem 0;
  }

  h5 {
    font-size: 0.875em;
    margin: 0.33rem 0;
  }

  h6 {
    font-size: 0.75em;
    margin: 0.25rem 0;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6:first-child {
    margin-top: 0;
  }

  /* Unordered List */
  ul {
    list-style-type: disc;
    margin-left: 1.5rem;
  }

  /* Ordered List */
  ol {
    list-style-type: decimal;
    margin-left: 1.5rem;
  }

  /* List Items */
  li {
    margin: 0.25rem 0;
  }

  p:not(:first-child) {
    margin-top: 0.75rem;
  }

  p:not(:last-child) {
    margin-bottom: 0.75rem;
  }

  ul > li {
    margin-left: 1rem;
  }

  /* Table */
  table,
  th,
  td {
    border: 1px solid lightgray;
    padding: 0.25rem;
  }

  @media (prefers-color-scheme: dark) {
    table,
    th,
    td {
      border: 1px solid #f2f2f2;
    }
  }
}


----- .\smollm-webgpu\src\components\Chat.jsx -----

import { marked } from "marked";
import DOMPurify from "dompurify";

import BotIcon from "./icons/BotIcon";
import UserIcon from "./icons/UserIcon";

import "./Chat.css";
import { useEffect } from "react";

function render(text) {
  return DOMPurify.sanitize(marked.parse(text));
}

export default function Chat({ messages }) {
  const empty = messages.length === 0;

  useEffect(() => {
    window.MathJax.typeset();
  }, [messages]);

  return (
    <div
      className={`flex-1 p-6 max-w-[960px] w-full ${empty ? "flex flex-col items-center justify-end" : "space-y-4"}`}
    >
      {empty ? (
        <div className="text-xl">Ready!</div>
      ) : (
        messages.map((msg, i) => (
          <div key={`message-${i}`} className="flex items-start space-x-4">
            {msg.role === "assistant" ? (
              <>
                <BotIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-gray-200 dark:bg-gray-700 rounded-lg p-4">
                  <p className="min-h-6 text-gray-800 dark:text-gray-200 overflow-wrap-anywhere">
                    {msg.content.length > 0 ? (
                      <span
                        className="markdown"
                        dangerouslySetInnerHTML={{
                          __html: render(msg.content),
                        }}
                      />
                    ) : (
                      <span className="h-6 flex items-center gap-1">
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-200"></span>
                        <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-400"></span>
                      </span>
                    )}
                  </p>
                </div>
              </>
            ) : (
              <>
                <UserIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
                <div className="bg-blue-500 text-white rounded-lg p-4">
                  <p className="min-h-6 overflow-wrap-anywhere">
                    {msg.content}
                  </p>
                </div>
              </>
            )}
          </div>
        ))
      )}
    </div>
  );
}


----- .\smollm-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\smollm-webgpu\src\components\icons\ArrowRightIcon.jsx -----

export default function ArrowRightIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M5 12h14" />
      <path d="m12 5 7 7-7 7" />
    </svg>
  );
}


----- .\smollm-webgpu\src\components\icons\BotIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M12 8V4H8" />
      <rect width="16" height="12" x="4" y="8" rx="2" />
      <path d="M2 14h2" />
      <path d="M20 14h2" />
      <path d="M15 13v2" />
      <path d="M9 13v2" />
    </svg>
  );
}


----- .\smollm-webgpu\src\components\icons\StopIcon.jsx -----

export default function StopIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
      <path
        fill="currentColor"
        d="M9 9.563C9 9.252 9.252 9 9.563 9h4.874c.311 0 .563.252.563.563v4.874c0 .311-.252.563-.563.563H9.564A.562.562 0 0 1 9 14.437V9.564Z"
      />
    </svg>
  );
}


----- .\smollm-webgpu\src\components\icons\UserIcon.jsx -----

export default function UserIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2" />
      <circle cx="12" cy="7" r="4" />
    </svg>
  );
}


----- .\smolvlm-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import Chat from "./components/Chat";
import ArrowRightIcon from "./components/icons/ArrowRightIcon";
import StopIcon from "./components/icons/StopIcon";
import Progress from "./components/Progress";
import ImageIcon from "./components/icons/ImageIcon";
import ImagePreview from "./components/ImagePreview";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;
const STICKY_SCROLL_THRESHOLD = 120;
const EXAMPLES = [
  {
    title: "Describe this image",
    text: "Can you describe this image?",
    images: [
      "https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/new-york.jpg",
    ],
  },
  {
    title: "Handwriting recognition",
    text: "What does this say?",
    images: [
      "https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/handwritten-math.jpg",
    ],
  },
  {
    title: "Chart analysis",
    text: "Where do the severe droughts happen according to this diagram?",
    images: [
      "https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/weather-events-diagram.png",
    ],
  },
];

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const textareaRef = useRef(null);
  const chatContainerRef = useRef(null);
  const imageUploadRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);
  const [isRunning, setIsRunning] = useState(false);

  // Inputs and outputs
  const [input, setInput] = useState("");
  const [images, setImages] = useState([]);
  const [messages, setMessages] = useState([]);
  const [tps, setTps] = useState(null);
  const [numTokens, setNumTokens] = useState(null);

  function onEnter(message, images) {
    const content = [
      ...images.map((image) => ({ type: "image", image })),
      { type: "text", text: message },
    ];
    setMessages((prev) => [...prev, { role: "user", content }]);
    setTps(null);
    setIsRunning(true);
    setInput("");
    setImages([]);
  }

  function onInterrupt() {
    // NOTE: We do not set isRunning to false here because the worker
    // will send a 'complete' message when it is done.
    worker.current.postMessage({ type: "interrupt" });
  }

  useEffect(() => {
    resizeInput();
  }, [input]);

  function resizeInput() {
    if (!textareaRef.current) return;

    const target = textareaRef.current;
    target.style.height = "auto";
    const newHeight = Math.min(Math.max(target.scrollHeight, 24), 200);
    target.style.height = `${newHeight}px`;
  }

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    if (!worker.current) {
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
      worker.current.postMessage({ type: "check" }); // Do a feature check
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "start":
          {
            // Start generation
            setMessages((prev) => [
              ...prev,
              { role: "assistant", content: [{ type: "text", text: "" }] },
            ]);
          }
          break;

        case "update":
          {
            // Generation update: update the output text.
            // Parse messages
            const { output, tps, numTokens } = e.data;
            setTps(tps);
            setNumTokens(numTokens);
            setMessages((prev) => {
              const cloned = [...prev];
              const last = cloned.at(-1);
              cloned[cloned.length - 1] = {
                ...last,
                content: [
                  { type: "text", text: last.content[0].text + output },
                ],
              };
              return cloned;
            });
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsRunning(false);
          break;

        case "error":
          setError(e.data.data);
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Listen for window resizes
    window.addEventListener("resize", resizeInput);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
      window.removeEventListener("resize", resizeInput);
    };
  }, []);

  // Send the messages to the worker thread whenever the `messages` state changes.
  useEffect(() => {
    if (messages.filter((x) => x.role === "user").length === 0) {
      // No user messages yet: do nothing.
      return;
    }
    if (messages.at(-1).role === "assistant") {
      // Do not update if the last message is from the assistant
      return;
    }
    setTps(null);
    worker.current.postMessage({ type: "generate", data: messages });
  }, [messages, isRunning]);

  useEffect(() => {
    if (!chatContainerRef.current || !isRunning) return;
    const element = chatContainerRef.current;
    if (
      element.scrollHeight - element.scrollTop - element.clientHeight <
      STICKY_SCROLL_THRESHOLD
    ) {
      element.scrollTop = element.scrollHeight;
    }
  }, [messages, isRunning]);

  const validInput = input.length > 0 && images.length > 0;

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto items justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {status === null && messages.length === 0 && (
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[360px] text-center">
            <img
              src="logo.png"
              width="100%"
              height="auto"
              className="block drop-shadow-lg bg-transparent"
            ></img>
            <h1 className="text-4xl font-bold mb-1">SmolVLM WebGPU</h1>
            <h2 className="font-semibold">
              The smallest multimodal model in the world.
              <br />
              Designed for efficiency.
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            <p className="max-w-[500px] mb-4">
              <br />
              You are about to load{" "}
              <a
                href="https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                SmolVLM-256M-Instruct
              </a>
              , a 256M parameter multimodal model optimized for in-browser
              inference. Everything runs entirely in your browser with{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning no data is sent to a server. Once
              loaded, it can even be used offline. The source code for the demo
              is available on{" "}
              <a
                href="https://github.com/huggingface/transformers.js-examples/tree/main/smolvlm-webgpu"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                GitHub
              </a>
              .
            </p>

            {error && (
              <div className="text-red-500 text-center mb-2">
                <p className="mb-1">
                  Unable to load model due to the following error:
                </p>
                <p className="text-sm">{error}</p>
              </div>
            )}

            <button
              className="border border-gray-300 dark:border-gray-600 px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 cursor-pointer disabled:cursor-not-allowed select-none"
              onClick={() => {
                worker.current.postMessage({ type: "load" });
                setStatus("loading");
              }}
              disabled={status !== null || error !== null}
            >
              Load model
            </button>
          </div>
        </div>
      )}
      {status === "loading" && (
        <>
          <div className="w-full max-w-[500px] text-left mx-auto p-4 bottom-0 mt-auto">
            <p className="text-center mb-1">{loadingMessage}</p>
            {progressItems.map(({ file, progress, total }, i) => (
              <Progress
                key={i}
                text={file}
                percentage={progress}
                total={total}
              />
            ))}
          </div>
        </>
      )}

      {status === "ready" && (
        <div
          ref={chatContainerRef}
          className="overflow-y-auto scrollbar-thin w-full flex flex-col items-center h-full"
        >
          <Chat messages={messages} />
          {messages.length === 0 && (
            <div className="flex">
              {EXAMPLES.map((msg, i) => (
                <div
                  key={i}
                  className="max-w-[250px] m-1 border border-gray-300 dark:border-gray-600 rounded-md p-2 bg-gray-100 dark:bg-gray-700 cursor-pointer"
                  onClick={() => onEnter(msg.text, msg.images)}
                >
                  {msg.text}
                  {msg.images.map((src, j) => (
                    <img
                      key={j}
                      src={src}
                      className="w-full h-auto mt-2"
                      alt="Example"
                    />
                  ))}
                </div>
              ))}
            </div>
          )}
          <p className="text-center text-sm min-h-6 text-gray-500 dark:text-gray-300">
            {tps && messages.length > 0 && (
              <>
                {!isRunning && (
                  <span>
                    Generated {numTokens} tokens in{" "}
                    {(numTokens / tps).toFixed(2)} seconds&nbsp;&#40;
                  </span>
                )}
                {
                  <>
                    <span className="font-medium text-center mr-1 text-black dark:text-white">
                      {tps.toFixed(2)}
                    </span>
                    <span className="text-gray-500 dark:text-gray-300">
                      tokens/second
                    </span>
                  </>
                }
                {!isRunning && (
                  <>
                    <span className="mr-1">&#41;.</span>
                    <span
                      className="underline cursor-pointer"
                      onClick={() => {
                        worker.current.postMessage({ type: "reset" });
                        setMessages([]);
                      }}
                    >
                      Reset
                    </span>
                  </>
                )}
              </>
            )}
          </p>
        </div>
      )}

      <div className="mt-2 border border-gray-300 dark:bg-gray-700 rounded-lg w-[600px] max-w-[80%] max-h-[200px] mx-auto relative mb-3 flex">
        <label
          htmlFor="file-upload"
          className={
            status === "ready"
              ? "cursor-pointer"
              : "cursor-not-allowed pointer-events-none"
          }
        >
          <ImageIcon
            className={`h-8 w-8 p-1 rounded-md ${status === "ready" ? "text-gray-800 dark:text-gray-100" : "text-gray-400 dark:text-gray-500"} absolute bottom-3 left-1.5`}
          ></ImageIcon>
          <input
            ref={imageUploadRef}
            id="file-upload"
            type="file"
            accept="image/*"
            className="hidden"
            multiple
            onInput={async (e) => {
              const files = Array.from(e.target.files);
              if (files.length === 0) {
                return;
              }

              const readers = files.map((file) => {
                return new Promise((resolve, reject) => {
                  const reader = new FileReader();
                  reader.onload = (e2) => resolve(e2.target.result);
                  reader.onerror = reject;
                  reader.readAsDataURL(file);
                });
              });

              const results = await Promise.all(readers);
              setImages((prev) => [...prev, ...results]);
              e.target.value = "";
            }}
          ></input>
        </label>
        <div className="w-full flex flex-col">
          <div className="flex w-full">
            {images.map((src, i) => (
              <ImagePreview
                key={i}
                onRemove={() => {
                  setImages((prev) => prev.filter((_, j) => j !== i));
                }}
                src={src}
                className="w-20 h-20 min-w-20 min-h-20 relative p-2"
              />
            ))}
          </div>

          <textarea
            ref={textareaRef}
            className="scrollbar-thin w-full pl-11 pr-12 dark:bg-gray-700 py-4 rounded-lg bg-transparent border-none outline-none text-gray-800 disabled:text-gray-400 dark:text-gray-100 placeholder-gray-500 disabled:placeholder-gray-200 dark:placeholder-gray-300 dark:disabled:placeholder-gray-500 resize-none disabled:cursor-not-allowed"
            placeholder="Select at least one image and enter message here..."
            type="text"
            rows={1}
            value={input}
            disabled={status !== "ready"}
            title={
              status === "ready" ? "Model is ready" : "Model not loaded yet"
            }
            onKeyDown={(e) => {
              if (
                validInput &&
                !isRunning &&
                e.key === "Enter" &&
                !e.shiftKey
              ) {
                e.preventDefault(); // Prevent default behavior of Enter key
                onEnter(input, images);
              }
            }}
            onInput={(e) => setInput(e.target.value)}
          />
        </div>
        {isRunning ? (
          <div className="cursor-pointer" onClick={onInterrupt}>
            <StopIcon className="h-8 w-8 p-1 rounded-md text-gray-800 dark:text-gray-100 absolute right-3 bottom-3" />
          </div>
        ) : validInput ? (
          <div
            className="cursor-pointer"
            onClick={() => onEnter(input, images)}
          >
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-800 dark:bg-gray-100 text-white dark:text-black rounded-md absolute right-3 bottom-3`}
            />
          </div>
        ) : (
          <div>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-200 dark:bg-gray-600 text-gray-50 dark:text-gray-800 rounded-md absolute right-3 bottom-3`}
            />
          </div>
        )}
      </div>

      <p className="text-xs text-gray-400 text-center mb-3">
        Disclaimer: Generated content may be inaccurate or false.
      </p>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\smolvlm-webgpu\src\index.css -----

@import "tailwindcss";

/* Custom scrollbar styles */
.scrollbar-thin::-webkit-scrollbar {
  width: 0.5rem; /* Equivalent to w-2 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-track {
  border-radius: 9999px; /* Equivalent to rounded-full in Tailwind */
  background-color: #f3f4f6; /* Equivalent to bg-gray-100 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-track.dark {
  background-color: #374151; /* Equivalent to dark:bg-gray-700 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-thumb {
  border-radius: 9999px; /* Equivalent to rounded-full in Tailwind */
  background-color: #d1d5db; /* Equivalent to bg-gray-300 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-thumb:hover {
  background-color: #6b7280; /* Equivalent to bg-gray-500 in Tailwind */
}

/* Animation delay classes */
.animation-delay-200 {
  animation-delay: 200ms;
}

.animation-delay-400 {
  animation-delay: 400ms;
}

/* Overflow wrap class */
.overflow-wrap-anywhere {
  overflow-wrap: anywhere;
}


----- .\smolvlm-webgpu\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import "./index.css";
import App from "./App.jsx";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\smolvlm-webgpu\src\worker.js -----

import {
  AutoProcessor,
  AutoModelForVision2Seq,
  TextStreamer,
  InterruptableStoppingCriteria,
  load_image,
} from "@huggingface/transformers";

const MAX_NEW_TOKENS = 1024;

/**
 * Helper function to perform feature detection for WebGPU
 */
let fp16_supported = false;
async function check() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) {
      throw new Error("WebGPU is not supported (no adapter found)");
    }
    fp16_supported = adapter.features.has("shader-f16");
  } catch (e) {
    self.postMessage({
      status: "error",
      data: e.toString(),
    });
  }
}

/**
 * This class uses the Singleton pattern to enable lazy-loading of the pipeline
 */
class SmolVLM {
  static model_id = "HuggingFaceTB/SmolVLM-256M-Instruct";

  static async getInstance(progress_callback = null) {
    this.processor ??= AutoProcessor.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= AutoModelForVision2Seq.from_pretrained(this.model_id, {
      dtype: "fp32",
      device: "webgpu",
      progress_callback,
    });

    return Promise.all([this.processor, this.model]);
  }
}

const stopping_criteria = new InterruptableStoppingCriteria();

let past_key_values_cache = null;
async function generate(messages) {
  // For this demo, we only respond to the last message
  messages = messages.slice(-1);

  // Retrieve the text-generation pipeline.
  const [processor, model] = await SmolVLM.getInstance();

  // Load all images
  const images = await Promise.all(
    messages
      .map((x) => x.content)
      .flat(Infinity)
      .filter((msg) => msg.image !== undefined)
      .map((msg) => load_image(msg.image)),
  );

  // Prepare inputs
  const text = processor.apply_chat_template(messages, {
    add_generation_prompt: true,
  });
  const inputs = await processor(text, images, {
    // Set `do_image_splitting: true` to split images into multiple patches.
    // NOTE: This uses more memory, but can provide more accurate results.
    // do_image_splitting: false,
  });

  let startTime;
  let numTokens = 0;
  let tps;
  const token_callback_function = (tokens) => {
    startTime ??= performance.now();

    if (numTokens++ > 0) {
      tps = (numTokens / (performance.now() - startTime)) * 1000;
    }
  };
  const callback_function = (output) => {
    self.postMessage({
      status: "update",
      output,
      tps,
      numTokens,
    });
  };

  const streamer = new TextStreamer(processor.tokenizer, {
    skip_prompt: true,
    skip_special_tokens: true,
    callback_function,
    token_callback_function,
  });

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  const { past_key_values, sequences } = await model
    .generate({
      ...inputs,
      // TODO: Add back when fixed
      // past_key_values: past_key_values_cache,

      // Sampling
      do_sample: false,
      repetition_penalty: 1.1,
      // top_k: 3,
      // temperature: 0.2,

      max_new_tokens: MAX_NEW_TOKENS,
      streamer,
      stopping_criteria,
      return_dict_in_generate: true,
    })
    .catch((e) => {
      self.postMessage({
        status: "error",
        data: e.toString(),
      });
    });
  past_key_values_cache = past_key_values;

  const decoded = processor.batch_decode(sequences, {
    skip_special_tokens: true,
  });

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: decoded,
  });
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [processor, model] = await SmolVLM.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  self.postMessage({ status: "ready" });
}
// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "check":
      check();
      break;

    case "load":
      load();
      break;

    case "generate":
      stopping_criteria.reset();
      generate(data);
      break;

    case "interrupt":
      stopping_criteria.interrupt();
      break;

    case "reset":
      past_key_values_cache = null;
      stopping_criteria.reset();
      break;
  }
});


----- .\smolvlm-webgpu\src\components\Chat.css -----

@scope (.markdown) {
  /* Code blocks */
  pre {
    margin: 0.5rem 0;
    white-space: break-spaces;
  }

  code {
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
    font-size: 0.9em;
  }

  pre,
  code {
    background-color: #f2f2f2;
  }

  @media (prefers-color-scheme: dark) {
    pre,
    code {
      background-color: #333;
    }
  }

  pre:has(code) {
    padding: 1rem 0.5rem;
  }

  pre > code {
    padding: 0;
  }

  /* Headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-weight: 600;
    line-height: 1.2;
  }

  h1 {
    font-size: 2em;
    margin: 1rem 0;
  }

  h2 {
    font-size: 1.5em;
    margin: 0.83rem 0;
  }

  h3 {
    font-size: 1.25em;
    margin: 0.67rem 0;
  }

  h4 {
    font-size: 1em;
    margin: 0.5rem 0;
  }

  h5 {
    font-size: 0.875em;
    margin: 0.33rem 0;
  }

  h6 {
    font-size: 0.75em;
    margin: 0.25rem 0;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6:first-child {
    margin-top: 0;
  }

  /* Unordered List */
  ul {
    list-style-type: disc;
    margin-left: 1.5rem;
  }

  /* Ordered List */
  ol {
    list-style-type: decimal;
    margin-left: 1.5rem;
  }

  /* List Items */
  li {
    margin: 0.25rem 0;
  }

  p:not(:first-child) {
    margin-top: 0.75rem;
  }

  p:not(:last-child) {
    margin-bottom: 0.75rem;
  }

  ul > li {
    margin-left: 1rem;
  }

  /* Table */
  table,
  th,
  td {
    border: 1px solid lightgray;
    padding: 0.25rem;
  }

  @media (prefers-color-scheme: dark) {
    table,
    th,
    td {
      border: 1px solid #f2f2f2;
    }
  }
}


----- .\smolvlm-webgpu\src\components\Chat.jsx -----

import { marked } from "marked";
import DOMPurify from "dompurify";

import BotIcon from "./icons/BotIcon";
import UserIcon from "./icons/UserIcon";

import { MathJaxContext, MathJax } from "better-react-mathjax";
import "./Chat.css";

function render(text) {
  // Replace all instances of single backslashes before brackets with double backslashes
  // See https://github.com/markedjs/marked/issues/546 for more information.
  text = text.replace(/\\([\[\]\(\)])/g, "\\\\$1");

  const result = DOMPurify.sanitize(
    marked.parse(text, {
      async: false,
      breaks: true,
    }),
  );
  return result;
}
function Message({ role, content }) {
  const isAssistant = role === "assistant";
  const Icon = isAssistant ? BotIcon : UserIcon;

  return (
    <div className="flex items-start space-x-4">
      <Icon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
      <div
        className={`rounded-lg p-4 ${isAssistant ? "bg-gray-200 dark:bg-gray-700" : "bg-blue-500 text-white"}`}
      >
        <div className="flex flex-col min-h-6 overflow-wrap-anywhere gap-2">
          {content.map((msg, i) =>
            msg.type === "text" ? (
              msg.text.length > 0 ? (
                <MathJax dynamic key={i}>
                  <span
                    className="markdown"
                    dangerouslySetInnerHTML={{
                      __html: render(msg.text),
                    }}
                  />
                </MathJax>
              ) : (
                <span key={i} className="h-6 flex items-center gap-1">
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse"></span>
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-200"></span>
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-400"></span>
                </span>
              )
            ) : msg.type === "image" ? (
              <img
                key={i}
                src={msg.image}
                className="w-full max-w-[384px] h-auto max-h-[300px] object-contain rounded-md"
              />
            ) : null,
          )}
        </div>
      </div>
    </div>
  );
}

export default function Chat({ messages }) {
  const empty = messages.length === 0;

  return (
    <div
      className={`flex-1 p-6 max-w-[960px] w-full ${empty ? "flex flex-col items-center justify-end" : "space-y-4"}`}
    >
      <MathJaxContext>
        {empty ? (
          <div className="text-xl">Ready!</div>
        ) : (
          messages.map((msg, i) => <Message key={`message-${i}`} {...msg} />)
        )}
      </MathJaxContext>
    </div>
  );
}


----- .\smolvlm-webgpu\src\components\ImagePreview.jsx -----

import { useState } from "react";
import CrossIcon from "./icons/CrossIcon";

export default function ImagePreview({ src, onRemove, ...props }) {
  const [hover, setHover] = useState(false);

  return (
    <div
      {...props}
      onMouseEnter={() => setHover(true)}
      onMouseLeave={() => setHover(false)}
    >
      <CrossIcon
        onClick={onRemove}
        className={`absolute top-0 right-0 cursor-pointer dark:fill-gray-400 dark:text-gray-100 fill-gray-200 text-gray-800 ${hover ? "" : "hidden"}`}
      />
      <img
        src={src}
        alt="Upload preview"
        className="w-full h-full object-cover rounded-md"
      />
    </div>
  );
}


----- .\smolvlm-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\smolvlm-webgpu\src\components\icons\ArrowRightIcon.jsx -----

export default function ArrowRightIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M5 12h14" />
      <path d="m12 5 7 7-7 7" />
    </svg>
  );
}


----- .\smolvlm-webgpu\src\components\icons\BotIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M12 8V4H8" />
      <rect width="16" height="12" x="4" y="8" rx="2" />
      <path d="M2 14h2" />
      <path d="M20 14h2" />
      <path d="M15 13v2" />
      <path d="M9 13v2" />
    </svg>
  );
}


----- .\smolvlm-webgpu\src\components\icons\CrossIcon.jsx -----

export default function CrossIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="m9.75 9.75 4.5 4.5m0-4.5-4.5 4.5M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
    </svg>
  );
}


----- .\smolvlm-webgpu\src\components\icons\ImageIcon.jsx -----

export default function ImageIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="m2.25 15.75 5.159-5.159a2.25 2.25 0 0 1 3.182 0l5.159 5.159m-1.5-1.5 1.409-1.409a2.25 2.25 0 0 1 3.182 0l2.909 2.909m-18 3.75h16.5a1.5 1.5 0 0 0 1.5-1.5V6a1.5 1.5 0 0 0-1.5-1.5H3.75A1.5 1.5 0 0 0 2.25 6v12a1.5 1.5 0 0 0 1.5 1.5Zm10.5-11.25h.008v.008h-.008V8.25Zm.375 0a.375.375 0 1 1-.75 0 .375.375 0 0 1 .75 0Z" />
    </svg>
  );
}


----- .\smolvlm-webgpu\src\components\icons\StopIcon.jsx -----

export default function StopIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
      <path
        fill="currentColor"
        d="M9 9.563C9 9.252 9.252 9 9.563 9h4.874c.311 0 .563.252.563.563v4.874c0 .311-.252.563-.563.563H9.564A.562.562 0 0 1 9 14.437V9.564Z"
      />
    </svg>
  );
}


----- .\smolvlm-webgpu\src\components\icons\UserIcon.jsx -----

export default function UserIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2" />
      <circle cx="12" cy="7" r="4" />
    </svg>
  );
}


----- .\speecht5-web\src\App.jsx -----

import { useState, useEffect, useRef } from "react";

import AudioPlayer from "./components/AudioPlayer";
import Progress from "./components/Progress";
import { SPEAKERS, DEFAULT_SPEAKER } from "./constants";

const App = () => {
  // Model loading
  const [ready, setReady] = useState(null);
  const [disabled, setDisabled] = useState(false);
  const [progressItems, setProgressItems] = useState([]);

  // Inputs and outputs
  const [text, setText] = useState("I love Hugging Face!");
  const [selectedSpeaker, setSelectedSpeaker] = useState(DEFAULT_SPEAKER);
  const [output, setOutput] = useState(null);

  // Create a reference to the worker object.
  const worker = useRef(null);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    if (!worker.current) {
      // Create the worker if it does not yet exist.
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "initiate":
          // Model file start load: add a new progress item to the list.
          setReady(false);
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, progress: e.data.progress };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setReady(true);
          break;

        case "complete":
          // Generation complete: re-enable the "Translate" button
          setDisabled(false);

          const blobUrl = URL.createObjectURL(e.data.output);
          setOutput(blobUrl);
          break;
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () =>
      worker.current.removeEventListener("message", onMessageReceived);
  });

  const handleGenerateSpeech = () => {
    setDisabled(true);
    worker.current.postMessage({
      text,
      speaker_id: selectedSpeaker,
    });
  };

  const isLoading = ready === false;
  return (
    <div className="min-h-screen flex items-center justify-center bg-gray-100">
      <div
        className="absolute gap-1 z-50 top-0 left-0 w-full h-full transition-all px-8 flex flex-col justify-center text-center"
        style={{
          opacity: isLoading ? 1 : 0,
          pointerEvents: isLoading ? "all" : "none",
          background: "rgba(0, 0, 0, 0.9)",
          backdropFilter: "blur(8px)",
        }}
      >
        {isLoading && (
          <label className="text-white text-xl p-3">
            Loading models... (only run once)
          </label>
        )}
        {progressItems.map((data) => (
          <div key={`${data.name}/${data.file}`}>
            <Progress
              text={`${data.name}/${data.file}`}
              percentage={data.progress}
            />
          </div>
        ))}
      </div>
      <div className="bg-white p-8 rounded-lg shadow-lg w-full max-w-xl m-2">
        <h1 className="text-3xl font-semibold text-gray-800 mb-1 text-center">
          In-browser Text to Speech
        </h1>
        <h2 className="text-base font-medium text-gray-700 mb-2 text-center">
          Made with{" "}
          <a
            href="https://huggingface.co/docs/transformers.js"
            target="_blank"
            rel="noreferrer"
          >
            ðŸ¤— Transformers.js
          </a>
        </h2>
        <div className="mb-4">
          <label
            htmlFor="text"
            className="block text-sm font-medium text-gray-600"
          >
            Text
          </label>
          <textarea
            id="text"
            className="border border-gray-300 rounded-md p-2 w-full"
            rows="4"
            placeholder="Enter text here"
            value={text}
            onChange={(e) => setText(e.target.value)}
          ></textarea>
        </div>
        <div className="mb-4">
          <label
            htmlFor="speaker"
            className="block text-sm font-medium text-gray-600"
          >
            Speaker
          </label>
          <select
            id="speaker"
            className="border border-gray-300 rounded-md p-2 w-full"
            value={selectedSpeaker}
            onChange={(e) => setSelectedSpeaker(e.target.value)}
          >
            {Object.entries(SPEAKERS).map(([key, value]) => (
              <option key={key} value={value}>
                {key}
              </option>
            ))}
          </select>
        </div>
        <div className="flex justify-center">
          <button
            className={`${
              disabled
                ? "bg-gray-400 cursor-not-allowed"
                : "bg-blue-500 cursor-pointer hover:bg-blue-600"
            } text-white rounded-md py-2 px-4`}
            onClick={handleGenerateSpeech}
            disabled={disabled}
          >
            {disabled ? "Generating..." : "Generate"}
          </button>
        </div>
        {output && <AudioPlayer audioUrl={output} mimeType="audio/wav" />}
      </div>
    </div>
  );
};

export default App;


----- .\speecht5-web\src\constants.js -----

export const SPEAKERS = {
  "US female 1": "cmu_us_slt_arctic-wav-arctic_a0001",
  "US female 2": "cmu_us_clb_arctic-wav-arctic_a0001",
  "US male 1": "cmu_us_bdl_arctic-wav-arctic_a0003",
  "US male 2": "cmu_us_rms_arctic-wav-arctic_a0003",
  "Canadian male": "cmu_us_jmk_arctic-wav-arctic_a0002",
  "Scottish male": "cmu_us_awb_arctic-wav-arctic_b0002",
  "Indian male": "cmu_us_ksp_arctic-wav-arctic_a0007",
};

export const DEFAULT_SPEAKER = "cmu_us_slt_arctic-wav-arctic_a0001";


----- .\speecht5-web\src\index.css -----

@import "tailwindcss";

:root {
  font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;
  color: #213547;
  background-color: #ffffff;

  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-text-size-adjust: 100%;
}

audio::-webkit-media-controls-panel {
  background-color: white;
}


----- .\speecht5-web\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\speecht5-web\src\utils.js -----

// Adapted from https://www.npmjs.com/package/audiobuffer-to-wav

export function encodeWAV(samples) {
  let offset = 44;
  const buffer = new ArrayBuffer(offset + samples.length * 4);
  const view = new DataView(buffer);
  const sampleRate = 16000;

  /* RIFF identifier */
  writeString(view, 0, "RIFF");
  /* RIFF chunk length */
  view.setUint32(4, 36 + samples.length * 4, true);
  /* RIFF type */
  writeString(view, 8, "WAVE");
  /* format chunk identifier */
  writeString(view, 12, "fmt ");
  /* format chunk length */
  view.setUint32(16, 16, true);
  /* sample format (raw) */
  view.setUint16(20, 3, true);
  /* channel count */
  view.setUint16(22, 1, true);
  /* sample rate */
  view.setUint32(24, sampleRate, true);
  /* byte rate (sample rate * block align) */
  view.setUint32(28, sampleRate * 4, true);
  /* block align (channel count * bytes per sample) */
  view.setUint16(32, 4, true);
  /* bits per sample */
  view.setUint16(34, 32, true);
  /* data chunk identifier */
  writeString(view, 36, "data");
  /* data chunk length */
  view.setUint32(40, samples.length * 4, true);

  for (let i = 0; i < samples.length; ++i, offset += 4) {
    view.setFloat32(offset, samples[i], true);
  }

  return buffer;
}

function writeString(view, offset, string) {
  for (let i = 0; i < string.length; ++i) {
    view.setUint8(offset + i, string.charCodeAt(i));
  }
}


----- .\speecht5-web\src\worker.js -----

import {
  Tensor,
  AutoTokenizer,
  SpeechT5ForTextToSpeech,
  SpeechT5HifiGan,
} from "@huggingface/transformers";
import { encodeWAV } from "./utils";

// Use the Singleton pattern to enable lazy construction of the pipeline.
class MyTextToSpeechPipeline {
  static BASE_URL =
    "https://huggingface.co/datasets/Xenova/cmu-arctic-xvectors-extracted/resolve/main/";

  static model_id = "Xenova/speecht5_tts";
  static vocoder_id = "Xenova/speecht5_hifigan";

  static tokenizer_instance = null;
  static model_instance = null;
  static vocoder_instance = null;

  static async getInstance(progress_callback = null) {
    this.tokenizer_instance ??= AutoTokenizer.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model_instance ??= SpeechT5ForTextToSpeech.from_pretrained(
      this.model_id,
      {
        dtype: "fp32",
        progress_callback,
      },
    );

    this.vocoder_instance ??= SpeechT5HifiGan.from_pretrained(this.vocoder_id, {
      dtype: "fp32",
      progress_callback,
    });

    return new Promise(async (resolve, reject) => {
      const result = await Promise.all([
        this.tokenizer_instance,
        this.model_instance,
        this.vocoder_instance,
      ]);
      self.postMessage({
        status: "ready",
      });
      resolve(result);
    });
  }

  static async getSpeakerEmbeddings(speaker_id) {
    // e.g., `cmu_us_awb_arctic-wav-arctic_a0001`
    const speaker_embeddings_url = `${this.BASE_URL}${speaker_id}.bin`;
    const speaker_embeddings = new Tensor(
      "float32",
      new Float32Array(
        await (await fetch(speaker_embeddings_url)).arrayBuffer(),
      ),
      [1, 512],
    );
    return speaker_embeddings;
  }
}

// Mapping of cached speaker embeddings
const speaker_embeddings_cache = new Map();

// Listen for messages from the main thread
self.addEventListener("message", async (event) => {
  // Load the pipeline
  const [tokenizer, model, vocoder] = await MyTextToSpeechPipeline.getInstance(
    (x) => {
      // We also add a progress callback so that we can track model loading.
      self.postMessage(x);
    },
  );

  // Tokenize the input
  const { input_ids } = tokenizer(event.data.text);

  // Load the speaker embeddings
  let speaker_embeddings = speaker_embeddings_cache.get(event.data.speaker_id);
  if (speaker_embeddings === undefined) {
    speaker_embeddings = await MyTextToSpeechPipeline.getSpeakerEmbeddings(
      event.data.speaker_id,
    );
    speaker_embeddings_cache.set(event.data.speaker_id, speaker_embeddings);
  }

  // Generate the waveform
  const { waveform } = await model.generate_speech(
    input_ids,
    speaker_embeddings,
    { vocoder },
  );

  // Encode the waveform as a WAV file
  const wav = encodeWAV(waveform.data);

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: new Blob([wav], { type: "audio/wav" }),
  });
});


----- .\speecht5-web\src\components\AudioPlayer.jsx -----

import { useEffect, useRef } from "react";

export default function AudioPlayer({ audioUrl, mimeType }) {
  const audioPlayer = useRef(null);
  const audioSource = useRef(null);

  // Updates src when url changes
  useEffect(() => {
    if (audioPlayer.current && audioSource.current) {
      audioSource.current.src = audioUrl;
      audioPlayer.current.load();
    }
  }, [audioUrl]);

  return (
    <div className="flex relative z-10 my-4 w-full">
      <audio
        ref={audioPlayer}
        controls
        className="w-full h-14 rounded-lg bg-white shadow-xl shadow-black/5 ring-1 ring-slate-700/10"
      >
        <source ref={audioSource} type={mimeType}></source>
      </audio>
    </div>
  );
}


----- .\speecht5-web\src\components\Progress.jsx -----

export default function Progress({ text, percentage }) {
  percentage ??= 0;
  return (
    <div className="relative text-black bg-white rounded-lg text-left overflow-hidden">
      <div
        className="px-2 w-[1%] h-full bg-blue-500 whitespace-nowrap"
        style={{ width: `${percentage}%` }}
      >
        {text} ({`${percentage.toFixed(2)}%`})
      </div>
    </div>
  );
}


----- .\text-to-speech-webgpu\src\App.jsx -----

import React, { useRef, useState, useEffect } from "react";
import { motion } from "motion/react";

export default function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const [inputText, setInputText] = useState(
    "Speech synthesis is the artificial production of human speech.",
  );
  const [selectedSpeaker, setSelectedSpeaker] = useState("male_1");

  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState(
    "Detecting WebGPU support...",
  );

  const [results, setResults] = useState([]);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    worker.current ??= new Worker(new URL("./worker.js", import.meta.url), {
      type: "module",
    });

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        // WebGPU feature checking
        case "feature-success":
          setLoadingMessage("Loading model (only downloaded once)...");
          break;

        case "feature-error":
          // TODO: Display error on screen
          setError(e.data.data);
          break;

        case "ready":
          setStatus("ready");
          break;

        case "complete":
          const { audio, text } = e.data;
          // Generation complete: re-enable the "Generate" button
          setResults((prev) => [{ text, src: audio }, ...prev]);
          setStatus("ready");
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
    };
  }, []);

  const handleSubmit = (e) => {
    e.preventDefault();
    setStatus("running");

    worker.current.postMessage({
      type: "generate",
      text: inputText.trim(),
      speaker_id: selectedSpeaker,
    });
  };

  return (
    <div className="relative w-full min-h-screen bg-gradient-to-br from-gray-900 to-gray-700 flex flex-col items-center justify-center p-4 relative overflow-hidden font-sans">
      <motion.div
        initial={{ opacity: 1 }}
        animate={{ opacity: status === null ? 1 : 0 }}
        transition={{ duration: 0.5 }}
        className="absolute w-screen h-screen justify-center flex flex-col items-center z-10 bg-gray-800/95 backdrop-blur-md"
        style={{ pointerEvents: status === null ? "auto" : "none" }}
      >
        <div className="w-[250px] h-[250px] border-4 border-white shadow-[0_0_0_5px_#4973ff] rounded-full overflow-hidden">
          <div className="loading-wave"></div>
        </div>
        <p
          className={`text-3xl my-5 text-center ${error ? "text-red-500" : "text-white"}`}
        >
          {error ?? loadingMessage}
        </p>
      </motion.div>

      <div className="max-w-3xl w-full space-y-8 relative z-[2]">
        <div className="text-center">
          <h1 className="text-5xl font-extrabold text-gray-100 mb-2 drop-shadow-lg font-heading">
            WebGPU Text-to-Speech
          </h1>
          <p className="text-2xl text-gray-300 font-semibold font-subheading">
            Powered by&nbsp;
            <a
              href="https://github.com/edwko/OuteTTS"
              target="_blank"
              rel="noreferrer"
              className="underline"
            >
              OuteTTS
            </a>
            &nbsp;and&nbsp;
            <a
              href="https://huggingface.co/docs/transformers.js"
              target="_blank"
              rel="noreferrer"
              className="underline"
            >
              <img
                width="40"
                src="hf-logo.svg"
                className="inline translate-y-[-2px] me-1"
              ></img>
              Transformers.js
            </a>
          </p>
        </div>
        <div className="bg-gray-800/50 backdrop-blur-sm border border-gray-700 rounded-lg p-6">
          <form onSubmit={handleSubmit} className="space-y-4">
            <textarea
              placeholder="Enter text..."
              value={inputText}
              onChange={(e) => setInputText(e.target.value)}
              className="w-full min-h-[100px] max-h-[300px] bg-gray-700/50 backdrop-blur-sm border-2 border-gray-600 rounded-xl resize-y text-gray-100 placeholder-gray-400 px-3 py-2 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
              rows={Math.min(8, inputText.split("\n").length)}
            />
            <div className="flex flex-col items-center space-y-4">
              <select
                value={selectedSpeaker}
                onChange={(e) => setSelectedSpeaker(e.target.value)}
                className="w-full bg-gray-700/50 backdrop-blur-sm border-2 border-gray-600 rounded-xl text-gray-100 px-3 py-2 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
              >
                <option value="male_1">Male 1</option>
                <option value="male_2">Male 2</option>
                <option value="male_3">Male 3</option>
                <option value="male_4">Male 4</option>
                <option value="female_1">Female 1</option>
                <option value="female_2">Female 2</option>
                <option value="random">Random</option>
              </select>
              <button
                type="submit"
                className="inline-flex justify-center items-center px-6 py-2 text-lg font-semibold bg-gradient-to-t from-blue-600 to-purple-600 hover:from-blue-700 hover:to-purple-700 transition-colors duration-300 rounded-xl text-white disabled:opacity-50"
                disabled={status === "running" || inputText.trim() === ""}
              >
                {status === "running" ? "Generating..." : "Generate"}
              </button>
            </div>
          </form>
        </div>

        {results.length > 0 && (
          <motion.div
            initial={{ y: 50, opacity: 0 }}
            animate={{ y: 0, opacity: 1 }}
            transition={{ duration: 0.5 }}
            className="max-h-[250px] overflow-y-auto px-2 mt-4 space-y-6 relative z-[2]"
          >
            {results.map((result, i) => (
              <div key={i}>
                <div className="text-white bg-gray-800/70 backdrop-blur-sm border border-gray-700 rounded-lg p-4 z-10">
                  <span className="absolute right-5 font-bold">
                    #{results.length - i}
                  </span>
                  <p className="mb-3 max-w-[95%]">{result.text}</p>
                  <audio controls src={result.src} className="w-full">
                    Your browser does not support the audio element.
                  </audio>
                </div>
              </div>
            ))}
          </motion.div>
        )}
      </div>

      <div className="bg-[#015871] pointer-events-none absolute left-0 w-full h-[5%] bottom-[-50px]">
        <div className="wave"></div>
        <div className="wave"></div>
      </div>
    </div>
  );
}


----- .\text-to-speech-webgpu\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;

/*
 * Wave animations adapted from the following two demos:
 * - https://codepen.io/upasanaasopa/pen/poObEWZ
 * - https://codepen.io/breakstorm00/pen/qBJZQNB
 */

*,
*:before,
*:after {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

.loading-wave {
  position: relative;
  top: 0;
  width: 100%;
  height: 100%;
  background: #2c74b3;
  border-radius: 50%;
  box-shadow: inset 0 0 50px 0 rgba(0, 0, 0, 0.5);
}

.loading-wave:before,
.loading-wave:after {
  content: "";
  position: absolute;
  top: 0;
  left: 50%;
  width: 200%;
  height: 200%;
  background: black;
  transform: translate(-50%, -75%);
}

.loading-wave:before {
  border-radius: 45%;
  background: rgba(255, 255, 255, 1);
  animation: animate 5s linear infinite;
}

.loading-wave:after {
  border-radius: 40%;
  background: rgba(255, 255, 255, 0.5);
  animation: animate 10s linear infinite;
}

.wave {
  background: url(/wave.svg) repeat-x;
  position: absolute;
  top: -198px;
  width: 6400px;
  height: 198px;
  animation: wave 7s cubic-bezier(0.36, 0.45, 0.63, 0.53) infinite;
  transform: translate3d(0, 0, 0);
}

.wave:nth-of-type(2) {
  top: -175px;
  animation:
    wave 7s cubic-bezier(0.36, 0.45, 0.63, 0.53) -0.125s infinite,
    swell 7s ease -1.25s infinite;
  opacity: 1;
}

@keyframes wave {
  0% {
    margin-left: 0;
  }

  100% {
    margin-left: -1600px;
  }
}

@keyframes swell {
  0%,
  100% {
    transform: translate3d(0, -25px, 0);
  }

  50% {
    transform: translate3d(0, 5px, 0);
  }
}

@keyframes animate {
  0% {
    transform: translate(-50%, -75%) rotate(0deg);
  }

  100% {
    transform: translate(-50%, -75%) rotate(360deg);
  }
}


----- .\text-to-speech-webgpu\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import "./index.css";
import App from "./App.jsx";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\text-to-speech-webgpu\src\worker.js -----

import { HFModelConfig_v1, InterfaceHF } from "outetts";

// Check if WebGPU is supported
let fp16_supported = false;
try {
  const adapter = await navigator.gpu.requestAdapter();
  if (!adapter) {
    throw new Error("WebGPU is not supported (no adapter found)");
  }
  fp16_supported = adapter.features.has("shader-f16");
  self.postMessage({ status: "feature-success" });
} catch (e) {
  self.postMessage({
    status: "feature-error",
    data: e.toString(),
  });
  throw e;
}

// Configure the model
const model_config = new HFModelConfig_v1({
  model_path: "onnx-community/OuteTTS-0.2-500M",
  language: "en", // Supported languages in v0.2: en, zh, ja, ko
  dtype: fp16_supported ? "q4f16" : "q4", // Supported dtypes: fp32, fp16, q8, q4, q4f16
  device: "webgpu", // Supported devices: webgpu, wasm
});

// Initialize the interface
const tts_interface = await InterfaceHF({
  model_version: "0.2",
  cfg: model_config,
});
self.postMessage({ status: "ready" });

// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { text, speaker_id } = e.data;

  // Load a default speaker
  const speaker =
    speaker_id === "random"
      ? null
      : tts_interface.load_default_speaker(speaker_id);

  // Generate speech
  const output = await tts_interface.generate({
    text,
    temperature: 0.1, // Lower temperature values may result in a more stable tone
    repetition_penalty: 1.1,
    max_length: 4096,

    // Optional: Use a speaker profile for consistent voice characteristics
    // Without a speaker profile, the model will generate a voice with random characteristics
    speaker,
  });

  // Send the audio file back to the main thread
  const buffer = output.to_wav("output.wav");
  const blob = new Blob([buffer], { type: "audio/wav" });
  self.postMessage({
    status: "complete",
    audio: URL.createObjectURL(blob),
    text,
  });
});


----- .\text-to-speech-webgpu\src\assets\react.svg -----

<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="35.93" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 228"><path fill="#00D8FF" d="M210.483 73.824a171.49 171.49 0 0 0-8.24-2.597c.465-1.9.893-3.777 1.273-5.621c6.238-30.281 2.16-54.676-11.769-62.708c-13.355-7.7-35.196.329-57.254 19.526a171.23 171.23 0 0 0-6.375 5.848a155.866 155.866 0 0 0-4.241-3.917C100.759 3.829 77.587-4.822 63.673 3.233C50.33 10.957 46.379 33.89 51.995 62.588a170.974 170.974 0 0 0 1.892 8.48c-3.28.932-6.445 1.924-9.474 2.98C17.309 83.498 0 98.307 0 113.668c0 15.865 18.582 31.778 46.812 41.427a145.52 145.52 0 0 0 6.921 2.165a167.467 167.467 0 0 0-2.01 9.138c-5.354 28.2-1.173 50.591 12.134 58.266c13.744 7.926 36.812-.22 59.273-19.855a145.567 145.567 0 0 0 5.342-4.923a168.064 168.064 0 0 0 6.92 6.314c21.758 18.722 43.246 26.282 56.54 18.586c13.731-7.949 18.194-32.003 12.4-61.268a145.016 145.016 0 0 0-1.535-6.842c1.62-.48 3.21-.974 4.76-1.488c29.348-9.723 48.443-25.443 48.443-41.52c0-15.417-17.868-30.326-45.517-39.844Zm-6.365 70.984c-1.4.463-2.836.91-4.3 1.345c-3.24-10.257-7.612-21.163-12.963-32.432c5.106-11 9.31-21.767 12.459-31.957c2.619.758 5.16 1.557 7.61 2.4c23.69 8.156 38.14 20.213 38.14 29.504c0 9.896-15.606 22.743-40.946 31.14Zm-10.514 20.834c2.562 12.94 2.927 24.64 1.23 33.787c-1.524 8.219-4.59 13.698-8.382 15.893c-8.067 4.67-25.32-1.4-43.927-17.412a156.726 156.726 0 0 1-6.437-5.87c7.214-7.889 14.423-17.06 21.459-27.246c12.376-1.098 24.068-2.894 34.671-5.345a134.17 134.17 0 0 1 1.386 6.193ZM87.276 214.515c-7.882 2.783-14.16 2.863-17.955.675c-8.075-4.657-11.432-22.636-6.853-46.752a156.923 156.923 0 0 1 1.869-8.499c10.486 2.32 22.093 3.988 34.498 4.994c7.084 9.967 14.501 19.128 21.976 27.15a134.668 134.668 0 0 1-4.877 4.492c-9.933 8.682-19.886 14.842-28.658 17.94ZM50.35 144.747c-12.483-4.267-22.792-9.812-29.858-15.863c-6.35-5.437-9.555-10.836-9.555-15.216c0-9.322 13.897-21.212 37.076-29.293c2.813-.98 5.757-1.905 8.812-2.773c3.204 10.42 7.406 21.315 12.477 32.332c-5.137 11.18-9.399 22.249-12.634 32.792a134.718 134.718 0 0 1-6.318-1.979Zm12.378-84.26c-4.811-24.587-1.616-43.134 6.425-47.789c8.564-4.958 27.502 2.111 47.463 19.835a144.318 144.318 0 0 1 3.841 3.545c-7.438 7.987-14.787 17.08-21.808 26.988c-12.04 1.116-23.565 2.908-34.161 5.309a160.342 160.342 0 0 1-1.76-7.887Zm110.427 27.268a347.8 347.8 0 0 0-7.785-12.803c8.168 1.033 15.994 2.404 23.343 4.08c-2.206 7.072-4.956 14.465-8.193 22.045a381.151 381.151 0 0 0-7.365-13.322Zm-45.032-43.861c5.044 5.465 10.096 11.566 15.065 18.186a322.04 322.04 0 0 0-30.257-.006c4.974-6.559 10.069-12.652 15.192-18.18ZM82.802 87.83a323.167 323.167 0 0 0-7.227 13.238c-3.184-7.553-5.909-14.98-8.134-22.152c7.304-1.634 15.093-2.97 23.209-3.984a321.524 321.524 0 0 0-7.848 12.897Zm8.081 65.352c-8.385-.936-16.291-2.203-23.593-3.793c2.26-7.3 5.045-14.885 8.298-22.6a321.187 321.187 0 0 0 7.257 13.246c2.594 4.48 5.28 8.868 8.038 13.147Zm37.542 31.03c-5.184-5.592-10.354-11.779-15.403-18.433c4.902.192 9.899.29 14.978.29c5.218 0 10.376-.117 15.453-.343c-4.985 6.774-10.018 12.97-15.028 18.486Zm52.198-57.817c3.422 7.8 6.306 15.345 8.596 22.52c-7.422 1.694-15.436 3.058-23.88 4.071a382.417 382.417 0 0 0 7.859-13.026a347.403 347.403 0 0 0 7.425-13.565Zm-16.898 8.101a358.557 358.557 0 0 1-12.281 19.815a329.4 329.4 0 0 1-23.444.823c-7.967 0-15.716-.248-23.178-.732a310.202 310.202 0 0 1-12.513-19.846h.001a307.41 307.41 0 0 1-10.923-20.627a310.278 310.278 0 0 1 10.89-20.637l-.001.001a307.318 307.318 0 0 1 12.413-19.761c7.613-.576 15.42-.876 23.31-.876H128c7.926 0 15.743.303 23.354.883a329.357 329.357 0 0 1 12.335 19.695a358.489 358.489 0 0 1 11.036 20.54a329.472 329.472 0 0 1-11 20.722Zm22.56-122.124c8.572 4.944 11.906 24.881 6.52 51.026c-.344 1.668-.73 3.367-1.15 5.09c-10.622-2.452-22.155-4.275-34.23-5.408c-7.034-10.017-14.323-19.124-21.64-27.008a160.789 160.789 0 0 1 5.888-5.4c18.9-16.447 36.564-22.941 44.612-18.3ZM128 90.808c12.625 0 22.86 10.235 22.86 22.86s-10.235 22.86-22.86 22.86s-22.86-10.235-22.86-22.86s10.235-22.86 22.86-22.86Z"></path></svg>

----- .\the-tokenizer-playground\src\App.css -----

#root {
  max-width: 1280px;
  width: 100%;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
  display: flex;
  justify-content: center;
  align-items: center;
  flex-direction: column;
}


----- .\the-tokenizer-playground\src\App.jsx -----

import { useCallback, useEffect, useRef, useState } from "react";
import { Token } from "./components/Token";
import "./App.css";

// Define list of tokenizers and their corresponding human-readable names
const TOKENIZER_OPTIONS = Object.freeze({
  "Xenova/gpt-4": "gpt-4 / gpt-3.5-turbo / text-embedding-ada-002",
  "Xenova/text-davinci-003": "text-davinci-003 / text-davinci-002",
  "Xenova/gpt-3": "gpt-3",
  "Xenova/grok-1-tokenizer": "Grok-1",
  "Xenova/claude-tokenizer": "Claude",
  "Xenova/mistral-tokenizer-v3": "Mistral v3",
  "Xenova/mistral-tokenizer-v1": "Mistral v1",
  "Xenova/gemma-tokenizer": "Gemma",
  "Xenova/llama-3-tokenizer": "Llama 3",
  "Xenova/llama-tokenizer": "LLaMA / Llama 2",
  "Xenova/c4ai-command-r-v01-tokenizer": "Cohere Command-R",
  "Xenova/t5-small": "T5",
  "Xenova/bert-base-cased": "bert-base-cased",
  "": "Custom",
});

function App() {
  // Allow user to set tokenizer and text via URL query parameters
  const urlParams = new URLSearchParams(window.location.search);
  const tokenizerParam = urlParams.get("tokenizer");
  const textParam = urlParams.get("text");

  const [tokenIds, setTokenIds] = useState([]);
  const [decodedTokens, setDecodedTokens] = useState([]);
  const [margins, setMargins] = useState([]);
  const [outputOption, setOutputOption] = useState("text");
  const [tokenizer, setTokenizer] = useState(tokenizerParam ?? "Xenova/gpt-4");
  const [customTokenizer, setCustomTokenizer] = useState("");

  const textareaRef = useRef(null);
  const outputRef = useRef(null);

  // Create a reference to the worker object.
  const worker = useRef(null);

  // We use the `useEffect` hook to set up the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    worker.current ??= new Worker(new URL("./worker.js", import.meta.url), {
      type: "module",
    });

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      setTokenIds(e.data.token_ids);
      setDecodedTokens(e.data.decoded);
      setMargins(e.data.margins);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () =>
      worker.current.removeEventListener("message", onMessageReceived);
  }, []);

  const resetOutput = useCallback(() => {
    setOutputOption("text");
    setTokenIds([]);
    setDecodedTokens([]);
    setMargins([]);
  }, []);

  const onInputChange = useCallback(
    (e) => {
      const model_id = tokenizer;
      const text = e.target.value;

      if (text.length > 10000) {
        setOutputOption(null);
        console.log(
          "User most likely pasted in a large body of text (> 10k chars), so we hide the output (until specifically requested by the user).",
        );
      }
      worker.current.postMessage({ model_id, text });
    },
    [tokenizer],
  );

  useEffect(() => {
    if (textParam) {
      onInputChange({ target: { value: textParam } });
    }
  }, [onInputChange, textParam]);

  const onTokenizerChange = useCallback((e) => {
    const model_id = e.target.value;
    setTokenizer(model_id);
    if (!model_id) return;
    worker.current.postMessage({ model_id, text: textareaRef.current.value });
  }, []);

  return (
    <div className="w-full max-w-[720px] flex flex-col gap-4 items-center">
      <div>
        <h1 className="text-5xl font-bold mb-2">The Tokenizer Playground</h1>
        <h2 className="text-lg font-normal">
          Experiment with different tokenizers (running{" "}
          <a
            className="text-gray-900 underline"
            href="https://github.com/huggingface/transformers.js"
          >
            locally
          </a>{" "}
          in your browser).
        </h2>
      </div>

      <div>
        <select
          value={
            tokenizer in TOKENIZER_OPTIONS && !customTokenizer ? tokenizer : ""
          }
          onChange={(e) => {
            resetOutput();
            setCustomTokenizer("");
            onTokenizerChange(e);
          }}
          className="bg-gray-50 border border-gray-300 text-gray-900 text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500 block w-full p-2"
        >
          {Object.entries(TOKENIZER_OPTIONS).map(([value, label]) => (
            <option key={value} value={value}>
              {label}
            </option>
          ))}
        </select>
        {(!(tokenizer in TOKENIZER_OPTIONS) ||
          customTokenizer ||
          tokenizer === "") && (
          <input
            type="text"
            placeholder="Custom tokenizer"
            defaultValue={customTokenizer || tokenizer}
            onChange={(e) => {
              setCustomTokenizer(e.target.value);
              onTokenizerChange(e);
            }}
            className="bg-white border border-gray-300 text-gray-900 text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500 block w-full py-1 px-2 mt-1"
          />
        )}
      </div>

      <textarea
        ref={textareaRef}
        onChange={onInputChange}
        rows="8"
        className="font-mono text-lg block w-full p-2.5 text-gray-900 bg-gray-50 rounded-lg border border-gray-200"
        placeholder="Enter some text"
        defaultValue={textParam ?? textareaRef.current?.value ?? ""}
      ></textarea>

      <div className="flex justify-center gap-5">
        <div className="flex flex-col">
          <h2 className="font-semibold uppercase leading-4">Tokens</h2>
          <h3 className="font-semibold text-3xl">
            {tokenIds.length.toLocaleString()}
          </h3>
        </div>
        <div className="flex flex-col">
          <h2 className="font-semibold uppercase leading-4">Characters</h2>
          <h3 className="font-semibold text-3xl">
            {(textareaRef.current?.value.length ?? 0).toLocaleString()}
          </h3>
        </div>
      </div>

      <div
        ref={outputRef}
        className="font-mono text-lg p-2.5 w-full bg-gray-100 rounded-lg border border-gray-200 whitespace-pre-wrap text-left h-[200px] overflow-y-auto"
      >
        {outputOption === "text"
          ? decodedTokens.map((token, index) => (
              <Token
                key={index}
                text={token}
                position={index}
                margin={margins[index]}
              />
            ))
          : outputOption === "token_ids"
            ? `[${tokenIds.join(", ")}]`
            : null}
      </div>

      <div className="flex items-center gap-2 self-end">
        <div className="flex items-center">
          <input
            checked={outputOption === "text"}
            onChange={() => setOutputOption("text")}
            id="output-radio-1"
            type="radio"
            value=""
            name="output-radio"
            className="w-4 h-4 text-blue-600 bg-gray-100 border-gray-300 focus:ring-blue-500"
          />
          <label
            htmlFor="output-radio-1"
            className="ml-1 text-sm font-medium text-gray-900 dark:text-gray-300"
          >
            Text
          </label>
        </div>
        <div className="flex items-center">
          <input
            checked={outputOption === "token_ids"}
            onChange={() => setOutputOption("token_ids")}
            id="output-radio-2"
            type="radio"
            value=""
            name="output-radio"
            className="w-4 h-4 text-blue-600 bg-gray-100 border-gray-300 focus:ring-blue-500"
          />
          <label
            htmlFor="output-radio-2"
            className="ml-1 text-sm font-medium text-gray-900 dark:text-gray-300"
          >
            Token IDs
          </label>
        </div>
        <div className="flex items-center">
          <input
            checked={outputOption === null}
            onChange={() => setOutputOption(null)}
            id="output-radio-3"
            type="radio"
            value=""
            name="output-radio"
            className="w-4 h-4 text-blue-600 bg-gray-100 border-gray-300 focus:ring-blue-500"
          />
          <label
            htmlFor="output-radio-3"
            className="ml-1 text-sm font-medium text-gray-900 dark:text-gray-300"
          >
            Hide
          </label>
        </div>
      </div>
    </div>
  );
}

export default App;


----- .\the-tokenizer-playground\src\index.css -----

@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;

  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: #242424;

  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-text-size-adjust: 100%;
}

body {
  margin: 0;
  display: flex;
  place-items: center;
  min-height: 100vh;
}

@media (prefers-color-scheme: light) {
  :root {
    color: #213547;
    background-color: #ffffff;
  }
  a:hover {
    color: #747bff;
  }
  button {
    background-color: #f9f9f9;
  }
}


----- .\the-tokenizer-playground\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\the-tokenizer-playground\src\worker.js -----

// Although not strictly necessary, we delegate the tokenization to a worker thread to avoid
// any potential issues with the tokenizer blocking the main thread (especially for large inputs).

import { AutoTokenizer } from "@huggingface/transformers";

// This is a map of all the tokenizer instances that we have loaded.
// model_id -> promise that resolves to tokenizer
const TOKENIZER_MAPPINGS = new Map();

// Listen for messages from the main thread
self.addEventListener("message", async (event) => {
  const { model_id, text } = event.data;

  // Only load the tokenizer if it hasn't been loaded yet
  let tokenizerPromise = TOKENIZER_MAPPINGS.get(model_id);
  if (!tokenizerPromise) {
    // For visualization purposes, we may need to modify the tokenizer slightly
    tokenizerPromise = AutoTokenizer.from_pretrained(model_id).then(
      (tokenizer) => {
        // NOTE: We just remove the StripDecoder from the llama tokenizer
        const tokenizer_class = (
          tokenizer._tokenizer_config?.tokenizer_class ?? ""
        ).replace(/Fast$/, "");
        switch (tokenizer_class) {
          case "LlamaTokenizer":
          case "Grok1Tokenizer":
            // tokenizer.decoder.decoders.at(-1).constructor.name === 'StripDecoder'
            tokenizer.decoder.decoders.pop();
            break;
          case "T5Tokenizer":
            tokenizer.decoder.addPrefixSpace = false;
            break;
        }
        return tokenizer;
      },
    );

    TOKENIZER_MAPPINGS.set(model_id, tokenizerPromise);
  }

  const tokenizer = await tokenizerPromise;

  // Tokenize the input text
  const token_ids = tokenizer.encode(text);

  // Decode the token IDs back to text
  let decoded = token_ids.map((x) => tokenizer.decode([x]));

  // Minor post-processing for visualization purposes
  let margins = [];
  switch (tokenizer.constructor.name) {
    case "BertTokenizer":
      margins = decoded.map((x, i) => (i === 0 || x.startsWith("##") ? 0 : 8));
      decoded = decoded.map((x) => x.replace("##", ""));
      break;
    case "T5Tokenizer":
      if (decoded.length > 0 && decoded !== " ") {
        decoded[0] = decoded[0].replace(/^ /, "");
      }
      break;
  }

  // Send the output back to the main thread
  self.postMessage({
    token_ids,
    decoded,
    margins,
  });
});


----- .\the-tokenizer-playground\src\components\Token.jsx -----

import { Fragment } from "react";

const COLOURS = [
  "bg-purple-300",
  "bg-green-300",
  "bg-yellow-300",
  "bg-red-300",
  "bg-blue-300",
];

export function Token({ text, position, margin }) {
  const textWithLineBreaks = text.split("\n").map((line, index, array) => (
    <Fragment key={index}>
      {line}
      {index !== array.length - 1 && <br />}
    </Fragment>
  ));
  return (
    <span
      style={{ marginLeft: margin }}
      className={`leading-5 ${textWithLineBreaks.length === 1 ? "inline-block " : ""}${COLOURS[position % COLOURS.length]}`}
    >
      {textWithLineBreaks}
    </span>
  );
}


----- .\tinyswallow-webgpu\src\App.jsx -----

import { useEffect, useState, useRef } from "react";

import Chat from "./components/Chat";
import ArrowRightIcon from "./components/icons/ArrowRightIcon";
import StopIcon from "./components/icons/StopIcon";
import Progress from "./components/Progress";

const IS_WEBGPU_AVAILABLE = !!navigator.gpu;
const STICKY_SCROLL_THRESHOLD = 120;
const EXAMPLES = [
  "çŸ¥è­˜è’¸ç•™ã«ã¤ã„ã¦ç°¡å˜ã«æ•™ãˆã¦ãã ã•ã„ã€‚",
  "ãƒžã‚·ãƒ¼ãƒ³ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¤ã„ã¦ã®è©©ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
  "nç•ªç›®ã®ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°ã‚’è¨ˆç®—ã™ã‚‹Pythonã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
];

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  const textareaRef = useRef(null);
  const chatContainerRef = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [error, setError] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);
  const [isRunning, setIsRunning] = useState(false);

  // Inputs and outputs
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState([]);
  const [tps, setTps] = useState(null);
  const [numTokens, setNumTokens] = useState(null);
  const [isCompositionOn, setIsCompositionOn] = useState(false);

  function onEnter(message) {
    setMessages((prev) => [...prev, { role: "user", content: message }]);
    setTps(null);
    setIsRunning(true);
    setInput("");
  }

  function onInterrupt() {
    // NOTE: We do not set isRunning to false here because the worker
    // will send a 'complete' message when it is done.
    worker.current.postMessage({ type: "interrupt" });
  }

  useEffect(() => {
    resizeInput();
  }, [input]);

  function resizeInput() {
    if (!textareaRef.current) return;

    const target = textareaRef.current;
    target.style.height = "auto";
    const newHeight = Math.min(Math.max(target.scrollHeight, 24), 200);
    target.style.height = `${newHeight}px`;
  }

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    if (!worker.current) {
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
      worker.current.postMessage({ type: "check" }); // Do a feature check
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "start":
          {
            // Start generation
            setMessages((prev) => [
              ...prev,
              { role: "assistant", content: "" },
            ]);
          }
          break;

        case "update":
          {
            // Generation update: update the output text.
            // Parse messages
            const { output, tps, numTokens } = e.data;
            setTps(tps);
            setNumTokens(numTokens);
            setMessages((prev) => {
              const cloned = [...prev];
              const last = cloned.at(-1);
              cloned[cloned.length - 1] = {
                ...last,
                content: last.content + output,
              };
              return cloned;
            });
          }
          break;

        case "complete":
          // Generation complete: re-enable the "Generate" button
          setIsRunning(false);
          break;

        case "error":
          setError(e.data.data);
          break;
      }
    };

    const onErrorReceived = (e) => {
      console.error("Worker error:", e);
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);
    worker.current.addEventListener("error", onErrorReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
      worker.current.removeEventListener("error", onErrorReceived);
    };
  }, []);

  // Send the messages to the worker thread whenever the `messages` state changes.
  useEffect(() => {
    if (messages.filter((x) => x.role === "user").length === 0) {
      // No user messages yet: do nothing.
      return;
    }
    if (messages.at(-1).role === "assistant") {
      // Do not update if the last message is from the assistant
      return;
    }
    setTps(null);
    worker.current.postMessage({ type: "generate", data: messages });
  }, [messages, isRunning]);

  useEffect(() => {
    if (!chatContainerRef.current || !isRunning) return;
    const element = chatContainerRef.current;
    if (
      element.scrollHeight - element.scrollTop - element.clientHeight <
      STICKY_SCROLL_THRESHOLD
    ) {
      element.scrollTop = element.scrollHeight;
    }
  }, [messages, isRunning]);

  return IS_WEBGPU_AVAILABLE ? (
    <div className="flex flex-col h-screen mx-auto items justify-end text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900">
      {status === null && messages.length === 0 && (
        <div className="h-full overflow-auto scrollbar-thin flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 max-w-[380px] text-center">
            <img
              src="logo.png"
              width="80%"
              height="auto"
              className="block drop-shadow-lg bg-transparent"
            ></img>
            <h1 className="text-4xl font-bold mb-1">TinySwallow WebGPU</h1>
            <h2 className="font-semibold">
              A compact Japanese language model that runs locally in your
              browser with WebGPU acceleration.
            </h2>
          </div>

          <div className="flex flex-col items-center px-4">
            <p className="max-w-[480px] mb-4">
              <br />
              You are about to load{" "}
              <a
                href="https://huggingface.co/onnx-community/TinySwallow-1.5B-Instruct-ONNX"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                TinySwallow-1.5B-Instruct
              </a>
              , a 1.5B parameter LLM optimized for in-browser inference.
              Everything runs entirely in your browser with{" "}
              <a
                href="https://huggingface.co/docs/transformers.js"
                target="_blank"
                rel="noreferrer"
                className="underline"
              >
                ðŸ¤—&nbsp;Transformers.js
              </a>{" "}
              and ONNX Runtime Web, meaning no data is sent to a server. Once
              loaded, it can even be used offline. The source code for the demo
              is available on{" "}
              <a
                href="https://github.com/huggingface/transformers.js-examples/tree/main/tinyswallow-webgpu"
                target="_blank"
                rel="noreferrer"
                className="font-medium underline"
              >
                GitHub
              </a>
              .
            </p>

            {error && (
              <div className="text-red-500 text-center mb-2">
                <p className="mb-1">
                  Unable to load model due to the following error:
                </p>
                <p className="text-sm">{error}</p>
              </div>
            )}

            <button
              className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 cursor-pointer disabled:cursor-not-allowed select-none"
              onClick={() => {
                worker.current.postMessage({ type: "load" });
                setStatus("loading");
              }}
              disabled={status !== null || error !== null}
            >
              Load model
            </button>
          </div>
        </div>
      )}
      {status === "loading" && (
        <>
          <div className="w-full max-w-[500px] text-left mx-auto p-4 bottom-0 mt-auto">
            <p className="text-center mb-1">{loadingMessage}</p>
            {progressItems.map(({ file, progress, total }, i) => (
              <Progress
                key={i}
                text={file}
                percentage={progress}
                total={total}
              />
            ))}
          </div>
        </>
      )}

      {status === "ready" && (
        <div
          ref={chatContainerRef}
          className="overflow-y-auto scrollbar-thin w-full flex flex-col items-center h-full"
        >
          <Chat messages={messages} />
          {messages.length === 0 && (
            <div>
              {EXAMPLES.map((msg, i) => (
                <div
                  key={i}
                  className="m-1 border border-gray-300 dark:border-gray-600 rounded-md p-2 bg-gray-100 dark:bg-gray-700 cursor-pointer"
                  onClick={() => onEnter(msg)}
                >
                  {msg}
                </div>
              ))}
            </div>
          )}
          <p className="text-center text-sm min-h-6 text-gray-500 dark:text-gray-300">
            {tps && messages.length > 0 && (
              <>
                {!isRunning && (
                  <span>
                    Generated {numTokens} tokens in{" "}
                    {(numTokens / tps).toFixed(2)} seconds&nbsp;&#40;
                  </span>
                )}
                {
                  <>
                    <span className="font-medium text-center mr-1 text-black dark:text-white">
                      {tps.toFixed(2)}
                    </span>
                    <span className="text-gray-500 dark:text-gray-300">
                      tokens/second
                    </span>
                  </>
                }
                {!isRunning && (
                  <>
                    <span className="mr-1">&#41;.</span>
                    <span
                      className="underline cursor-pointer"
                      onClick={() => {
                        worker.current.postMessage({ type: "reset" });
                        setMessages([]);
                      }}
                    >
                      Reset
                    </span>
                  </>
                )}
              </>
            )}
          </p>
        </div>
      )}

      <div className="mt-2 border border-gray-300 dark:bg-gray-700 rounded-lg w-[600px] max-w-[80%] max-h-[200px] mx-auto relative mb-3 flex">
        <textarea
          ref={textareaRef}
          className="scrollbar-thin w-[550px] dark:bg-gray-700 px-3 py-4 rounded-lg bg-transparent border-none outline-hidden text-gray-800 disabled:text-gray-400 dark:text-gray-200 placeholder-gray-500 dark:placeholder-gray-400 disabled:placeholder-gray-200 resize-none disabled:cursor-not-allowed"
          placeholder="Type your message..."
          type="text"
          rows={1}
          value={input}
          disabled={status !== "ready"}
          title={status === "ready" ? "Model is ready" : "Model not loaded yet"}
          onKeyDown={(e) => {
            if (
              input.length > 0 &&
              !isRunning &&
              e.key === "Enter" &&
              !e.shiftKey &&
              !isCompositionOn
            ) {
              e.preventDefault(); // Prevent default behavior of Enter key
              onEnter(input);
            }
          }}
          onCompositionStart={() => setIsCompositionOn(true)}
          onCompositionEnd={() => setIsCompositionOn(false)}
          onInput={(e) => setInput(e.target.value)}
        />
        {isRunning ? (
          <div className="cursor-pointer" onClick={onInterrupt}>
            <StopIcon className="h-8 w-8 p-1 rounded-md text-gray-800 dark:text-gray-100 absolute right-3 bottom-3" />
          </div>
        ) : input.length > 0 ? (
          <div className="cursor-pointer" onClick={() => onEnter(input)}>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-800 dark:bg-gray-100 text-white dark:text-black rounded-md absolute right-3 bottom-3`}
            />
          </div>
        ) : (
          <div>
            <ArrowRightIcon
              className={`h-8 w-8 p-1 bg-gray-200 dark:bg-gray-600 text-gray-50 dark:text-gray-800 rounded-md absolute right-3 bottom-3`}
            />
          </div>
        )}
      </div>

      <p className="text-xs text-gray-400 text-center mb-3">
        Disclaimer: Generated content may be inaccurate or false.
      </p>
    </div>
  ) : (
    <div className="fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] text-white text-2xl font-semibold flex justify-center items-center text-center">
      WebGPU is not supported
      <br />
      by this browser :&#40;
    </div>
  );
}

export default App;


----- .\tinyswallow-webgpu\src\index.css -----

@import "tailwindcss";

/* Custom scrollbar styles */
.scrollbar-thin::-webkit-scrollbar {
  width: 0.5rem; /* Equivalent to w-2 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-track {
  border-radius: 9999px; /* Equivalent to rounded-full in Tailwind */
  background-color: #f3f4f6; /* Equivalent to bg-gray-100 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-track.dark {
  background-color: #374151; /* Equivalent to dark:bg-gray-700 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-thumb {
  border-radius: 9999px; /* Equivalent to rounded-full in Tailwind */
  background-color: #d1d5db; /* Equivalent to bg-gray-300 in Tailwind */
}

.scrollbar-thin::-webkit-scrollbar-thumb:hover {
  background-color: #6b7280; /* Equivalent to bg-gray-500 in Tailwind */
}

/* Animation delay classes */
.animation-delay-200 {
  animation-delay: 200ms;
}

.animation-delay-400 {
  animation-delay: 400ms;
}

/* Overflow wrap class */
.overflow-wrap-anywhere {
  overflow-wrap: anywhere;
}


----- .\tinyswallow-webgpu\src\main.jsx -----

import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import "./index.css";
import App from "./App.jsx";

createRoot(document.getElementById("root")).render(
  <StrictMode>
    <App />
  </StrictMode>,
);


----- .\tinyswallow-webgpu\src\worker.js -----

import {
  AutoTokenizer,
  AutoModelForCausalLM,
  TextStreamer,
  InterruptableStoppingCriteria,
} from "@huggingface/transformers";

/**
 * Helper function to perform feature detection for WebGPU
 */
let fp16_supported = false;
async function check() {
  try {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) {
      throw new Error("WebGPU is not supported (no adapter found)");
    }
    fp16_supported = adapter.features.has("shader-f16");
  } catch (e) {
    self.postMessage({
      status: "error",
      data: e.toString(),
    });
  }
}

/**
 * This class uses the Singleton pattern to enable lazy-loading of the pipeline
 */
class TextGenerationPipeline {
  static model_id = "onnx-community/TinySwallow-1.5B-Instruct-ONNX";

  static async getInstance(progress_callback = null) {
    this.tokenizer ??= AutoTokenizer.from_pretrained(this.model_id, {
      progress_callback,
    });

    this.model ??= AutoModelForCausalLM.from_pretrained(this.model_id, {
      dtype: fp16_supported ? "q4f16" : "q4",
      device: "webgpu",
      progress_callback,
      use_external_data_format: true,
    });

    return Promise.all([this.tokenizer, this.model]);
  }
}

const stopping_criteria = new InterruptableStoppingCriteria();

let past_key_values_cache = null;
async function generate(messages) {
  // Retrieve the text-generation pipeline.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance();

  const inputs = tokenizer.apply_chat_template(messages, {
    add_generation_prompt: true,
    return_dict: true,
  });

  let startTime;
  let numTokens = 0;
  let tps;
  const token_callback_function = (tokens) => {
    startTime ??= performance.now();

    if (numTokens++ > 0) {
      tps = (numTokens / (performance.now() - startTime)) * 1000;
    }
  };
  const callback_function = (output) => {
    self.postMessage({
      status: "update",
      output,
      tps,
      numTokens,
    });
  };

  const streamer = new TextStreamer(tokenizer, {
    skip_prompt: true,
    skip_special_tokens: true,
    callback_function,
    token_callback_function,
  });

  // Tell the main thread we are starting
  self.postMessage({ status: "start" });

  const { past_key_values, sequences } = await model.generate({
    ...inputs,
    // TODO: Add back when fixed
    // past_key_values: past_key_values_cache,

    // Sampling
    do_sample: false,
    // repetition_penalty: 1.1,
    // top_k: 3,
    // temperature: 0.2,

    max_new_tokens: 2048,
    streamer,
    stopping_criteria,
    return_dict_in_generate: true,
  });
  past_key_values_cache = past_key_values;

  const decoded = tokenizer.batch_decode(sequences, {
    skip_special_tokens: true,
  });

  // Send the output back to the main thread
  self.postMessage({
    status: "complete",
    output: decoded,
  });
}

async function load() {
  self.postMessage({
    status: "loading",
    data: "Loading model...",
  });

  // Load the pipeline and save it for future use.
  const [tokenizer, model] = await TextGenerationPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  self.postMessage({
    status: "loading",
    data: "Compiling shaders and warming up model...",
  });

  // Run model with dummy input to compile shaders
  const inputs = tokenizer("a");
  await model.generate({ ...inputs, max_new_tokens: 1 });
  self.postMessage({ status: "ready" });
}
// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "check":
      check();
      break;

    case "load":
      load();
      break;

    case "generate":
      stopping_criteria.reset();
      generate(data);
      break;

    case "interrupt":
      stopping_criteria.interrupt();
      break;

    case "reset":
      past_key_values_cache = null;
      stopping_criteria.reset();
      break;
  }
});


----- .\tinyswallow-webgpu\src\components\Chat.css -----

@scope (.markdown) {
  /* Code blocks */
  pre {
    margin: 0.5rem 0;
    white-space: break-spaces;
  }

  code {
    padding: 0.2em 0.4em;
    border-radius: 4px;
    font-family: Consolas, Monaco, "Andale Mono", "Ubuntu Mono", monospace;
    font-size: 0.9em;
  }

  pre,
  code {
    background-color: #f2f2f2;
  }

  @media (prefers-color-scheme: dark) {
    pre,
    code {
      background-color: #333;
    }
  }

  pre:has(code) {
    padding: 1rem 0.5rem;
  }

  pre > code {
    padding: 0;
  }

  /* Headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-weight: 600;
    line-height: 1.2;
  }

  h1 {
    font-size: 2em;
    margin: 1rem 0;
  }

  h2 {
    font-size: 1.5em;
    margin: 0.83rem 0;
  }

  h3 {
    font-size: 1.25em;
    margin: 0.67rem 0;
  }

  h4 {
    font-size: 1em;
    margin: 0.5rem 0;
  }

  h5 {
    font-size: 0.875em;
    margin: 0.33rem 0;
  }

  h6 {
    font-size: 0.75em;
    margin: 0.25rem 0;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6:first-child {
    margin-top: 0;
  }

  /* Unordered List */
  ul {
    list-style-type: disc;
    margin-left: 1.5rem;
  }

  /* Ordered List */
  ol {
    list-style-type: decimal;
    margin-left: 1.5rem;
  }

  /* List Items */
  li {
    margin: 0.25rem 0;
  }

  p:not(:first-child) {
    margin-top: 0.75rem;
  }

  p:not(:last-child) {
    margin-bottom: 0.75rem;
  }

  ul > li {
    margin-left: 1rem;
  }

  /* Table */
  table,
  th,
  td {
    border: 1px solid lightgray;
    padding: 0.25rem;
  }

  @media (prefers-color-scheme: dark) {
    table,
    th,
    td {
      border: 1px solid #f2f2f2;
    }
  }
}


----- .\tinyswallow-webgpu\src\components\Chat.jsx -----

import { useState } from "react";
import { marked } from "marked";
import DOMPurify from "dompurify";

import BotIcon from "./icons/BotIcon";
import UserIcon from "./icons/UserIcon";

import { MathJaxContext, MathJax } from "better-react-mathjax";
import "./Chat.css";

function render(text) {
  // Replace all instances of single backslashes before brackets with double backslashes
  // See https://github.com/markedjs/marked/issues/546 for more information.
  text = text.replace(/\\([\[\]\(\)])/g, "\\\\$1");

  const result = DOMPurify.sanitize(
    marked.parse(text, {
      async: false,
      breaks: true,
    }),
  );
  return result;
}
function Message({ role, content }) {
  return (
    <div className="flex items-start space-x-4">
      {role === "assistant" ? (
        <>
          <BotIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
          <div className="bg-gray-200 dark:bg-gray-700 rounded-lg p-4">
            <div className="min-h-6 text-gray-800 dark:text-gray-200 overflow-wrap-anywhere">
              {content.length > 0 ? (
                <MathJax className="mt-2" dynamic>
                  <span
                    className="markdown"
                    dangerouslySetInnerHTML={{
                      __html: render(content),
                    }}
                  />
                </MathJax>
              ) : (
                <span className="h-6 flex items-center gap-1">
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse"></span>
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-200"></span>
                  <span className="w-2.5 h-2.5 bg-gray-600 dark:bg-gray-300 rounded-full animate-pulse animation-delay-400"></span>
                </span>
              )}
            </div>
          </div>
        </>
      ) : (
        <>
          <UserIcon className="h-6 w-6 min-h-6 min-w-6 my-3 text-gray-500 dark:text-gray-300" />
          <div className="bg-blue-500 text-white rounded-lg p-4">
            <p className="min-h-6 overflow-wrap-anywhere">{content}</p>
          </div>
        </>
      )}
    </div>
  );
}

export default function Chat({ messages }) {
  const empty = messages.length === 0;

  return (
    <div
      className={`flex-1 p-6 max-w-[960px] w-full ${empty ? "flex flex-col items-center justify-end" : "space-y-4"}`}
    >
      <MathJaxContext>
        {empty ? (
          <div className="text-xl">Ready!</div>
        ) : (
          messages.map((msg, i) => <Message key={`message-${i}`} {...msg} />)
        )}
      </MathJaxContext>
    </div>
  );
}


----- .\tinyswallow-webgpu\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\tinyswallow-webgpu\src\components\icons\ArrowRightIcon.jsx -----

export default function ArrowRightIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M5 12h14" />
      <path d="m12 5 7 7-7 7" />
    </svg>
  );
}


----- .\tinyswallow-webgpu\src\components\icons\BotIcon.jsx -----

export default function BotIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M12 8V4H8" />
      <rect width="16" height="12" x="4" y="8" rx="2" />
      <path d="M2 14h2" />
      <path d="M20 14h2" />
      <path d="M15 13v2" />
      <path d="M9 13v2" />
    </svg>
  );
}


----- .\tinyswallow-webgpu\src\components\icons\StopIcon.jsx -----

export default function StopIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" />
      <path
        fill="currentColor"
        d="M9 9.563C9 9.252 9.252 9 9.563 9h4.874c.311 0 .563.252.563.563v4.874c0 .311-.252.563-.563.563H9.564A.562.562 0 0 1 9 14.437V9.564Z"
      />
    </svg>
  );
}


----- .\tinyswallow-webgpu\src\components\icons\UserIcon.jsx -----

export default function UserIcon(props) {
  return (
    <svg
      {...props}
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
    >
      <path d="M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2" />
      <circle cx="12" cy="7" r="4" />
    </svg>
  );
}


----- .\whisper-word-timestamps\src\App.jsx -----

import { useEffect, useState, useRef, useCallback } from "react";

import Progress from "./components/Progress";
import MediaInput from "./components/MediaInput";
import Transcript from "./components/Transcript";
import LanguageSelector from "./components/LanguageSelector";

async function hasWebGPU() {
  if (!navigator.gpu) {
    return false;
  }
  try {
    const adapter = await navigator.gpu.requestAdapter();
    return !!adapter;
  } catch (e) {
    return false;
  }
}

function App() {
  // Create a reference to the worker object.
  const worker = useRef(null);

  // Model loading and progress
  const [status, setStatus] = useState(null);
  const [loadingMessage, setLoadingMessage] = useState("");
  const [progressItems, setProgressItems] = useState([]);

  const mediaInputRef = useRef(null);
  const [audio, setAudio] = useState(null);
  const [language, setLanguage] = useState("en");

  const [result, setResult] = useState(null);
  const [time, setTime] = useState(null);
  const [currentTime, setCurrentTime] = useState(0);

  const [device, setDevice] = useState("webgpu"); // Try use WebGPU first
  const [modelSize, setModelSize] = useState("gpu" in navigator ? 196 : 77); // WebGPU=196MB, WebAssembly=77MB
  useEffect(() => {
    hasWebGPU().then((result) => {
      setModelSize(result ? 196 : 77);
      setDevice(result ? "webgpu" : "wasm");
    });
  }, []);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    // Create the worker if it does not yet exist.
    worker.current ??= new Worker(new URL("./worker.js", import.meta.url), {
      type: "module",
    });

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case "loading":
          // Model file start load: add a new progress item to the list.
          setStatus("loading");
          setLoadingMessage(e.data.data);
          break;

        case "initiate":
          setProgressItems((prev) => [...prev, e.data]);
          break;

        case "progress":
          // Model file progress: update one of the progress items.
          setProgressItems((prev) =>
            prev.map((item) => {
              if (item.file === e.data.file) {
                return { ...item, ...e.data };
              }
              return item;
            }),
          );
          break;

        case "done":
          // Model file loaded: remove the progress item from the list.
          setProgressItems((prev) =>
            prev.filter((item) => item.file !== e.data.file),
          );
          break;

        case "ready":
          // Pipeline ready: the worker is ready to accept messages.
          setStatus("ready");
          break;

        case "complete":
          setResult(e.data.result);
          setTime(e.data.time);
          setStatus("ready");
          break;
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () => {
      worker.current.removeEventListener("message", onMessageReceived);
    };
  }, []);

  const handleClick = useCallback(() => {
    setResult(null);
    setTime(null);
    if (status === null) {
      setStatus("loading");
      worker.current.postMessage({ type: "load", data: { device } });
    } else {
      setStatus("running");
      worker.current.postMessage({
        type: "run",
        data: { audio, language },
      });
    }
  }, [status, audio, language, device]);

  return (
    <div className="w-screen h-screen text-gray-800 dark:text-gray-200 bg-white dark:bg-gray-900 ">
      <div className="flex flex-col mx-auto items justify-end max-w-[560px] h-full">
        {status === "loading" && (
          <div className="flex justify-center items-center fixed w-screen h-screen bg-black z-10 bg-opacity-[92%] top-0 left-0">
            <div className="w-[500px]">
              <p className="text-center mb-1 text-white text-md">
                {loadingMessage}
              </p>
              {progressItems.map(({ file, progress, total }, i) => (
                <Progress
                  key={i}
                  text={file}
                  percentage={progress}
                  total={total}
                />
              ))}
            </div>
          </div>
        )}
        <div className="h-full flex justify-center items-center flex-col relative">
          <div className="flex flex-col items-center mb-1 text-center">
            <h1 className="text-5xl font-bold mb-2">Whisper Timestamped</h1>
            <h2 className="text-xl font-semibold">
              In-browser speech recognition w/ word-level timestamps
            </h2>
          </div>

          <div className="w-full min-h-[220px] flex flex-col justify-center items-center p-2">
            {!audio && (
              <p className="mb-2">
                You are about to download{" "}
                <a
                  href="https://huggingface.co/onnx-community/whisper-base_timestamped"
                  target="_blank"
                  rel="noreferrer"
                  className="font-medium underline"
                >
                  whisper-base (timestamped)
                </a>
                , a 73 million parameter speech recognition model with the
                ability to generate word-level timestamps across 100 different
                languages. Once loaded, the model ({modelSize}&nbsp;MB) will be
                cached and reused when you revisit the page.
                <br />
                <br />
                Everything runs locally in your browser using{" "}
                <a
                  href="https://huggingface.co/docs/transformers.js"
                  target="_blank"
                  rel="noreferrer"
                  className="underline"
                >
                  ðŸ¤—&nbsp;Transformers.js
                </a>{" "}
                and ONNX Runtime Web, meaning no API calls are made to a server
                for inference. You can even disconnect from the internet after
                the model has loaded!
              </p>
            )}

            <div className="flex flex-col w-full m-3">
              <span className="text-sm mb-0.5">Input audio/video</span>
              <MediaInput
                ref={mediaInputRef}
                className="flex items-center border rounded-md cursor-pointer min-h-[100px] max-h-[500px] overflow-hidden"
                onInputChange={(result) => setAudio(result)}
                onTimeUpdate={(time) => setCurrentTime(time)}
              />
            </div>

            <div className="relative w-full flex justify-center items-center">
              <button
                className="border px-4 py-2 rounded-lg bg-blue-400 text-white hover:bg-blue-500 disabled:bg-blue-100 disabled:cursor-not-allowed select-none cursor-pointer"
                onClick={handleClick}
                disabled={
                  status === "running" || (status !== null && audio === null)
                }
              >
                {status === null
                  ? "Load model"
                  : status === "running"
                    ? "Running..."
                    : "Run model"}
              </button>

              {status !== null && (
                <div className="absolute right-0 bottom-0">
                  <span className="text-xs">Language:</span>
                  <br />
                  <LanguageSelector
                    className="border rounded-lg p-1 max-w-[100px] dark:bg-gray-800"
                    language={language}
                    setLanguage={setLanguage}
                  />
                </div>
              )}
            </div>

            {result && time && (
              <>
                <div className="w-full mt-4 border rounded-md">
                  <Transcript
                    className="p-2 max-h-[200px] overflow-y-auto scrollbar-thin select-none"
                    transcript={result}
                    currentTime={currentTime}
                    setCurrentTime={(time) => {
                      setCurrentTime(time);
                      mediaInputRef.current.setMediaTime(time);
                    }}
                  />
                </div>
                <p className="text-sm text-gray-600 dark:text-gray-300 text-end p-1">
                  Generation time:{" "}
                  <span className="text-gray-800 dark:text-gray-200 font-semibold">
                    {time.toFixed(2)}ms
                  </span>
                </p>
              </>
            )}
          </div>
        </div>
      </div>
    </div>
  );
}

export default App;


----- .\whisper-word-timestamps\src\index.css -----

@import "tailwindcss";

*::-webkit-scrollbar {
  width: 0.5rem;
}

*::-webkit-scrollbar-track {
  border-radius: 9999px;
  background-color: #f3f4f6;
  /* bg-gray-100 */
}

*::-webkit-scrollbar-thumb {
  border-radius: 9999px;
  background-color: #d1d5db;
  /* bg-gray-300 */
}

*::-webkit-scrollbar-thumb:hover {
  background-color: #6b7280;
  /* bg-gray-500 */
}

@media (prefers-color-scheme: dark) {
  *::-webkit-scrollbar-track {
    background-color: #374151;
    /* dark:bg-gray-700 */
  }

  *::-webkit-scrollbar-thumb {
    background-color: #4b5563;
    /* dark:bg-gray-600 */
  }
}


----- .\whisper-word-timestamps\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\whisper-word-timestamps\src\worker.js -----

import { pipeline } from "@huggingface/transformers";

const PER_DEVICE_CONFIG = {
  webgpu: {
    dtype: {
      encoder_model: "fp32",
      decoder_model_merged: "q4",
    },
    device: "webgpu",
  },
  wasm: {
    dtype: "q8",
    device: "wasm",
  },
};

/**
 * This class uses the Singleton pattern to ensure that only one instance of the model is loaded.
 */
class PipelineSingeton {
  static model_id = "onnx-community/whisper-base_timestamped";
  static instance = null;

  static async getInstance(progress_callback = null, device = "webgpu") {
    if (!this.instance) {
      this.instance = pipeline("automatic-speech-recognition", this.model_id, {
        ...PER_DEVICE_CONFIG[device],
        progress_callback,
      });
    }
    return this.instance;
  }
}

async function load({ device }) {
  self.postMessage({
    status: "loading",
    data: `Loading model (${device})...`,
  });

  // Load the pipeline and save it for future use.
  const transcriber = await PipelineSingeton.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  }, device);

  if (device === "webgpu") {
    self.postMessage({
      status: "loading",
      data: "Compiling shaders and warming up model...",
    });

    await transcriber(new Float32Array(16_000), {
      language: "en",
    });
  }

  self.postMessage({ status: "ready" });
}

async function run({ audio, language }) {
  const transcriber = await PipelineSingeton.getInstance();

  // Read and preprocess image
  const start = performance.now();

  const result = await transcriber(audio, {
    language,
    return_timestamps: "word",
    chunk_length_s: 30,
  });

  const end = performance.now();

  self.postMessage({ status: "complete", result, time: end - start });
}

// Listen for messages from the main thread
self.addEventListener("message", async (e) => {
  const { type, data } = e.data;

  switch (type) {
    case "load":
      load(data);
      break;

    case "run":
      run(data);
      break;
  }
});


----- .\whisper-word-timestamps\src\components\LanguageSelector.jsx -----

function titleCase(str) {
  str = str.toLowerCase();
  return (str.match(/\w+.?/g) || [])
    .map((word) => {
      return word.charAt(0).toUpperCase() + word.slice(1);
    })
    .join("");
}

// List of supported languages:
// https://help.openai.com/en/articles/7031512-whisper-api-faq
// https://github.com/openai/whisper/blob/248b6cb124225dd263bb9bd32d060b6517e067f8/whisper/tokenizer.py#L79
const LANGUAGES = {
  en: "english",
  zh: "chinese",
  de: "german",
  es: "spanish/castilian",
  ru: "russian",
  ko: "korean",
  fr: "french",
  ja: "japanese",
  pt: "portuguese",
  tr: "turkish",
  pl: "polish",
  ca: "catalan/valencian",
  nl: "dutch/flemish",
  ar: "arabic",
  sv: "swedish",
  it: "italian",
  id: "indonesian",
  hi: "hindi",
  fi: "finnish",
  vi: "vietnamese",
  he: "hebrew",
  uk: "ukrainian",
  el: "greek",
  ms: "malay",
  cs: "czech",
  ro: "romanian/moldavian/moldovan",
  da: "danish",
  hu: "hungarian",
  ta: "tamil",
  no: "norwegian",
  th: "thai",
  ur: "urdu",
  hr: "croatian",
  bg: "bulgarian",
  lt: "lithuanian",
  la: "latin",
  mi: "maori",
  ml: "malayalam",
  cy: "welsh",
  sk: "slovak",
  te: "telugu",
  fa: "persian",
  lv: "latvian",
  bn: "bengali",
  sr: "serbian",
  az: "azerbaijani",
  sl: "slovenian",
  kn: "kannada",
  et: "estonian",
  mk: "macedonian",
  br: "breton",
  eu: "basque",
  is: "icelandic",
  hy: "armenian",
  ne: "nepali",
  mn: "mongolian",
  bs: "bosnian",
  kk: "kazakh",
  sq: "albanian",
  sw: "swahili",
  gl: "galician",
  mr: "marathi",
  pa: "punjabi/panjabi",
  si: "sinhala/sinhalese",
  km: "khmer",
  sn: "shona",
  yo: "yoruba",
  so: "somali",
  af: "afrikaans",
  oc: "occitan",
  ka: "georgian",
  be: "belarusian",
  tg: "tajik",
  sd: "sindhi",
  gu: "gujarati",
  am: "amharic",
  yi: "yiddish",
  lo: "lao",
  uz: "uzbek",
  fo: "faroese",
  ht: "haitian creole/haitian",
  ps: "pashto/pushto",
  tk: "turkmen",
  nn: "nynorsk",
  mt: "maltese",
  sa: "sanskrit",
  lb: "luxembourgish/letzeburgesch",
  my: "myanmar/burmese",
  bo: "tibetan",
  tl: "tagalog",
  mg: "malagasy",
  as: "assamese",
  tt: "tatar",
  haw: "hawaiian",
  ln: "lingala",
  ha: "hausa",
  ba: "bashkir",
  jw: "javanese",
  su: "sundanese",
};
function LanguageSelector({ language, setLanguage, ...props }) {
  const handleLanguageChange = (event) => {
    setLanguage(event.target.value);
  };

  const names = Object.values(LANGUAGES).map(titleCase);

  return (
    <select {...props} value={language} onChange={handleLanguageChange}>
      {Object.keys(LANGUAGES).map((key, i) => (
        <option key={key} value={key}>
          {names[i]}
        </option>
      ))}
    </select>
  );
}
export default LanguageSelector;


----- .\whisper-word-timestamps\src\components\MediaInput.jsx -----

import {
  useState,
  forwardRef,
  useRef,
  useImperativeHandle,
  useEffect,
  useCallback,
} from "react";

const EXAMPLE_URL =
  "https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/whisper-timestamps-demo.mp4";

const MediaInput = forwardRef(
  ({ onInputChange, onTimeUpdate, ...props }, ref) => {
    // UI states
    const [dragging, setDragging] = useState(false);
    const fileInputRef = useRef(null);

    // Create a reference to the audio and video elements
    const audioElement = useRef(null);
    const videoElement = useRef(null);

    const currentTimeRef = useRef(0);
    useImperativeHandle(ref, () => ({
      setMediaTime(time) {
        if (audioElement.current?.src) {
          audioElement.current.currentTime = time;
        } else if (videoElement.current?.src) {
          videoElement.current.currentTime = time;
        }
        currentTimeRef.current = time;
      },
    }));

    const onBufferLoad = (arrayBuffer, type) => {
      const blob = new Blob([arrayBuffer.slice(0)], { type: type });
      const url = URL.createObjectURL(blob);
      processFile(arrayBuffer);

      // Create a URL for the Blob
      if (type.startsWith("audio/")) {
        // Dispose the previous source
        videoElement.current.pause();
        videoElement.current.removeAttribute("src");
        videoElement.current.load();

        audioElement.current.src = url;
      } else if (type.startsWith("video/")) {
        // Dispose the previous source
        audioElement.current.pause();
        audioElement.current.removeAttribute("src");
        audioElement.current.load();

        videoElement.current.src = url;
      } else {
        alert(`Unsupported file type: ${type}`);
      }
    };

    const readFile = (file) => {
      if (!file) return;

      // file.type
      const reader = new FileReader();
      reader.onload = (e) => {
        onBufferLoad(e.target.result, file.type);
      };
      reader.readAsArrayBuffer(file);
    };

    const handleInputChange = (event) => {
      readFile(event.target.files[0]);
    };

    const handleDragOver = (event) => {
      event.preventDefault();
    };

    const handleDrop = (event) => {
      event.preventDefault();
      setDragging(false);
      readFile(event.dataTransfer.files[0]);
    };

    const handleClick = (e) => {
      if (e.target.tagName === "VIDEO" || e.target.tagName === "AUDIO") {
        e.preventDefault();
        fileInputRef.current.click();
      } else if (e.target.tagName === "INPUT") {
        e.stopPropagation();
      } else {
        fileInputRef.current.click();
        e.stopPropagation();
      }
    };

    const processFile = async (buffer) => {
      const audioContext = new (window.AudioContext ||
        window.webkitAudioContext)({ sampleRate: 16_000 });

      try {
        const audioBuffer = await audioContext.decodeAudioData(buffer);
        let audio;
        if (audioBuffer.numberOfChannels === 2) {
          // Merge channels
          const SCALING_FACTOR = Math.sqrt(2);
          const left = audioBuffer.getChannelData(0);
          const right = audioBuffer.getChannelData(1);
          audio = new Float32Array(left.length);
          for (let i = 0; i < audioBuffer.length; ++i) {
            audio[i] = (SCALING_FACTOR * (left[i] + right[i])) / 2;
          }
        } else {
          audio = audioBuffer.getChannelData(0);
        }
        onInputChange(audio);
      } catch (e) {
        alert(e);
      }
    };

    const requestRef = useRef();

    const updateTime = useCallback(() => {
      let elem;
      if (audioElement.current?.src) {
        elem = audioElement.current;
      } else if (videoElement.current?.src) {
        elem = videoElement.current;
      }

      if (elem && currentTimeRef.current !== elem.currentTime) {
        currentTimeRef.current = elem.currentTime;
        onTimeUpdate(elem.currentTime);
      }

      // Request the next frame
      requestRef.current = requestAnimationFrame(updateTime);
    }, [onTimeUpdate]);

    useEffect(() => {
      // Start the animation
      requestRef.current = requestAnimationFrame(updateTime);

      return () => {
        // Cleanup on component unmount
        cancelAnimationFrame(requestRef.current);
      };
    }, [updateTime]);
    return (
      <div
        {...props}
        onClick={handleClick}
        onDragOver={handleDragOver}
        onDrop={handleDrop}
        onDragEnter={(e) => setDragging(true)}
        onDragLeave={(e) => setDragging(false)}
      >
        <input
          type="file"
          accept="audio/*,video/*"
          onChange={handleInputChange}
          ref={fileInputRef}
          className="hidden"
        />
        {
          <audio
            ref={audioElement}
            controls
            style={{ display: audioElement.current?.src ? "block" : "none" }}
            className="w-full max-h-full"
          />
        }
        {
          <video
            ref={videoElement}
            controls
            style={{ display: videoElement.current?.src ? "block" : "none" }}
            className="w-full max-h-full"
          />
        }
        {!audioElement.current?.src && !videoElement.current?.src && (
          <div
            className="w-full flex flex-col items-center justify-center border-2 border-dashed border-gray-300 rounded-md h-[250px]"
            style={{ borderColor: dragging ? "blue" : "lightgray" }}
          >
            <span className="text-gray-600 text-center">
              <u>Drag & drop</u> or <u>click</u>
              <br />
              to select media
            </span>
            <span
              className="text-gray-500 text-sm hover:text-gray-800 dark:hover:text-gray-300 mt-2"
              onClick={async (e) => {
                e.stopPropagation();
                const buffer = await fetch(EXAMPLE_URL).then((r) =>
                  r.arrayBuffer(),
                );
                videoElement.current.src = URL.createObjectURL(
                  new Blob([buffer], { type: "video/mp4" }),
                );
                onBufferLoad(buffer, "video/mp4");
              }}
            >
              (or <u>try an example</u>)
            </span>
          </div>
        )}
      </div>
    );
  },
);
MediaInput.displayName = "MediaInput";

export default MediaInput;


----- .\whisper-word-timestamps\src\components\Progress.jsx -----

function formatBytes(size) {
  const i = size == 0 ? 0 : Math.floor(Math.log(size) / Math.log(1024));
  return (
    +(size / Math.pow(1024, i)).toFixed(2) * 1 +
    ["B", "kB", "MB", "GB", "TB"][i]
  );
}

export default function Progress({ text, percentage, total }) {
  percentage ??= 0;
  return (
    <div className="w-full bg-gray-100 dark:bg-gray-700 text-left rounded-lg overflow-hidden mb-0.5">
      <div
        className="bg-blue-400 whitespace-nowrap px-1 text-sm"
        style={{ width: `${percentage}%` }}
      >
        {text} ({percentage.toFixed(2)}%
        {isNaN(total) ? "" : ` of ${formatBytes(total)}`})
      </div>
    </div>
  );
}


----- .\whisper-word-timestamps\src\components\Transcript.jsx -----

import { useMemo } from "react";

const Chunk = ({ chunk, currentTime, onClick, ...props }) => {
  const { text, timestamp } = chunk;
  const [start, end] = timestamp;

  const bolded = start <= currentTime && currentTime < end;

  return (
    <span {...props}>
      {text.startsWith(" ") ? " " : ""}
      <span
        onClick={onClick}
        className="text-md text-gray-600 dark:text-gray-300 cursor-pointer hover:text-red-600"
        title={timestamp.map((x) => x.toFixed(2)).join(" â†’ ")}
        style={{
          textDecoration: bolded ? "underline" : "none",
          textShadow: bolded ? "0 0 1px #000" : "none",
        }}
      >
        {text.trim()}
      </span>
    </span>
  );
};

const Transcript = ({ transcript, currentTime, setCurrentTime, ...props }) => {
  const jsonTranscript = useMemo(() => {
    return (
      JSON.stringify(transcript, null, 2)
        // post-process the JSON to make it more readable
        .replace(/( {4}"timestamp": )\[\s+(\S+)\s+(\S+)\s+\]/gm, "$1[$2 $3]")
    );
  }, [transcript]);

  const downloadTranscript = () => {
    const blob = new Blob([jsonTranscript], { type: "application/json" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    a.download = "transcript.json";
    a.click();
    URL.revokeObjectURL(url);
  };

  return (
    <>
      <div {...props}>
        {transcript.chunks.map((chunk, i) => (
          <Chunk
            key={i}
            chunk={chunk}
            currentTime={currentTime}
            onClick={(e) => {
              setCurrentTime(chunk.timestamp[0]); // Set to start of chunk
            }}
          />
        ))}
      </div>

      <div className="flex justify-center border-t text-sm text-gray-600 max-h-[150px] overflow-y-auto p-2 scrollbar-thin">
        <button
          className="flex items-center border px-2 py-1 rounded-lg bg-green-400 text-white hover:bg-green-500 cursor-pointer"
          onClick={downloadTranscript}
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            fill="none"
            viewBox="0 0 24 24"
            strokeWidth={1.5}
            stroke="currentColor"
            className="size-6 mr-1"
          >
            <path
              strokeLinecap="round"
              strokeLinejoin="round"
              d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"
            />
          </svg>
          Download transcript
        </button>
      </div>
    </>
  );
};
export default Transcript;


----- .\zero-shot-classification\src\App.jsx -----

import { useState, useRef, useEffect, useCallback } from "react";

const PLACEHOLDER_REVIEWS = [
  // battery/charging problems
  "Disappointed with the battery life! The phone barely lasts half a day with regular use. Considering how much I paid for it, I expected better performance in this department.",
  "I bought this phone a week ago, and I'm already frustrated with the battery life. It barely lasts half a day with normal usage. I expected more from a supposedly high-end device",
  "The charging port is so finicky. Sometimes it takes forever to charge, and other times it doesn't even recognize the charger. Frustrating experience!",

  // overheating
  "This phone heats up way too quickly, especially when using demanding apps. It's uncomfortable to hold, and I'm concerned it might damage the internal components over time. Not what I expected",
  "This phone is like holding a hot potato. Video calls turn it into a scalding nightmare. Seriously, can't it keep its cool?",
  "Forget about a heatwave outside; my phone's got its own. It's like a little portable heater. Not what I signed up for.",

  // poor build quality
  "I dropped the phone from a short distance, and the screen cracked easily. Not as durable as I expected from a flagship device.",
  "Took a slight bump in my bag, and the frame got dinged. Are we back in the flip phone era?",
  "So, my phone's been in my pocket with just keys â€“ no ninja moves or anything. Still, it managed to get some scratches. Disappointed with the build quality.",

  // software
  "The software updates are a nightmare. Each update seems to introduce new bugs, and it takes forever for them to be fixed.",
  "Constant crashes and freezes make me want to throw it into a black hole.",
  "Every time I open Instagram, my phone freezes and crashes. It's so frustrating!",

  // other
  "I'm not sure what to make of this phone. It's not bad, but it's not great either. I'm on the fence about it.",
  "I hate the color of this phone. It's so ugly!",
  "This phone sucks! I'm returning it.",
].sort(() => Math.random() - 0.5);

const PLACEHOLDER_SECTIONS = [
  "Battery and charging problems",
  "Overheating",
  "Poor build quality",
  "Software issues",
  "Other",
];

function App() {
  const [text, setText] = useState(PLACEHOLDER_REVIEWS.join("\n"));

  const [sections, setSections] = useState(
    PLACEHOLDER_SECTIONS.map((title) => ({ title, items: [] })),
  );

  const [status, setStatus] = useState("idle");

  // Create a reference to the worker object.
  const worker = useRef(null);

  // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
  useEffect(() => {
    if (!worker.current) {
      // Create the worker if it does not yet exist.
      worker.current = new Worker(new URL("./worker.js", import.meta.url), {
        type: "module",
      });
    }

    // Create a callback function for messages from the worker thread.
    const onMessageReceived = (e) => {
      const status = e.data.status;
      if (status === "initiate") {
        setStatus("loading");
      } else if (status === "ready") {
        setStatus("ready");
      } else if (status === "output") {
        const { sequence, labels, scores } = e.data.output;

        // Threshold for classification
        const label = scores[0] > 0.5 ? labels[0] : "Other";

        const sectionID =
          sections.map((x) => x.title).indexOf(label) ?? sections.length - 1;
        setSections((sections) => {
          const newSections = [...sections];
          newSections[sectionID] = {
            ...newSections[sectionID],
            items: [...newSections[sectionID].items, sequence],
          };
          return newSections;
        });
      } else if (status === "complete") {
        setStatus("idle");
      }
    };

    // Attach the callback function as an event listener.
    worker.current.addEventListener("message", onMessageReceived);

    // Define a cleanup function for when the component is unmounted.
    return () =>
      worker.current.removeEventListener("message", onMessageReceived);
  }, [sections]);

  const classify = useCallback(() => {
    setStatus("processing");
    worker.current.postMessage({
      text,
      labels: sections
        .slice(0, sections.length - 1)
        .map((section) => section.title),
    });
  }, [text, sections]);

  const busy = status !== "idle";

  return (
    <div className="flex flex-col h-screen w-screen p-1">
      <textarea
        className="border w-full p-1 h-1/2"
        value={text}
        onChange={(e) => setText(e.target.value)}
      ></textarea>
      <div className="flex flex-col justify-center items-center m-2 gap-1">
        <button
          className="border py-1 px-2 bg-blue-400 rounded text-white text-lg font-medium disabled:opacity-50 disabled:cursor-not-allowed cursor-pointer"
          disabled={busy}
          onClick={classify}
        >
          {!busy
            ? "Categorize"
            : status === "loading"
              ? "Model loading..."
              : "Processing"}
        </button>
        <div className="flex gap-1">
          <button
            className="border py-1 px-2 bg-green-400 rounded text-white text-sm font-medium cursor-pointer"
            onClick={(e) => {
              setSections((sections) => {
                const newSections = [...sections];
                // add at position 2 from the end
                newSections.splice(newSections.length - 1, 0, {
                  title: "New Category",
                  items: [],
                });
                return newSections;
              });
            }}
          >
            Add category
          </button>
          <button
            className="border py-1 px-2 bg-red-400 rounded text-white text-sm font-medium cursor-pointer"
            disabled={sections.length <= 1}
            onClick={(e) => {
              setSections((sections) => {
                const newSections = [...sections];
                newSections.splice(newSections.length - 2, 1); // Remove second last element
                return newSections;
              });
            }}
          >
            Remove category
          </button>
          <button
            className="border py-1 px-2 bg-orange-400 rounded text-white text-sm font-medium cursor-pointer"
            onClick={(e) => {
              setSections((sections) =>
                sections.map((section) => ({
                  ...section,
                  items: [],
                })),
              );
            }}
          >
            Clear
          </button>
        </div>
      </div>

      <div className="flex justify-between flex-grow overflow-x-auto max-h-[40%]">
        {sections.map((section, index) => (
          <div key={index} className="flex flex-col w-full">
            <input
              disabled={section.title === "Other"}
              className="w-full border px-1 text-center"
              value={section.title}
              onChange={(e) => {
                setSections((sections) => {
                  const newSections = [...sections];
                  newSections[index].title = e.target.value;
                  return newSections;
                });
              }}
            ></input>
            <div className="overflow-y-auto h-full border">
              {section.items.map((item, index) => (
                <div
                  className="m-2 border bg-red-50 rounded p-1 text-sm"
                  key={index}
                >
                  {item}
                </div>
              ))}
            </div>
          </div>
        ))}
      </div>
    </div>
  );
}

export default App;


----- .\zero-shot-classification\src\index.css -----

@import "tailwindcss";


----- .\zero-shot-classification\src\main.jsx -----

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.jsx";
import "./index.css";

ReactDOM.createRoot(document.getElementById("root")).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


----- .\zero-shot-classification\src\worker.js -----

import { pipeline } from "@huggingface/transformers";

class MyZeroShotClassificationPipeline {
  static task = "zero-shot-classification";
  static model = "MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33";
  static instance = null;

  static async getInstance(progress_callback = null) {
    this.instance ??= pipeline(this.task, this.model, {
      progress_callback,
    });

    return this.instance;
  }
}

// Listen for messages from the main thread
self.addEventListener("message", async (event) => {
  // Retrieve the pipeline. When called for the first time,
  // this will load the pipeline and save it for future use.
  const classifier = await MyZeroShotClassificationPipeline.getInstance((x) => {
    // We also add a progress callback to the pipeline so that we can
    // track model loading.
    self.postMessage(x);
  });

  const { text, labels } = event.data;

  const split = text.split("\n");
  for (const line of split) {
    const output = await classifier(line, labels, {
      hypothesis_template: "This text is about {}.",
      multi_label: true,
    });
    // Send the output back to the main thread
    self.postMessage({ status: "output", output });
  }
  // Send the output back to the main thread
  self.postMessage({ status: "complete" });
});


